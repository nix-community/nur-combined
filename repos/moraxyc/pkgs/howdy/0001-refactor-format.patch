From 4ff8447109bdbbd4a5305ef0a6908c9f7218747b Mon Sep 17 00:00:00 2001
From: Moraxyc <i@qaq.li>
Date: Tue, 29 Apr 2025 21:32:13 +0800
Subject: [PATCH 1/2] refactor: format

---
 .envrc                               |    1 +
 .gitignore                           |    5 +
 howdy-gtk/src/authsticky.py          |  256 +++---
 howdy-gtk/src/i18n.py                |    4 +-
 howdy-gtk/src/init.py                |    4 +-
 howdy-gtk/src/onboarding.py          |  674 ++++++++--------
 howdy-gtk/src/tab_models.py          |  223 +++---
 howdy-gtk/src/tab_video.py           |  103 +--
 howdy-gtk/src/window.py              |  155 ++--
 howdy/src/cli.py                     |  109 +--
 howdy/src/cli/add.py                 |  185 +++--
 howdy/src/cli/clear.py               |   24 +-
 howdy/src/cli/config.py              |    4 +-
 howdy/src/cli/disable.py             |   30 +-
 howdy/src/cli/list.py                |   48 +-
 howdy/src/cli/remove.py              |   94 +--
 howdy/src/cli/set.py                 |   20 +-
 howdy/src/cli/snap.py                |   31 +-
 howdy/src/cli/test.py                |  410 ++++++----
 howdy/src/compare.py                 |  522 +++++++------
 howdy/src/i18n.py                    |    4 +-
 howdy/src/recorders/ffmpeg_reader.py |  239 +++---
 howdy/src/recorders/pyv4l2_reader.py |  177 ++---
 howdy/src/recorders/v4l2.py          | 1078 +++++++++++++-------------
 howdy/src/recorders/video_capture.py |  269 ++++---
 howdy/src/rubberstamps/__init__.py   |  324 ++++----
 howdy/src/rubberstamps/hotkey.py     |  128 +--
 howdy/src/rubberstamps/nod.py        |  180 ++---
 howdy/src/snapshot.py                |  113 +--
 shell.nix                            |   55 ++
 30 files changed, 2952 insertions(+), 2517 deletions(-)
 create mode 100644 .envrc
 create mode 100644 shell.nix

diff --git a/.envrc b/.envrc
new file mode 100644
index 0000000..1d953f4
--- /dev/null
+++ b/.envrc
@@ -0,0 +1 @@
+use nix
diff --git a/.gitignore b/.gitignore
index e0bd171..ea8b39d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -128,3 +128,8 @@ debian/howdy
 
 # Meson
 subprojects/
+
+# Nix
+result
+result-*
+.direnv/
diff --git a/howdy-gtk/src/authsticky.py b/howdy-gtk/src/authsticky.py
index 4a8c8a3..e7100af 100644
--- a/howdy-gtk/src/authsticky.py
+++ b/howdy-gtk/src/authsticky.py
@@ -23,132 +23,136 @@ windowHeight = 100
 
 
 class StickyWindow(gtk.Window):
-	# Set default messages to show in the popup
-	message = _("Loading...  ")
-	subtext = ""
-
-	def __init__(self):
-		"""Initialize the sticky window"""
-		# Make the class a GTK window
-		gtk.Window.__init__(self)
-
-		# Get the absolute or relative path to the logo file
-		logo_path = paths_factory.logo_path()
-
-		# Create image and calculate scale size based on image size
-		self.logo_surface = cairo.ImageSurface.create_from_png(logo_path)
-		self.logo_ratio = float(windowHeight - 20) / float(self.logo_surface.get_height())
-
-		# Set the title of the window
-		self.set_title(_("Howdy Authentication"))
-
-		# Set a bunch of options to make the window stick and be on top of everything
-		self.stick()
-		self.set_gravity(gdk.Gravity.STATIC)
-		self.set_resizable(False)
-		self.set_keep_above(True)
-		self.set_app_paintable(True)
-		self.set_skip_pager_hint(True)
-		self.set_skip_taskbar_hint(True)
-		self.set_can_focus(False)
-		self.set_can_default(False)
-		self.set_focus(None)
-		self.set_type_hint(gdk.WindowTypeHint.NOTIFICATION)
-		self.set_decorated(False)
-
-		# Listen for a window redraw
-		self.connect("draw", self.draw)
-		# Listen for a force close or click event and exit
-		self.connect("destroy", self.exit)
-		self.connect("delete_event", self.exit)
-		self.connect("button-press-event", self.exit)
-		self.connect("button-release-event", self.exit)
-
-		# Create a GDK drawing, restricts the window size
-		darea = gtk.DrawingArea()
-		darea.set_size_request(windowWidth, windowHeight)
-		self.add(darea)
-
-		# Get the default screen
-		screen = gdk.Screen.get_default()
-		visual = screen.get_rgba_visual()
-		self.set_visual(visual)
-
-		# Move the window to the center top of the default window, where a webcam usually is
-		self.move((screen.get_width() / 2) - (windowWidth / 2), 0)
-
-		# Show window and force a resize again
-		self.show_all()
-		self.resize(windowWidth, windowHeight)
-
-		# Add a timeout to catch input passed from compare.py
-		gobject.timeout_add(100, self.catch_stdin)
-
-		# Start GTK main loop
-		gtk.main()
-
-	def draw(self, widget, ctx):
-		"""Draw the UI"""
-		# Change cursor to the kill icon
-		self.get_window().set_cursor(gdk.Cursor(gdk.CursorType.PIRATE))
-
-		# Draw a semi transparent background
-		ctx.set_source_rgba(0, 0, 0, .7)
-		ctx.set_operator(cairo.OPERATOR_SOURCE)
-		ctx.paint()
-		ctx.set_operator(cairo.OPERATOR_OVER)
-
-		# Position and draw the logo
-		ctx.translate(15, 10)
-		ctx.scale(self.logo_ratio, self.logo_ratio)
-		ctx.set_source_surface(self.logo_surface)
-		ctx.paint()
-
-		# Calculate main message positioning, as the text is higher if there's a subtext
-		if self.subtext:
-			ctx.move_to(380, 145)
-		else:
-			ctx.move_to(380, 175)
-
-		# Draw the main message
-		ctx.set_source_rgba(255, 255, 255, .9)
-		ctx.set_font_size(80)
-		ctx.select_font_face("Arial", cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_NORMAL)
-		ctx.show_text(self.message)
-
-		# Draw the subtext if there is one
-		if self.subtext:
-			ctx.move_to(380, 210)
-			ctx.set_source_rgba(230, 230, 230, .8)
-			ctx.set_font_size(40)
-			ctx.select_font_face("Arial", cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_NORMAL)
-			ctx.show_text(self.subtext)
-
-	def catch_stdin(self):
-		"""Catch input from stdin and redraw"""
-		# Wait for a line on stdin
-		comm = sys.stdin.readline()[:-1]
-
-		# If the line is not empty
-		if comm:
-			# Parse a message
-			if comm[0] == "M":
-				self.message = comm[2:].strip()
-			# Parse subtext
-			if comm[0] == "S":
-				# self.subtext += " "
-				self.subtext = comm[2:].strip()
-
-		# Redraw the ui
-		self.queue_draw()
-
-		# Fire this function again in 10ms, as we're waiting on IO in readline anyway
-		gobject.timeout_add(10, self.catch_stdin)
-
-	def exit(self, widget, context):
-		"""Cleanly exit"""
-		gtk.main_quit()
-		return True
+    # Set default messages to show in the popup
+    message = _("Loading...  ")
+    subtext = ""
+
+    def __init__(self):
+        """Initialize the sticky window"""
+        # Make the class a GTK window
+        gtk.Window.__init__(self)
+
+        # Get the absolute or relative path to the logo file
+        logo_path = paths_factory.logo_path()
+
+        # Create image and calculate scale size based on image size
+        self.logo_surface = cairo.ImageSurface.create_from_png(logo_path)
+        self.logo_ratio = float(windowHeight - 20) / float(
+            self.logo_surface.get_height()
+        )
+
+        # Set the title of the window
+        self.set_title(_("Howdy Authentication"))
+
+        # Set a bunch of options to make the window stick and be on top of everything
+        self.stick()
+        self.set_gravity(gdk.Gravity.STATIC)
+        self.set_resizable(False)
+        self.set_keep_above(True)
+        self.set_app_paintable(True)
+        self.set_skip_pager_hint(True)
+        self.set_skip_taskbar_hint(True)
+        self.set_can_focus(False)
+        self.set_can_default(False)
+        self.set_focus(None)
+        self.set_type_hint(gdk.WindowTypeHint.NOTIFICATION)
+        self.set_decorated(False)
+
+        # Listen for a window redraw
+        self.connect("draw", self.draw)
+        # Listen for a force close or click event and exit
+        self.connect("destroy", self.exit)
+        self.connect("delete_event", self.exit)
+        self.connect("button-press-event", self.exit)
+        self.connect("button-release-event", self.exit)
+
+        # Create a GDK drawing, restricts the window size
+        darea = gtk.DrawingArea()
+        darea.set_size_request(windowWidth, windowHeight)
+        self.add(darea)
+
+        # Get the default screen
+        screen = gdk.Screen.get_default()
+        visual = screen.get_rgba_visual()
+        self.set_visual(visual)
+
+        # Move the window to the center top of the default window, where a webcam usually is
+        self.move((screen.get_width() / 2) - (windowWidth / 2), 0)
+
+        # Show window and force a resize again
+        self.show_all()
+        self.resize(windowWidth, windowHeight)
+
+        # Add a timeout to catch input passed from compare.py
+        gobject.timeout_add(100, self.catch_stdin)
+
+        # Start GTK main loop
+        gtk.main()
+
+    def draw(self, widget, ctx):
+        """Draw the UI"""
+        # Change cursor to the kill icon
+        self.get_window().set_cursor(gdk.Cursor(gdk.CursorType.PIRATE))
+
+        # Draw a semi transparent background
+        ctx.set_source_rgba(0, 0, 0, 0.7)
+        ctx.set_operator(cairo.OPERATOR_SOURCE)
+        ctx.paint()
+        ctx.set_operator(cairo.OPERATOR_OVER)
+
+        # Position and draw the logo
+        ctx.translate(15, 10)
+        ctx.scale(self.logo_ratio, self.logo_ratio)
+        ctx.set_source_surface(self.logo_surface)
+        ctx.paint()
+
+        # Calculate main message positioning, as the text is higher if there's a subtext
+        if self.subtext:
+            ctx.move_to(380, 145)
+        else:
+            ctx.move_to(380, 175)
+
+        # Draw the main message
+        ctx.set_source_rgba(255, 255, 255, 0.9)
+        ctx.set_font_size(80)
+        ctx.select_font_face("Arial", cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_NORMAL)
+        ctx.show_text(self.message)
+
+        # Draw the subtext if there is one
+        if self.subtext:
+            ctx.move_to(380, 210)
+            ctx.set_source_rgba(230, 230, 230, 0.8)
+            ctx.set_font_size(40)
+            ctx.select_font_face(
+                "Arial", cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_NORMAL
+            )
+            ctx.show_text(self.subtext)
+
+    def catch_stdin(self):
+        """Catch input from stdin and redraw"""
+        # Wait for a line on stdin
+        comm = sys.stdin.readline()[:-1]
+
+        # If the line is not empty
+        if comm:
+            # Parse a message
+            if comm[0] == "M":
+                self.message = comm[2:].strip()
+            # Parse subtext
+            if comm[0] == "S":
+                # self.subtext += " "
+                self.subtext = comm[2:].strip()
+
+        # Redraw the ui
+        self.queue_draw()
+
+        # Fire this function again in 10ms, as we're waiting on IO in readline anyway
+        gobject.timeout_add(10, self.catch_stdin)
+
+    def exit(self, widget, context):
+        """Cleanly exit"""
+        gtk.main_quit()
+        return True
 
 
 # Make sure we quit on a SIGINT
diff --git a/howdy-gtk/src/i18n.py b/howdy-gtk/src/i18n.py
index 2b3afb2..0175e0a 100644
--- a/howdy-gtk/src/i18n.py
+++ b/howdy-gtk/src/i18n.py
@@ -5,7 +5,9 @@ import gettext
 import os
 
 # Get the right translation based on locale, falling back to base if none found
-translation = gettext.translation("gtk", localedir=os.path.join(os.path.dirname(__file__), "locales"), fallback=True)
+translation = gettext.translation(
+    "gtk", localedir=os.path.join(os.path.dirname(__file__), "locales"), fallback=True
+)
 translation.install()
 
 # Export translation function as _
diff --git a/howdy-gtk/src/init.py b/howdy-gtk/src/init.py
index b351050..9c9e57b 100755
--- a/howdy-gtk/src/init.py
+++ b/howdy-gtk/src/init.py
@@ -2,6 +2,6 @@
 import sys
 
 if "--start-auth-ui" in sys.argv:
-	import authsticky
+    import authsticky
 else:
-	import window
+    import window
diff --git a/howdy-gtk/src/onboarding.py b/howdy-gtk/src/onboarding.py
index d4713c8..e952104 100644
--- a/howdy-gtk/src/onboarding.py
+++ b/howdy-gtk/src/onboarding.py
@@ -14,313 +14,367 @@ from gi.repository import Pango as pango
 
 
 class OnboardingWindow(gtk.Window):
-	def __init__(self):
-		"""Initialize the sticky window"""
-		# Make the class a GTK window
-		gtk.Window.__init__(self)
-
-		self.builder = gtk.Builder()
-		self.builder.add_from_file(paths_factory.onboarding_wireframe_path())
-		self.builder.connect_signals(self)
-
-		self.window = self.builder.get_object("onboardingwindow")
-		self.slidecontainer = self.builder.get_object("slidecontainer")
-		self.nextbutton = self.builder.get_object("nextbutton")
-
-		self.window.connect("destroy", self.exit)
-		self.window.connect("delete_event", self.exit)
-
-		self.slides = [
-			self.builder.get_object("slide0"),
-			self.builder.get_object("slide1"),
-			self.builder.get_object("slide2"),
-			self.builder.get_object("slide3"),
-			self.builder.get_object("slide4"),
-			self.builder.get_object("slide5"),
-			self.builder.get_object("slide6")
-		]
-
-		self.window.show_all()
-		self.window.resize(500, 400)
-
-		self.window.current_slide = 0
-
-		# Start GTK main loop
-		gtk.main()
-
-	def go_next_slide(self, button=None):
-		self.nextbutton.set_sensitive(False)
-
-		self.slides[self.window.current_slide].hide()
-		self.slides[self.window.current_slide + 1].show()
-		self.window.current_slide += 1
-		# the shown child may have zero/wrong dimensions
-		self.slidecontainer.queue_resize()
-
-		if self.window.current_slide == 1:
-			self.execute_slide1()
-		elif self.window.current_slide == 2:
-			gobject.timeout_add(10, self.execute_slide2)
-		elif self.window.current_slide == 3:
-			self.execute_slide3()
-		elif self.window.current_slide == 4:
-			self.execute_slide4()
-		elif self.window.current_slide == 5:
-			self.execute_slide5()
-		elif self.window.current_slide == 6:
-			self.execute_slide6()
-
-	def execute_slide1(self):
-		self.downloadoutputlabel = self.builder.get_object("downloadoutputlabel")
-		eventbox = self.builder.get_object("downloadeventbox")
-		eventbox.modify_bg(gtk.StateType.NORMAL, gdk.Color(red=0, green=0, blue=0))
-
-		# TODO: Better way to do this?
-		if os.path.exists(paths_factory.dlib_data_dir_path() / "shape_predictor_5_face_landmarks.dat"):
-			self.downloadoutputlabel.set_text(_("Datafiles have already been downloaded!\nClick Next to continue"))
-			self.enable_next()
-			return
-
-		self.proc = subprocess.Popen("./install.sh", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, cwd=paths_factory.dlib_data_dir_path())
-
-		self.download_lines = []
-		self.read_download_line()
-
-	def read_download_line(self):
-		line = self.proc.stdout.readline()
-		self.download_lines.append(line.decode("utf-8"))
-
-		print("install.sh output:")
-		print(line.decode("utf-8"))
-
-		if len(self.download_lines) > 10:
-			self.download_lines.pop(0)
-
-		self.downloadoutputlabel.set_text(" ".join(self.download_lines))
-
-		if line:
-			gobject.timeout_add(10, self.read_download_line)
-			return
-
-		# Wait for the process to finish and check the status code
-		if self.proc.wait(5) != 0:
-			self.show_error(_("Error while downloading datafiles"), " ".join(self.download_lines))
-
-		self.downloadoutputlabel.set_text(_("Done!\nClick Next to continue"))
-		self.enable_next()
-
-	def execute_slide2(self):
-		def is_gray(frame):
-			for row in frame:
-				for pixel in row:
-					if not pixel[0] == pixel[1] == pixel[2]:
-						return False
-			return True
-
-		try:
-			import cv2
-		except Exception:
-			self.show_error(_("Error while importing OpenCV2"), _("Try reinstalling cv2"))
-
-		device_rows = []
-		try:
-			device_ids = os.listdir("/dev/v4l/by-path")
-		except Exception:
-			self.show_error(_("No webcams found on system"), _("Please configure your camera yourself if you are sure a compatible camera is connected"))
-
-		# Loop though all devices
-		for dev in device_ids:
-			time.sleep(.5)
-
-			# The full path to the device is the default name
-			device_path = "/dev/v4l/by-path/" + dev
-			device_name = dev
-
-			# Get the udevadm details to try to get a better name
-			udevadm = subprocess.check_output(["udevadm info -r --query=all -n " + device_path], shell=True).decode("utf-8")
-
-			# Loop though udevadm to search for a better name
-			for line in udevadm.split("\n"):
-				# Match it and encase it in quotes
-				re_name = re.search('product.*=(.*)$', line, re.IGNORECASE)
-				if re_name:
-					device_name = re_name.group(1)
-
-			capture = cv2.VideoCapture(device_path)
-			is_open, frame = capture.read()
-			if not is_open:
-				device_rows.append([device_name, device_path, -9, _("No, camera can't be opened")])
-				continue
-
-			try:
-				if not is_gray(frame):
-					raise Exception()
-			except Exception:
-				device_rows.append([device_name, device_path, -5, _("No, not an infrared camera")])
-				capture.release()
-				continue
-
-			device_rows.append([device_name, device_path, 5, _("Yes, compatible infrared camera")])
-			capture.release()
-
-		device_rows = sorted(device_rows, key=lambda k: -k[2])
-
-		self.loadinglabel = self.builder.get_object("loadinglabel")
-		self.devicelistbox = self.builder.get_object("devicelistbox")
-
-		self.treeview = gtk.TreeView()
-		self.treeview.set_vexpand(True)
-
-		# Set the columns
-		for i, column in enumerate([_("Camera identifier or path"), _("Recommended")]):
-			cell = gtk.CellRendererText()
-			cell.set_property("ellipsize", pango.EllipsizeMode.END)
-			col = gtk.TreeViewColumn(column, cell, text=i)
-			self.treeview.append_column(col)
-
-		# Add the treeview
-		self.devicelistbox.add(self.treeview)
-
-		# Create a datamodel
-		self.listmodel = gtk.ListStore(str, str, str, bool)
-
-		for device in device_rows:
-			is_gray = device[2] == 5
-			self.listmodel.append([device[0], device[3], device[1], is_gray])
-
-		self.treeview.set_model(self.listmodel)
-		self.treeview.set_cursor(0)
-
-		self.treeview.show()
-		self.loadinglabel.hide()
-		self.enable_next()
-
-	def execute_slide3(self):
-		try:
-			import cv2
-		except Exception:
-			self.show_error(_("Error while importing OpenCV2"), _("Try reinstalling cv2"))
-
-		selection = self.treeview.get_selection()
-		(listmodel, rowlist) = selection.get_selected_rows()
-		
-		if len(rowlist) != 1:
-			self.show_error(_("Error selecting camera"))
-   
-		device_path = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
-		is_gray = listmodel.get_value(listmodel.get_iter(rowlist[0]), 3)
-
-		if is_gray:
-			# test if linux-enable-ir-emitter help should be displayed, 
-			# the user must click on the yes/no button which calls the method slide3_button_yes|no
-			self.capture = cv2.VideoCapture(device_path)
-			if not self.capture.isOpened():
-				self.show_error(_("The selected camera cannot be opened"), _("Try to select another one"))
-			self.capture.read()
-		else:  
-			# skip, the selected camera is not infrared
-			self.go_next_slide()
-
-	def slide3_button_yes(self, button):
-		self.capture.release()
-		self.go_next_slide()
-
-	def slide3_button_no(self, button):
-		self.capture.release()
-		self.builder.get_object("leiestatus").set_markup(_("Please visit\n<a href=\"https://github.com/EmixamPP/linux-enable-ir-emitter\">https://github.com/EmixamPP/linux-enable-ir-emitter</a>\nto enable your ir emitter"))
-		self.builder.get_object("leieyesbutton").hide()
-		self.builder.get_object("leienobutton").hide()
-
-	def execute_slide4(self):
-		selection = self.treeview.get_selection()
-		(listmodel, rowlist) = selection.get_selected_rows()
-
-		if len(rowlist) != 1:
-			self.show_error(_("Error selecting camera"))
-
-		device_path = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
-		self.proc = subprocess.Popen("howdy set device_path " + device_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
-
-		self.window.set_focus(self.builder.get_object("scanbutton"))
-
-	def on_scanbutton_click(self, button):
-		status = self.proc.wait(2)
-
-		# if status != 0:
-		# 	self.show_error(_("Error setting camera path"), _("Please set the camera path manually"))
-
-		self.dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL)
-		self.dialog.set_title(_("Creating Model"))
-		self.dialog.props.text = _("Please look directly into the camera")
-		self.dialog.show_all()
-
-		# Wait a bit to allow the user to read the dialog
-		gobject.timeout_add(600, self.run_add)
-
-	def run_add(self):
-		status, output = subprocess.getstatusoutput(["howdy add -y"])
-
-		print("howdy add output:")
-		print(output)
-
-		self.dialog.destroy()
-
-		if status != 0:
-			self.show_error(_("Can't save face model"), output)
-
-		gobject.timeout_add(10, self.go_next_slide)
-
-	def execute_slide5(self):
-		self.enable_next()
-
-	def execute_slide6(self):
-		radio_buttons = self.builder.get_object("radiobalanced").get_group()
-		radio_selected = False
-		radio_certanty = 5.0
-
-		for button in radio_buttons:
-			if button.get_active():
-				radio_selected = gtk.Buildable.get_name(button)
-
-		if not radio_selected:
-			self.show_error(_("Error reading radio buttons"))
-		elif radio_selected == "radiofast":
-			radio_certanty = 4.2
-		elif radio_selected == "radiobalanced":
-			radio_certanty = 3.5
-		elif radio_selected == "radiosecure":
-			radio_certanty = 2.2
-
-		self.proc = subprocess.Popen("howdy set certainty " + str(radio_certanty), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
-
-		self.nextbutton.hide()
-		self.builder.get_object("cancelbutton").hide()
-
-		finishbutton = self.builder.get_object("finishbutton")
-		finishbutton.show()
-		self.window.set_focus(finishbutton)
-
-		status = self.proc.wait(2)
-
-		if status != 0:
-			self.show_error(_("Error setting certainty"), _("Certainty is set to the default value, Howdy setup is complete"))
-
-	def enable_next(self):
-		self.nextbutton.set_sensitive(True)
-		self.window.set_focus(self.nextbutton)
-
-	def show_error(self, error, secon=""):
-		dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, type=gtk.MessageType.ERROR, buttons=gtk.ButtonsType.CLOSE)
-		dialog.set_title(_("Howdy Error"))
-		dialog.props.text = error
-		dialog.format_secondary_text(secon)
-
-		dialog.run()
-
-		dialog.destroy()
-		self.exit()
-
-	def exit(self, widget=None, context=None):
-		"""Cleanly exit"""
-		gtk.main_quit()
-		sys.exit(0)
+    def __init__(self):
+        """Initialize the sticky window"""
+        # Make the class a GTK window
+        gtk.Window.__init__(self)
+
+        self.builder = gtk.Builder()
+        self.builder.add_from_file(paths_factory.onboarding_wireframe_path())
+        self.builder.connect_signals(self)
+
+        self.window = self.builder.get_object("onboardingwindow")
+        self.slidecontainer = self.builder.get_object("slidecontainer")
+        self.nextbutton = self.builder.get_object("nextbutton")
+
+        self.window.connect("destroy", self.exit)
+        self.window.connect("delete_event", self.exit)
+
+        self.slides = [
+            self.builder.get_object("slide0"),
+            self.builder.get_object("slide1"),
+            self.builder.get_object("slide2"),
+            self.builder.get_object("slide3"),
+            self.builder.get_object("slide4"),
+            self.builder.get_object("slide5"),
+            self.builder.get_object("slide6"),
+        ]
+
+        self.window.show_all()
+        self.window.resize(500, 400)
+
+        self.window.current_slide = 0
+
+        # Start GTK main loop
+        gtk.main()
+
+    def go_next_slide(self, button=None):
+        self.nextbutton.set_sensitive(False)
+
+        self.slides[self.window.current_slide].hide()
+        self.slides[self.window.current_slide + 1].show()
+        self.window.current_slide += 1
+        # the shown child may have zero/wrong dimensions
+        self.slidecontainer.queue_resize()
+
+        if self.window.current_slide == 1:
+            self.execute_slide1()
+        elif self.window.current_slide == 2:
+            gobject.timeout_add(10, self.execute_slide2)
+        elif self.window.current_slide == 3:
+            self.execute_slide3()
+        elif self.window.current_slide == 4:
+            self.execute_slide4()
+        elif self.window.current_slide == 5:
+            self.execute_slide5()
+        elif self.window.current_slide == 6:
+            self.execute_slide6()
+
+    def execute_slide1(self):
+        self.downloadoutputlabel = self.builder.get_object("downloadoutputlabel")
+        eventbox = self.builder.get_object("downloadeventbox")
+        eventbox.modify_bg(gtk.StateType.NORMAL, gdk.Color(red=0, green=0, blue=0))
+
+        # TODO: Better way to do this?
+        if os.path.exists(
+            paths_factory.dlib_data_dir_path() / "shape_predictor_5_face_landmarks.dat"
+        ):
+            self.downloadoutputlabel.set_text(
+                _("Datafiles have already been downloaded!\nClick Next to continue")
+            )
+            self.enable_next()
+            return
+
+        self.proc = subprocess.Popen(
+            "./install.sh",
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT,
+            shell=True,
+            cwd=paths_factory.dlib_data_dir_path(),
+        )
+
+        self.download_lines = []
+        self.read_download_line()
+
+    def read_download_line(self):
+        line = self.proc.stdout.readline()
+        self.download_lines.append(line.decode("utf-8"))
+
+        print("install.sh output:")
+        print(line.decode("utf-8"))
+
+        if len(self.download_lines) > 10:
+            self.download_lines.pop(0)
+
+        self.downloadoutputlabel.set_text(" ".join(self.download_lines))
+
+        if line:
+            gobject.timeout_add(10, self.read_download_line)
+            return
+
+        # Wait for the process to finish and check the status code
+        if self.proc.wait(5) != 0:
+            self.show_error(
+                _("Error while downloading datafiles"), " ".join(self.download_lines)
+            )
+
+        self.downloadoutputlabel.set_text(_("Done!\nClick Next to continue"))
+        self.enable_next()
+
+    def execute_slide2(self):
+        def is_gray(frame):
+            for row in frame:
+                for pixel in row:
+                    if not pixel[0] == pixel[1] == pixel[2]:
+                        return False
+            return True
+
+        try:
+            import cv2
+        except Exception:
+            self.show_error(
+                _("Error while importing OpenCV2"), _("Try reinstalling cv2")
+            )
+
+        device_rows = []
+        try:
+            device_ids = os.listdir("/dev/v4l/by-path")
+        except Exception:
+            self.show_error(
+                _("No webcams found on system"),
+                _(
+                    "Please configure your camera yourself if you are sure a compatible camera is connected"
+                ),
+            )
+
+        # Loop though all devices
+        for dev in device_ids:
+            time.sleep(0.5)
+
+            # The full path to the device is the default name
+            device_path = "/dev/v4l/by-path/" + dev
+            device_name = dev
+
+            # Get the udevadm details to try to get a better name
+            udevadm = subprocess.check_output(
+                ["udevadm info -r --query=all -n " + device_path], shell=True
+            ).decode("utf-8")
+
+            # Loop though udevadm to search for a better name
+            for line in udevadm.split("\n"):
+                # Match it and encase it in quotes
+                re_name = re.search("product.*=(.*)$", line, re.IGNORECASE)
+                if re_name:
+                    device_name = re_name.group(1)
+
+            capture = cv2.VideoCapture(device_path)
+            is_open, frame = capture.read()
+            if not is_open:
+                device_rows.append(
+                    [device_name, device_path, -9, _("No, camera can't be opened")]
+                )
+                continue
+
+            try:
+                if not is_gray(frame):
+                    raise Exception()
+            except Exception:
+                device_rows.append(
+                    [device_name, device_path, -5, _("No, not an infrared camera")]
+                )
+                capture.release()
+                continue
+
+            device_rows.append(
+                [device_name, device_path, 5, _("Yes, compatible infrared camera")]
+            )
+            capture.release()
+
+        device_rows = sorted(device_rows, key=lambda k: -k[2])
+
+        self.loadinglabel = self.builder.get_object("loadinglabel")
+        self.devicelistbox = self.builder.get_object("devicelistbox")
+
+        self.treeview = gtk.TreeView()
+        self.treeview.set_vexpand(True)
+
+        # Set the columns
+        for i, column in enumerate([_("Camera identifier or path"), _("Recommended")]):
+            cell = gtk.CellRendererText()
+            cell.set_property("ellipsize", pango.EllipsizeMode.END)
+            col = gtk.TreeViewColumn(column, cell, text=i)
+            self.treeview.append_column(col)
+
+        # Add the treeview
+        self.devicelistbox.add(self.treeview)
+
+        # Create a datamodel
+        self.listmodel = gtk.ListStore(str, str, str, bool)
+
+        for device in device_rows:
+            is_gray = device[2] == 5
+            self.listmodel.append([device[0], device[3], device[1], is_gray])
+
+        self.treeview.set_model(self.listmodel)
+        self.treeview.set_cursor(0)
+
+        self.treeview.show()
+        self.loadinglabel.hide()
+        self.enable_next()
+
+    def execute_slide3(self):
+        try:
+            import cv2
+        except Exception:
+            self.show_error(
+                _("Error while importing OpenCV2"), _("Try reinstalling cv2")
+            )
+
+        selection = self.treeview.get_selection()
+        (listmodel, rowlist) = selection.get_selected_rows()
+
+        if len(rowlist) != 1:
+            self.show_error(_("Error selecting camera"))
+
+        device_path = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
+        is_gray = listmodel.get_value(listmodel.get_iter(rowlist[0]), 3)
+
+        if is_gray:
+            # test if linux-enable-ir-emitter help should be displayed,
+            # the user must click on the yes/no button which calls the method slide3_button_yes|no
+            self.capture = cv2.VideoCapture(device_path)
+            if not self.capture.isOpened():
+                self.show_error(
+                    _("The selected camera cannot be opened"),
+                    _("Try to select another one"),
+                )
+            self.capture.read()
+        else:
+            # skip, the selected camera is not infrared
+            self.go_next_slide()
+
+    def slide3_button_yes(self, button):
+        self.capture.release()
+        self.go_next_slide()
+
+    def slide3_button_no(self, button):
+        self.capture.release()
+        self.builder.get_object("leiestatus").set_markup(
+            _(
+                'Please visit\n<a href="https://github.com/EmixamPP/linux-enable-ir-emitter">https://github.com/EmixamPP/linux-enable-ir-emitter</a>\nto enable your ir emitter'
+            )
+        )
+        self.builder.get_object("leieyesbutton").hide()
+        self.builder.get_object("leienobutton").hide()
+
+    def execute_slide4(self):
+        selection = self.treeview.get_selection()
+        (listmodel, rowlist) = selection.get_selected_rows()
+
+        if len(rowlist) != 1:
+            self.show_error(_("Error selecting camera"))
+
+        device_path = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
+        self.proc = subprocess.Popen(
+            "howdy set device_path " + device_path,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT,
+            shell=True,
+        )
+
+        self.window.set_focus(self.builder.get_object("scanbutton"))
+
+    def on_scanbutton_click(self, button):
+        status = self.proc.wait(2)
+
+        # if status != 0:
+        #     self.show_error(_("Error setting camera path"), _("Please set the camera path manually"))
+
+        self.dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL)
+        self.dialog.set_title(_("Creating Model"))
+        self.dialog.props.text = _("Please look directly into the camera")
+        self.dialog.show_all()
+
+        # Wait a bit to allow the user to read the dialog
+        gobject.timeout_add(600, self.run_add)
+
+    def run_add(self):
+        status, output = subprocess.getstatusoutput(["howdy add -y"])
+
+        print("howdy add output:")
+        print(output)
+
+        self.dialog.destroy()
+
+        if status != 0:
+            self.show_error(_("Can't save face model"), output)
+
+        gobject.timeout_add(10, self.go_next_slide)
+
+    def execute_slide5(self):
+        self.enable_next()
+
+    def execute_slide6(self):
+        radio_buttons = self.builder.get_object("radiobalanced").get_group()
+        radio_selected = False
+        radio_certanty = 5.0
+
+        for button in radio_buttons:
+            if button.get_active():
+                radio_selected = gtk.Buildable.get_name(button)
+
+        if not radio_selected:
+            self.show_error(_("Error reading radio buttons"))
+        elif radio_selected == "radiofast":
+            radio_certanty = 4.2
+        elif radio_selected == "radiobalanced":
+            radio_certanty = 3.5
+        elif radio_selected == "radiosecure":
+            radio_certanty = 2.2
+
+        self.proc = subprocess.Popen(
+            "howdy set certainty " + str(radio_certanty),
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT,
+            shell=True,
+        )
+
+        self.nextbutton.hide()
+        self.builder.get_object("cancelbutton").hide()
+
+        finishbutton = self.builder.get_object("finishbutton")
+        finishbutton.show()
+        self.window.set_focus(finishbutton)
+
+        status = self.proc.wait(2)
+
+        if status != 0:
+            self.show_error(
+                _("Error setting certainty"),
+                _("Certainty is set to the default value, Howdy setup is complete"),
+            )
+
+    def enable_next(self):
+        self.nextbutton.set_sensitive(True)
+        self.window.set_focus(self.nextbutton)
+
+    def show_error(self, error, secon=""):
+        dialog = gtk.MessageDialog(
+            parent=self,
+            flags=gtk.DialogFlags.MODAL,
+            type=gtk.MessageType.ERROR,
+            buttons=gtk.ButtonsType.CLOSE,
+        )
+        dialog.set_title(_("Howdy Error"))
+        dialog.props.text = error
+        dialog.format_secondary_text(secon)
+
+        dialog.run()
+
+        dialog.destroy()
+        self.exit()
+
+    def exit(self, widget=None, context=None):
+        """Cleanly exit"""
+        gtk.main_quit()
+        sys.exit(0)
diff --git a/howdy-gtk/src/tab_models.py b/howdy-gtk/src/tab_models.py
index e391d7f..cbb12ec 100644
--- a/howdy-gtk/src/tab_models.py
+++ b/howdy-gtk/src/tab_models.py
@@ -7,118 +7,155 @@ from gi.repository import GObject as gobject
 
 
 def on_user_change(self, select):
-	self.active_user = select.get_active_text()
-	self.load_model_list()
+    self.active_user = select.get_active_text()
+    self.load_model_list()
 
 
 def on_user_add(self, button):
-	# Open question dialog
-	dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, type=gtk.MessageType.QUESTION, buttons=gtk.ButtonsType.OK_CANCEL)
-	dialog.set_title(_("Confirm User Creation"))
-	dialog.props.text = _("Please enter the username of the user you want to add to Howdy")
+    # Open question dialog
+    dialog = gtk.MessageDialog(
+        parent=self,
+        flags=gtk.DialogFlags.MODAL,
+        type=gtk.MessageType.QUESTION,
+        buttons=gtk.ButtonsType.OK_CANCEL,
+    )
+    dialog.set_title(_("Confirm User Creation"))
+    dialog.props.text = _(
+        "Please enter the username of the user you want to add to Howdy"
+    )
 
-	# Create the input field
-	entry = gtk.Entry()
+    # Create the input field
+    entry = gtk.Entry()
 
-	# Add a label to ask for a model name
-	hbox = gtk.HBox()
-	hbox.pack_start(gtk.Label(_("Username:")), False, 5, 5)
-	hbox.pack_end(entry, True, True, 5)
+    # Add a label to ask for a model name
+    hbox = gtk.HBox()
+    hbox.pack_start(gtk.Label(_("Username:")), False, 5, 5)
+    hbox.pack_end(entry, True, True, 5)
 
-	# Add the box and show the dialog
-	dialog.vbox.pack_end(hbox, True, True, 0)
-	dialog.show_all()
+    # Add the box and show the dialog
+    dialog.vbox.pack_end(hbox, True, True, 0)
+    dialog.show_all()
 
-	# Show dialog
-	response = dialog.run()
+    # Show dialog
+    response = dialog.run()
 
-	entered_user = entry.get_text()
-	dialog.destroy()
+    entered_user = entry.get_text()
+    dialog.destroy()
 
-	if response == gtk.ResponseType.OK:
-		self.userlist.append_text(entered_user)
-		self.userlist.set_active(self.userlist.items)
-		self.userlist.items += 1
+    if response == gtk.ResponseType.OK:
+        self.userlist.append_text(entered_user)
+        self.userlist.set_active(self.userlist.items)
+        self.userlist.items += 1
 
-		self.active_user = entered_user
-		self.load_model_list()
+        self.active_user = entered_user
+        self.load_model_list()
 
 
 def on_model_add(self, button):
-	if self.userlist.items == 0:
-		return
-	# Open question dialog
-	dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, type=gtk.MessageType.QUESTION, buttons=gtk.ButtonsType.OK_CANCEL)
-	dialog.set_title(_("Confirm Model Creation"))
-	dialog.props.text = _("Please enter a name for the new model, 24 characters max")
-
-	# Create the input field
-	entry = gtk.Entry()
-
-	# Add a label to ask for a model name
-	hbox = gtk.HBox()
-	hbox.pack_start(gtk.Label(_("Model name:")), False, 5, 5)
-	hbox.pack_end(entry, True, True, 5)
-
-	# Add the box and show the dialog
-	dialog.vbox.pack_end(hbox, True, True, 0)
-	dialog.show_all()
-
-	# Show dialog
-	response = dialog.run()
-
-	entered_name = entry.get_text()
-	dialog.destroy()
-
-	if response == gtk.ResponseType.OK:
-		dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, buttons=gtk.ButtonsType.NONE)
-		dialog.set_title(_("Creating Model"))
-		dialog.props.text = _("Please look directly into the camera")
-		dialog.show_all()
-
-		# Wait a bit to allow the user to read the dialog
-		gobject.timeout_add(600, lambda: execute_add(self, dialog, entered_name))
+    if self.userlist.items == 0:
+        return
+    # Open question dialog
+    dialog = gtk.MessageDialog(
+        parent=self,
+        flags=gtk.DialogFlags.MODAL,
+        type=gtk.MessageType.QUESTION,
+        buttons=gtk.ButtonsType.OK_CANCEL,
+    )
+    dialog.set_title(_("Confirm Model Creation"))
+    dialog.props.text = _("Please enter a name for the new model, 24 characters max")
+
+    # Create the input field
+    entry = gtk.Entry()
+
+    # Add a label to ask for a model name
+    hbox = gtk.HBox()
+    hbox.pack_start(gtk.Label(_("Model name:")), False, 5, 5)
+    hbox.pack_end(entry, True, True, 5)
+
+    # Add the box and show the dialog
+    dialog.vbox.pack_end(hbox, True, True, 0)
+    dialog.show_all()
+
+    # Show dialog
+    response = dialog.run()
+
+    entered_name = entry.get_text()
+    dialog.destroy()
+
+    if response == gtk.ResponseType.OK:
+        dialog = gtk.MessageDialog(
+            parent=self, flags=gtk.DialogFlags.MODAL, buttons=gtk.ButtonsType.NONE
+        )
+        dialog.set_title(_("Creating Model"))
+        dialog.props.text = _("Please look directly into the camera")
+        dialog.show_all()
+
+        # Wait a bit to allow the user to read the dialog
+        gobject.timeout_add(600, lambda: execute_add(self, dialog, entered_name))
 
 
 def execute_add(box, dialog, entered_name):
 
-	status, output = subprocess.getstatusoutput(["howdy add '" + entered_name + "' -y -U " + box.active_user])
+    status, output = subprocess.getstatusoutput(
+        ["howdy add '" + entered_name + "' -y -U " + box.active_user]
+    )
+
+    dialog.destroy()
 
-	dialog.destroy()
+    if status != 0:
+        dialog = gtk.MessageDialog(
+            parent=box,
+            flags=gtk.DialogFlags.MODAL,
+            type=gtk.MessageType.ERROR,
+            buttons=gtk.ButtonsType.CLOSE,
+        )
+        dialog.set_title(_("Howdy Error"))
+        dialog.props.text = _("Error while adding model, error code {}: \n\n").format(
+            str(status)
+        )
+        dialog.format_secondary_text(output)
+        dialog.run()
+        dialog.destroy()
 
-	if status != 0:
-		dialog = gtk.MessageDialog(parent=box, flags=gtk.DialogFlags.MODAL, type=gtk.MessageType.ERROR, buttons=gtk.ButtonsType.CLOSE)
-		dialog.set_title(_("Howdy Error"))
-		dialog.props.text = _("Error while adding model, error code {}: \n\n").format(str(status))
-		dialog.format_secondary_text(output)
-		dialog.run()
-		dialog.destroy()
+    box.load_model_list()
 
-	box.load_model_list()
 
 def on_model_delete(self, button):
-	selection = self.treeview.get_selection()
-	(listmodel, rowlist) = selection.get_selected_rows()
-
-	if len(rowlist) == 1:
-		id = listmodel.get_value(listmodel.get_iter(rowlist[0]), 0)
-		name = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
-
-		dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, buttons=gtk.ButtonsType.OK_CANCEL)
-		dialog.set_title(_("Confirm Model Deletion"))
-		dialog.props.text = _("Are you sure you want to delete model {id} ({name})?").format(id=id, name=name)
-		response = dialog.run()
-		dialog.destroy()
-
-		if response == gtk.ResponseType.OK:
-			status, output = subprocess.getstatusoutput(["howdy remove " + id + " -y -U " + self.active_user])
-
-			if status != 0:
-				dialog = gtk.MessageDialog(parent=self, flags=gtk.DialogFlags.MODAL, type=gtk.MessageType.ERROR, buttons=gtk.ButtonsType.CLOSE)
-				dialog.set_title(_("Howdy Error"))
-				dialog.props.text = _("Error while deleting model, error code {}: \n\n").format(status)
-				dialog.format_secondary_text(output)
-				dialog.run()
-				dialog.destroy()
-
-			self.load_model_list()
+    selection = self.treeview.get_selection()
+    (listmodel, rowlist) = selection.get_selected_rows()
+
+    if len(rowlist) == 1:
+        id = listmodel.get_value(listmodel.get_iter(rowlist[0]), 0)
+        name = listmodel.get_value(listmodel.get_iter(rowlist[0]), 2)
+
+        dialog = gtk.MessageDialog(
+            parent=self, flags=gtk.DialogFlags.MODAL, buttons=gtk.ButtonsType.OK_CANCEL
+        )
+        dialog.set_title(_("Confirm Model Deletion"))
+        dialog.props.text = _(
+            "Are you sure you want to delete model {id} ({name})?"
+        ).format(id=id, name=name)
+        response = dialog.run()
+        dialog.destroy()
+
+        if response == gtk.ResponseType.OK:
+            status, output = subprocess.getstatusoutput(
+                ["howdy remove " + id + " -y -U " + self.active_user]
+            )
+
+            if status != 0:
+                dialog = gtk.MessageDialog(
+                    parent=self,
+                    flags=gtk.DialogFlags.MODAL,
+                    type=gtk.MessageType.ERROR,
+                    buttons=gtk.ButtonsType.CLOSE,
+                )
+                dialog.set_title(_("Howdy Error"))
+                dialog.props.text = _(
+                    "Error while deleting model, error code {}: \n\n"
+                ).format(status)
+                dialog.format_secondary_text(output)
+                dialog.run()
+                dialog.destroy()
+
+            self.load_model_list()
diff --git a/howdy-gtk/src/tab_video.py b/howdy-gtk/src/tab_video.py
index e194a15..8b156df 100644
--- a/howdy-gtk/src/tab_video.py
+++ b/howdy-gtk/src/tab_video.py
@@ -13,69 +13,82 @@ MAX_WIDTH = 300
 
 
 def on_page_switch(self, notebook, page, page_num):
-	if page_num == 1:
+    if page_num == 1:
 
-		try:
-			self.config = configparser.ConfigParser()
-			self.config.read(paths_factory.config_file_path())
-		except Exception:
-			print(_("Can't open camera"))
+        try:
+            self.config = configparser.ConfigParser()
+            self.config.read(paths_factory.config_file_path())
+        except Exception:
+            print(_("Can't open camera"))
 
-		path = self.config.get("video", "device_path")
+        path = self.config.get("video", "device_path")
 
-		try:
-			# if not self.cv2:
-			import cv2
-			self.cv2 = cv2
-		except Exception:
-			print(_("Can't import OpenCV2"))
+        try:
+            # if not self.cv2:
+            import cv2
 
-		try:
-			self.capture = cv2.VideoCapture(path)
-		except Exception:
-			print(_("Can't open camera"))
+            self.cv2 = cv2
+        except Exception:
+            print(_("Can't import OpenCV2"))
 
-		opencvbox = self.builder.get_object("opencvbox")
-		opencvbox.modify_bg(gtk.StateType.NORMAL, gdk.Color(red=0, green=0, blue=0))
+        try:
+            self.capture = cv2.VideoCapture(path)
+        except Exception:
+            print(_("Can't open camera"))
 
-		height = self.capture.get(self.cv2.CAP_PROP_FRAME_HEIGHT) or 1
-		width = self.capture.get(self.cv2.CAP_PROP_FRAME_WIDTH) or 1
+        opencvbox = self.builder.get_object("opencvbox")
+        opencvbox.modify_bg(gtk.StateType.NORMAL, gdk.Color(red=0, green=0, blue=0))
 
-		self.scaling_factor = (MAX_HEIGHT / height) or 1
+        height = self.capture.get(self.cv2.CAP_PROP_FRAME_HEIGHT) or 1
+        width = self.capture.get(self.cv2.CAP_PROP_FRAME_WIDTH) or 1
 
-		if width * self.scaling_factor > MAX_WIDTH:
-			self.scaling_factor = (MAX_WIDTH / width) or 1
+        self.scaling_factor = (MAX_HEIGHT / height) or 1
 
-		config_height = self.config.getfloat("video", "max_height", fallback=320.0)
-		config_scaling = (config_height / height) or 1
+        if width * self.scaling_factor > MAX_WIDTH:
+            self.scaling_factor = (MAX_WIDTH / width) or 1
 
-		self.builder.get_object("videoid").set_text(path.split("/")[-1])
-		self.builder.get_object("videores").set_text(str(int(width)) + "x" + str(int(height)))
-		self.builder.get_object("videoresused").set_text(str(int(width * config_scaling)) + "x" + str(int(height * config_scaling)))
-		self.builder.get_object("videorecorder").set_text(self.config.get("video", "recording_plugin", fallback=_("Unknown")))
+        config_height = self.config.getfloat("video", "max_height", fallback=320.0)
+        config_scaling = (config_height / height) or 1
 
-		gobject.timeout_add(10, self.capture_frame)
+        self.builder.get_object("videoid").set_text(path.split("/")[-1])
+        self.builder.get_object("videores").set_text(
+            str(int(width)) + "x" + str(int(height))
+        )
+        self.builder.get_object("videoresused").set_text(
+            str(int(width * config_scaling)) + "x" + str(int(height * config_scaling))
+        )
+        self.builder.get_object("videorecorder").set_text(
+            self.config.get("video", "recording_plugin", fallback=_("Unknown"))
+        )
 
-	elif self.capture is not None:
-		self.capture.release()
-		self.capture = None
+        gobject.timeout_add(10, self.capture_frame)
+
+    elif self.capture is not None:
+        self.capture.release()
+        self.capture = None
 
 
 def capture_frame(self):
-	if self.capture is None:
-		return
+    if self.capture is None:
+        return
 
-	ret, frame = self.capture.read()
+    ret, frame = self.capture.read()
 
-	frame = self.cv2.resize(frame, None, fx=self.scaling_factor, fy=self.scaling_factor, interpolation=self.cv2.INTER_AREA)
+    frame = self.cv2.resize(
+        frame,
+        None,
+        fx=self.scaling_factor,
+        fy=self.scaling_factor,
+        interpolation=self.cv2.INTER_AREA,
+    )
 
-	retval, buffer = self.cv2.imencode(".png", frame)
+    retval, buffer = self.cv2.imencode(".png", frame)
 
-	loader = pixbuf.PixbufLoader()
-	loader.write(buffer)
-	loader.close()
-	buffer = loader.get_pixbuf()
+    loader = pixbuf.PixbufLoader()
+    loader.write(buffer)
+    loader.close()
+    buffer = loader.get_pixbuf()
 
-	self.opencvimage.set_from_pixbuf(buffer)
+    self.opencvimage.set_from_pixbuf(buffer)
 
-	gobject.timeout_add(20, self.capture_frame)
+    gobject.timeout_add(20, self.capture_frame)
diff --git a/howdy-gtk/src/window.py b/howdy-gtk/src/window.py
index 4bc13eb..df6eec6 100644
--- a/howdy-gtk/src/window.py
+++ b/howdy-gtk/src/window.py
@@ -18,100 +18,104 @@ from gi.repository import Gtk as gtk
 
 
 class MainWindow(gtk.Window):
-	def __init__(self):
-		"""Initialize the sticky window"""
-		# Make the class a GTK window
-		gtk.Window.__init__(self)
+    def __init__(self):
+        """Initialize the sticky window"""
+        # Make the class a GTK window
+        gtk.Window.__init__(self)
 
-		self.builder = gtk.Builder()
-		self.builder.add_from_file(paths_factory.main_window_wireframe_path())
-		self.builder.connect_signals(self)
+        self.builder = gtk.Builder()
+        self.builder.add_from_file(paths_factory.main_window_wireframe_path())
+        self.builder.connect_signals(self)
 
-		self.window = self.builder.get_object("mainwindow")
-		self.userlist = self.builder.get_object("userlist")
-		self.modellistbox = self.builder.get_object("modellistbox")
-		self.opencvimage = self.builder.get_object("opencvimage")
+        self.window = self.builder.get_object("mainwindow")
+        self.userlist = self.builder.get_object("userlist")
+        self.modellistbox = self.builder.get_object("modellistbox")
+        self.opencvimage = self.builder.get_object("opencvimage")
 
-		self.window.connect("destroy", self.exit)
-		self.window.connect("delete_event", self.exit)
+        self.window.connect("destroy", self.exit)
+        self.window.connect("delete_event", self.exit)
 
-		# Init capture for video tab
-		self.capture = None
+        # Init capture for video tab
+        self.capture = None
 
-		# Create a treeview that will list the model data
-		self.treeview = gtk.TreeView()
-		self.treeview.set_vexpand(True)
+        # Create a treeview that will list the model data
+        self.treeview = gtk.TreeView()
+        self.treeview.set_vexpand(True)
 
-		# Set the columns
-		for i, column in enumerate([_("ID"), _("Created"), _("Label")]):
-			col = gtk.TreeViewColumn(column, gtk.CellRendererText(), text=i)
-			self.treeview.append_column(col)
+        # Set the columns
+        for i, column in enumerate([_("ID"), _("Created"), _("Label")]):
+            col = gtk.TreeViewColumn(column, gtk.CellRendererText(), text=i)
+            self.treeview.append_column(col)
 
-		# Add the treeview
-		self.modellistbox.add(self.treeview)
+        # Add the treeview
+        self.modellistbox.add(self.treeview)
 
-		filelist = os.listdir(paths_factory.user_models_dir_path())
-		self.active_user = ""
+        filelist = os.listdir(paths_factory.user_models_dir_path())
+        self.active_user = ""
 
-		self.userlist.items = 0
+        self.userlist.items = 0
 
-		for file in filelist:
-			self.userlist.append_text(file[:-4])
-			self.userlist.items += 1
+        for file in filelist:
+            self.userlist.append_text(file[:-4])
+            self.userlist.items += 1
 
-			if not self.active_user:
-				self.active_user = file[:-4]
+            if not self.active_user:
+                self.active_user = file[:-4]
 
-		self.userlist.set_active(0)
+        self.userlist.set_active(0)
 
-		self.window.show_all()
+        self.window.show_all()
 
-		# Start GTK main loop
-		gtk.main()
+        # Start GTK main loop
+        gtk.main()
 
-	def load_model_list(self):
-		"""(Re)load the model list"""
+    def load_model_list(self):
+        """(Re)load the model list"""
 
-		# Get username and default to none if there are no models at all yet
-		user = 'none'
-		if self.active_user: user = self.active_user
+        # Get username and default to none if there are no models at all yet
+        user = "none"
+        if self.active_user:
+            user = self.active_user
 
-		# Execute the list command to get the models
-		status, output = subprocess.getstatusoutput(["howdy list --plain -U " + user])
+        # Execute the list command to get the models
+        status, output = subprocess.getstatusoutput(["howdy list --plain -U " + user])
 
-		# Create a datamodel
-		self.listmodel = gtk.ListStore(str, str, str)
+        # Create a datamodel
+        self.listmodel = gtk.ListStore(str, str, str)
 
-		# If there was no error
-		if status == 0:
-			# Split the output per line
-			lines = output.split("\n")
+        # If there was no error
+        if status == 0:
+            # Split the output per line
+            lines = output.split("\n")
 
-			# Add the models to the datamodel
-			for i in range(len(lines)):
-				items = lines[i].split(",")
-				if len(items) < 3: continue
-				self.listmodel.append(items)
+            # Add the models to the datamodel
+            for i in range(len(lines)):
+                items = lines[i].split(",")
+                if len(items) < 3:
+                    continue
+                self.listmodel.append(items)
 
-		self.treeview.set_model(self.listmodel)
+        self.treeview.set_model(self.listmodel)
 
-	def on_about_link(self, label, uri):
-		"""Open links on about page as a non-root user"""
-		try:
-			user = os.getlogin()
-		except Exception:
-			user = os.environ.get("SUDO_USER")
+    def on_about_link(self, label, uri):
+        """Open links on about page as a non-root user"""
+        try:
+            user = os.getlogin()
+        except Exception:
+            user = os.environ.get("SUDO_USER")
 
-		status, output = subprocess.getstatusoutput(["sudo -u " + user + " timeout 10 xdg-open " + uri])
-		return True
+        status, output = subprocess.getstatusoutput(
+            ["sudo -u " + user + " timeout 10 xdg-open " + uri]
+        )
+        return True
 
-	def exit(self, widget=None, context=None):
-		"""Cleanly exit"""
-		if self.capture is not None:
-			self.capture.release()
+    def exit(self, widget=None, context=None):
+        """Cleanly exit"""
+        if self.capture is not None:
+            self.capture.release()
 
-		gtk.main_quit()
-		sys.exit(0)
+        gtk.main_quit()
+        sys.exit(0)
 
 
 # Make sure we quit on a SIGINT
@@ -121,19 +125,24 @@ signal.signal(signal.SIGINT, signal.SIG_DFL)
 elevate.elevate()
 
 # If no models have been created yet or when it is forced, start the onboarding
-if "--force-onboarding" in sys.argv or not os.path.exists(paths_factory.user_models_dir_path()):
-	import onboarding
-	onboarding.OnboardingWindow()
+if "--force-onboarding" in sys.argv or not os.path.exists(
+    paths_factory.user_models_dir_path()
+):
+    import onboarding
 
-	sys.exit(0)
+    onboarding.OnboardingWindow()
+
+    sys.exit(0)
 
 # Class is split so it isn't too long, import split functions
 import tab_models
+
 MainWindow.on_user_add = tab_models.on_user_add
 MainWindow.on_user_change = tab_models.on_user_change
 MainWindow.on_model_add = tab_models.on_model_add
 MainWindow.on_model_delete = tab_models.on_model_delete
 import tab_video
+
 MainWindow.on_page_switch = tab_video.on_page_switch
 MainWindow.capture_frame = tab_video.capture_frame
 
diff --git a/howdy/src/cli.py b/howdy/src/cli.py
index 2a4b33b..c2bda94 100755
--- a/howdy/src/cli.py
+++ b/howdy/src/cli.py
@@ -25,56 +25,71 @@ if user == "":
 
 # Basic command setup
 parser = argparse.ArgumentParser(
-	description=_("Command line interface for Howdy face authentication."),
-	formatter_class=argparse.RawDescriptionHelpFormatter,
-	add_help=False,
-	prog="howdy",
-	usage="howdy [-U USER] [--plain] [-h] [-y] {command} [{arguments}...]".format(command=_("command"), arguments=_("arguments")),
-	epilog=_("For support please visit\nhttps://github.com/boltgolt/howdy"))
+    description=_("Command line interface for Howdy face authentication."),
+    formatter_class=argparse.RawDescriptionHelpFormatter,
+    add_help=False,
+    prog="howdy",
+    usage="howdy [-U USER] [--plain] [-h] [-y] {command} [{arguments}...]".format(
+        command=_("command"), arguments=_("arguments")
+    ),
+    epilog=_("For support please visit\nhttps://github.com/boltgolt/howdy"),
+)
 
 # Add an argument for the command
 parser.add_argument(
-	"command",
-	help=_("The command option to execute, can be one of the following: add, clear, config, disable, list, remove, snapshot, set, test or version."),
-	metavar="command",
-	choices=["add", "clear", "config", "disable", "list", "remove", "set", "snapshot", "test", "version"])
+    "command",
+    help=_(
+        "The command option to execute, can be one of the following: add, clear, config, disable, list, remove, snapshot, set, test or version."
+    ),
+    metavar="command",
+    choices=[
+        "add",
+        "clear",
+        "config",
+        "disable",
+        "list",
+        "remove",
+        "set",
+        "snapshot",
+        "test",
+        "version",
+    ],
+)
 
 # Add an argument for the extra arguments of disable and remove
 parser.add_argument(
-	"arguments",
-	help=_("Optional arguments for the add, disable, remove and set commands."),
-	nargs="*")
+    "arguments",
+    help=_("Optional arguments for the add, disable, remove and set commands."),
+    nargs="*",
+)
 
 # Add the user flag
 parser.add_argument(
-	"-U", "--user",
-	default=user,
-	help=_("Set the user account to use."))
+    "-U", "--user", default=user, help=_("Set the user account to use.")
+)
 
 # Add the -y flag
-parser.add_argument(
-	"-y",
-	help=_("Skip all questions."),
-	action="store_true")
+parser.add_argument("-y", help=_("Skip all questions."), action="store_true")
 
 # Add the --plain flag
 parser.add_argument(
-	"--plain",
-	help=_("Print machine-friendly output."),
-	action="store_true")
+    "--plain", help=_("Print machine-friendly output."), action="store_true"
+)
 
 # Overwrite the default help message so we can use a uppercase S
 parser.add_argument(
-	"-h", "--help",
-	action="help",
-	default=argparse.SUPPRESS,
-	help=_("Show this help message and exit."))
+    "-h",
+    "--help",
+    action="help",
+    default=argparse.SUPPRESS,
+    help=_("Show this help message and exit."),
+)
 
 # If we only have 1 argument we print the help text
 if len(sys.argv) < 2:
-	print(_("current active user: ") + user + "\n")
-	parser.print_help()
-	sys.exit(0)
+    print(_("current active user: ") + user + "\n")
+    parser.print_help()
+    sys.exit(0)
 
 # Parse all arguments above
 args = parser.parse_args()
@@ -86,33 +101,37 @@ builtins.howdy_user = args.user
 # Check if we have rootish rights
 # This is this far down the file so running the command for help is always possible
 if os.geteuid() != 0:
-	print(_("Please run this command as root:\n"))
-	print("\tsudo howdy " + " ".join(sys.argv[1:]))
-	sys.exit(1)
+    print(_("Please run this command as root:\n"))
+    print("\tsudo howdy " + " ".join(sys.argv[1:]))
+    sys.exit(1)
 
 # Beyond this point the user can't change anymore, if we still have root as user we need to abort
 if args.user == "root":
-	print(_("Can't run howdy commands as root, please run this command with the --user flag"))
-	sys.exit(1)
+    print(
+        _(
+            "Can't run howdy commands as root, please run this command with the --user flag"
+        )
+    )
+    sys.exit(1)
 
 # Execute the right command
 if args.command == "add":
-	import cli.add
+    import cli.add
 elif args.command == "clear":
-	import cli.clear
+    import cli.clear
 elif args.command == "config":
-	import cli.config
+    import cli.config
 elif args.command == "disable":
-	import cli.disable
+    import cli.disable
 elif args.command == "list":
-	import cli.list
+    import cli.list
 elif args.command == "remove":
-	import cli.remove
+    import cli.remove
 elif args.command == "set":
-	import cli.set
+    import cli.set
 elif args.command == "snapshot":
-	import cli.snap
+    import cli.snap
 elif args.command == "test":
-	import cli.test
+    import cli.test
 else:
-	print("Howdy 3.0.0 BETA")
+    print("Howdy 3.0.0 BETA")
diff --git a/howdy/src/cli/add.py b/howdy/src/cli/add.py
index 9a2c9a0..e703634 100644
--- a/howdy/src/cli/add.py
+++ b/howdy/src/cli/add.py
@@ -16,23 +16,23 @@ from i18n import _
 # Try to import dlib and give a nice error if we can't
 # Add should be the first point where import issues show up
 try:
-	import dlib
+    import dlib
 except ImportError as err:
-	print(err)
+    print(err)
 
-	print(_("\nCan't import the dlib module, check the output of"))
-	print("pip3 show dlib")
-	sys.exit(1)
+    print(_("\nCan't import the dlib module, check the output of"))
+    print("pip3 show dlib")
+    sys.exit(1)
 
 # OpenCV needs to be imported after dlib
 import cv2
 
 # Test if at lest 1 of the data files is there and abort if it's not
 if not os.path.isfile(paths_factory.shape_predictor_5_face_landmarks_path()):
-	print(_("Data files have not been downloaded, please run the following commands:"))
-	print("\n\tcd " + paths_factory.dlib_data_dir_path())
-	print("\tsudo ./install.sh\n")
-	sys.exit(1)
+    print(_("Data files have not been downloaded, please run the following commands:"))
+    print("\n\tcd " + paths_factory.dlib_data_dir_path())
+    print("\tsudo ./install.sh\n")
+    sys.exit(1)
 
 # Read config from disk
 config = configparser.ConfigParser()
@@ -40,12 +40,18 @@ config.read(paths_factory.config_file_path())
 
 use_cnn = config.getboolean("core", "use_cnn", fallback=False)
 if use_cnn:
-	face_detector = dlib.cnn_face_detection_model_v1(paths_factory.mmod_human_face_detector_path())
+    face_detector = dlib.cnn_face_detection_model_v1(
+        paths_factory.mmod_human_face_detector_path()
+    )
 else:
-	face_detector = dlib.get_frontal_face_detector()
+    face_detector = dlib.get_frontal_face_detector()
 
-pose_predictor = dlib.shape_predictor(paths_factory.shape_predictor_5_face_landmarks_path())
-face_encoder = dlib.face_recognition_model_v1(paths_factory.dlib_face_recognition_resnet_model_v1_path())
+pose_predictor = dlib.shape_predictor(
+    paths_factory.shape_predictor_5_face_landmarks_path()
+)
+face_encoder = dlib.face_recognition_model_v1(
+    paths_factory.dlib_face_recognition_resnet_model_v1_path()
+)
 
 user = builtins.howdy_user
 # The permanent file to store the encoded model in
@@ -55,23 +61,27 @@ encodings = []
 
 # Make the ./models folder if it doesn't already exist
 if not os.path.exists(paths_factory.user_models_dir_path()):
-	print(_("No face model folder found, creating one"))
-	os.makedirs(paths_factory.user_models_dir_path())
+    print(_("No face model folder found, creating one"))
+    os.makedirs(paths_factory.user_models_dir_path())
 
 # To try read a premade encodings file if it exists
 try:
-	encodings = json.load(open(enc_file))
+    encodings = json.load(open(enc_file))
 except FileNotFoundError:
-	encodings = []
+    encodings = []
 
 # Print a warning if too many encodings are being added
 if len(encodings) > 3:
-	print(_("NOTICE: Each additional model slows down the face recognition engine slightly"))
-	print(_("Press Ctrl+C to cancel\n"))
+    print(
+        _(
+            "NOTICE: Each additional model slows down the face recognition engine slightly"
+        )
+    )
+    print(_("Press Ctrl+C to cancel\n"))
 
 # Make clear what we are doing if not human
 if not builtins.howdy_args.plain:
-	print(_("Adding face model for the user ") + user)
+    print(_("Adding face model for the user ") + user)
 
 # Set the default label
 label = "Initial model"
@@ -81,35 +91,30 @@ next_id = encodings[-1]["id"] + 1 if encodings else 0
 
 # Get the label from the cli arguments if provided
 if builtins.howdy_args.arguments:
-	label = builtins.howdy_args.arguments[0]
+    label = builtins.howdy_args.arguments[0]
 
 # Or set the default label
 else:
-	label = _("Model #") + str(next_id)
+    label = _("Model #") + str(next_id)
 
 # Keep de default name if we can't ask questions
 if builtins.howdy_args.y:
-	print(_('Using default label "%s" because of -y flag') % (label, ))
+    print(_('Using default label "%s" because of -y flag') % (label,))
 else:
-	# Ask the user for a custom label
-	label_in = input(_("Enter a label for this new model [{}]: ").format(label))
+    # Ask the user for a custom label
+    label_in = input(_("Enter a label for this new model [{}]: ").format(label))
 
-	# Set the custom label (if any) and limit it to 24 characters
-	if label_in != "":
-		label = label_in[:24]
+    # Set the custom label (if any) and limit it to 24 characters
+    if label_in != "":
+        label = label_in[:24]
 
 # Remove illegal characters
 if "," in label:
-	print(_("NOTICE: Removing illegal character \",\" from model name"))
-	label = label.replace(",", "")
+    print(_('NOTICE: Removing illegal character "," from model name'))
+    label = label.replace(",", "")
 
 # Prepare the metadata for insertion
-insert_model = {
-	"time": int(time.time()),
-	"label": label,
-	"id": next_id,
-	"data": []
-}
+insert_model = {"time": int(time.time()), "label": label, "id": next_id, "data": []}
 
 # Set up video_capture
 video_capture = VideoCapture(config)
@@ -138,63 +143,68 @@ clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
 
 # Loop through frames till we hit a timeout
 while frames < 60:
-	frames += 1
-	# Grab a single frame of video
-	frame, gsframe = video_capture.read_frame()
-	gsframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
-	gsframe = clahe.apply(gsframe)
-
-	# Create a histogram of the image with 8 values
-	hist = cv2.calcHist([gsframe], [0], None, [8], [0, 256])
-	# All values combined for percentage calculation
-	hist_total = np.sum(hist)
-
-	# Calculate frame darkness
-	darkness = (hist[0] / hist_total * 100)
-
-	# If the image is fully black due to a bad camera read,
-	# skip to the next frame
-	if (hist_total == 0) or (darkness == 100):
-		continue
-
-	# Include this frame in calculating our average session brightness
-	dark_running_total += darkness
-	valid_frames += 1
-
-	# If the image exceeds darkness threshold due to subject distance,
-	# skip to the next frame
-	if (darkness > dark_threshold):
-		dark_tries += 1
-		continue
-
-	# Get all faces from that frame as encodings
-	face_locations = face_detector(gsframe, 1)
-
-	# If we've found at least one, we can continue
-	if face_locations:
-		break
+    frames += 1
+    # Grab a single frame of video
+    frame, gsframe = video_capture.read_frame()
+    gsframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
+    gsframe = clahe.apply(gsframe)
+
+    # Create a histogram of the image with 8 values
+    hist = cv2.calcHist([gsframe], [0], None, [8], [0, 256])
+    # All values combined for percentage calculation
+    hist_total = np.sum(hist)
+
+    # Calculate frame darkness
+    darkness = hist[0] / hist_total * 100
+
+    # If the image is fully black due to a bad camera read,
+    # skip to the next frame
+    if (hist_total == 0) or (darkness == 100):
+        continue
+
+    # Include this frame in calculating our average session brightness
+    dark_running_total += darkness
+    valid_frames += 1
+
+    # If the image exceeds darkness threshold due to subject distance,
+    # skip to the next frame
+    if darkness > dark_threshold:
+        dark_tries += 1
+        continue
+
+    # Get all faces from that frame as encodings
+    face_locations = face_detector(gsframe, 1)
+
+    # If we've found at least one, we can continue
+    if face_locations:
+        break
 
 video_capture.release()
 
 # If we've found no faces, try to determine why
 if not face_locations:
-	if valid_frames == 0:
-		print(_("Camera saw only black frames - is IR emitter working?"))
-	elif valid_frames == dark_tries:
-		print(_("All frames were too dark, please check dark_threshold in config"))
-		print(_("Average darkness: {avg}, Threshold: {threshold}").format(avg=str(dark_running_total / valid_frames), threshold=str(dark_threshold)))
-	else:
-		print(_("No face detected, aborting"))
-	sys.exit(1)
+    if valid_frames == 0:
+        print(_("Camera saw only black frames - is IR emitter working?"))
+    elif valid_frames == dark_tries:
+        print(_("All frames were too dark, please check dark_threshold in config"))
+        print(
+            _("Average darkness: {avg}, Threshold: {threshold}").format(
+                avg=str(dark_running_total / valid_frames),
+                threshold=str(dark_threshold),
+            )
+        )
+    else:
+        print(_("No face detected, aborting"))
+    sys.exit(1)
 
 # If more than 1 faces are detected we can't know which one belongs to the user
 elif len(face_locations) > 1:
-	print(_("Multiple faces detected, aborting"))
-	sys.exit(1)
+    print(_("Multiple faces detected, aborting"))
+    sys.exit(1)
 
 face_location = face_locations[0]
 if use_cnn:
-	face_location = face_location.rect
+    face_location = face_location.rect
 
 # Get the encodings in the frame
 face_landmark = pose_predictor(frame, face_location)
@@ -207,8 +217,13 @@ encodings.append(insert_model)
 
 # Save the new encodings to disk
 with open(enc_file, "w") as datafile:
-	json.dump(encodings, datafile)
+    json.dump(encodings, datafile)
 
 # Give let the user know how it went
-print(_("""\nScan complete
-Added a new model to """) + user)
+print(
+    _(
+        """\nScan complete
+Added a new model to """
+    )
+    + user
+)
diff --git a/howdy/src/cli/clear.py b/howdy/src/cli/clear.py
index 64941ba..f32b9d0 100644
--- a/howdy/src/cli/clear.py
+++ b/howdy/src/cli/clear.py
@@ -13,24 +13,24 @@ user = builtins.howdy_user
 
 # Check if the models folder is there
 if not os.path.exists(paths_factory.user_models_dir_path()):
-	print(_("No models created yet, can't clear them if they don't exist"))
-	sys.exit(1)
+    print(_("No models created yet, can't clear them if they don't exist"))
+    sys.exit(1)
 
 # Check if the user has a models file to delete
 if not os.path.isfile(paths_factory.user_model_path(user)):
-	print(_("{} has no models or they have been cleared already").format(user))
-	sys.exit(1)
+    print(_("{} has no models or they have been cleared already").format(user))
+    sys.exit(1)
 
 # Only ask the user if there's no -y flag
 if not builtins.howdy_args.y:
-	# Double check with the user
-	print(_("This will clear all models for ") + user)
-	ans = input(_("Do you want to continue [y/N]: "))
-
-	# Abort if they don't answer y or Y
-	if (ans.lower() != "y"):
-		print(_('\nInterpreting as a "NO", aborting'))
-		sys.exit(1)
+    # Double check with the user
+    print(_("This will clear all models for ") + user)
+    ans = input(_("Do you want to continue [y/N]: "))
+
+    # Abort if they don't answer y or Y
+    if ans.lower() != "y":
+        print(_('\nInterpreting as a "NO", aborting'))
+        sys.exit(1)
 
 # Delete otherwise
 os.remove(paths_factory.user_model_path(user))
diff --git a/howdy/src/cli/config.py b/howdy/src/cli/config.py
index fae7b08..8e11ed6 100644
--- a/howdy/src/cli/config.py
+++ b/howdy/src/cli/config.py
@@ -15,9 +15,9 @@ editor = "/bin/nano"
 
 # Use the user preferred editor if available
 if "EDITOR" in os.environ:
-	editor = os.environ["EDITOR"]
+    editor = os.environ["EDITOR"]
 elif os.path.isfile("/etc/alternatives/editor"):
-	editor = "/etc/alternatives/editor"
+    editor = "/etc/alternatives/editor"
 
 # Open the editor as a subprocess and fork it
 subprocess.call([editor, paths_factory.config_file_path()])
diff --git a/howdy/src/cli/disable.py b/howdy/src/cli/disable.py
index dc87e23..e1ac21d 100644
--- a/howdy/src/cli/disable.py
+++ b/howdy/src/cli/disable.py
@@ -19,33 +19,39 @@ config.read(config_path)
 
 # Check if enough arguments have been passed
 if not builtins.howdy_args.arguments:
-	print(_("Please add a 0 (enable) or a 1 (disable) as an argument"))
-	sys.exit(1)
+    print(_("Please add a 0 (enable) or a 1 (disable) as an argument"))
+    sys.exit(1)
 
 # Get the cli argument
 argument = builtins.howdy_args.arguments[0]
 
 # Translate the argument to the right string
 if argument == "1" or argument.lower() == "true":
-	out_value = "true"
+    out_value = "true"
 elif argument == "0" or argument.lower() == "false":
-	out_value = "false"
+    out_value = "false"
 else:
-	# Of it's not a 0 or a 1, it's invalid
-	print(_("Please only use 0 (enable) or 1 (disable) as an argument"))
-	sys.exit(1)
+    # Of it's not a 0 or a 1, it's invalid
+    print(_("Please only use 0 (enable) or 1 (disable) as an argument"))
+    sys.exit(1)
 
 # Don't do anything when the state is already the requested one
 if out_value == config.get("core", "disabled", fallback=True):
-	print(_("The disable option has already been set to ") + out_value)
-	sys.exit(1)
+    print(_("The disable option has already been set to ") + out_value)
+    sys.exit(1)
 
 # Loop though the config file and only replace the line containing the disable config
 for line in fileinput.input([config_path], inplace=1):
-	print(line.replace("disabled = " + config.get("core", "disabled", fallback=True), "disabled = " + out_value), end="")
+    print(
+        line.replace(
+            "disabled = " + config.get("core", "disabled", fallback=True),
+            "disabled = " + out_value,
+        ),
+        end="",
+    )
 
 # Print what we just did
 if out_value == "true":
-	print(_("Howdy has been disabled"))
+    print(_("Howdy has been disabled"))
 else:
-	print(_("Howdy has been enabled"))
+    print(_("Howdy has been enabled"))
diff --git a/howdy/src/cli/list.py b/howdy/src/cli/list.py
index 01628de..f054c4d 100644
--- a/howdy/src/cli/list.py
+++ b/howdy/src/cli/list.py
@@ -14,47 +14,47 @@ user = builtins.howdy_user
 
 # Check if the models file has been created yet
 if not os.path.exists(paths_factory.user_models_dir_path()):
-	print(_("Face models have not been initialized yet, please run:"))
-	print("\n\tsudo howdy -U " + user + " add\n")
-	sys.exit(1)
+    print(_("Face models have not been initialized yet, please run:"))
+    print("\n\tsudo howdy -U " + user + " add\n")
+    sys.exit(1)
 
 # Path to the models file
 enc_file = paths_factory.user_model_path(user)
 
 # Try to load the models file and abort if the user does not have it yet
 try:
-	encodings = json.load(open(enc_file))
+    encodings = json.load(open(enc_file))
 except FileNotFoundError:
-	if not builtins.howdy_args.plain:
-		print(_("No face model known for the user {}, please run:").format(user))
-		print("\n\tsudo howdy -U " + user + " add\n")
-	sys.exit(1)
+    if not builtins.howdy_args.plain:
+        print(_("No face model known for the user {}, please run:").format(user))
+        print("\n\tsudo howdy -U " + user + " add\n")
+    sys.exit(1)
 
 # Print a header if we're not in plain mode
 if not builtins.howdy_args.plain:
-	print(_("Known face models for {}:").format(user))
-	print("\n\033[1;29m" + _("ID  Date                 Label\033[0m"))
+    print(_("Known face models for {}:").format(user))
+    print("\n\033[1;29m" + _("ID  Date                 Label\033[0m"))
 
 # Loop through all encodings and print info about them
 for enc in encodings:
-	# Start with the id
-	print(str(enc["id"]), end="")
+    # Start with the id
+    print(str(enc["id"]), end="")
 
-	# Add comma for machine reading
-	if builtins.howdy_args.plain:
-		print(",", end="")
-	# Print padding spaces after the id for a nice layout
-	else:
-		print((4 - len(str(enc["id"]))) * " ", end="")
+    # Add comma for machine reading
+    if builtins.howdy_args.plain:
+        print(",", end="")
+    # Print padding spaces after the id for a nice layout
+    else:
+        print((4 - len(str(enc["id"]))) * " ", end="")
 
-	# Format the time as ISO in the local timezone
-	print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(enc["time"])), end="")
+    # Format the time as ISO in the local timezone
+    print(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(enc["time"])), end="")
 
-	# Separate with commas again for machines, spaces otherwise
-	print("," if builtins.howdy_args.plain else "  ", end="")
+    # Separate with commas again for machines, spaces otherwise
+    print("," if builtins.howdy_args.plain else "  ", end="")
 
-	# End with the label
-	print(enc["label"])
+    # End with the label
+    print(enc["label"])
 
 # Add a closing enter
 print()
diff --git a/howdy/src/cli/remove.py b/howdy/src/cli/remove.py
index cec96c7..3974665 100644
--- a/howdy/src/cli/remove.py
+++ b/howdy/src/cli/remove.py
@@ -13,29 +13,29 @@ user = builtins.howdy_user
 
 # Check if enough arguments have been passed
 if not builtins.howdy_args.arguments:
-	print(_("Please add the ID of the model you want to remove as an argument"))
-	print(_("For example:"))
-	print("\n\thowdy remove 0\n")
-	print(_("You can find the IDs by running:"))
-	print("\n\thowdy list\n")
-	sys.exit(1)
+    print(_("Please add the ID of the model you want to remove as an argument"))
+    print(_("For example:"))
+    print("\n\thowdy remove 0\n")
+    print(_("You can find the IDs by running:"))
+    print("\n\thowdy list\n")
+    sys.exit(1)
 
 # Check if the models file has been created yet
 if not os.path.exists(paths_factory.user_models_dir_path()):
-	print(_("Face models have not been initialized yet, please run:"))
-	print("\n\thowdy add\n")
-	sys.exit(1)
+    print(_("Face models have not been initialized yet, please run:"))
+    print("\n\thowdy add\n")
+    sys.exit(1)
 
 # Path to the models file
 enc_file = paths_factory.user_model_path(user)
 
 # Try to load the models file and abort if the user does not have it yet
 try:
-	encodings = json.load(open(enc_file))
+    encodings = json.load(open(enc_file))
 except FileNotFoundError:
-	print(_("No face model known for the user {}, please run:").format(user))
-	print("\n\thowdy add\n")
-	sys.exit(1)
+    print(_("No face model known for the user {}, please run:").format(user))
+    print("\n\thowdy add\n")
+    sys.exit(1)
 
 # Tracks if a encoding with that id has been found
 found = False
@@ -45,45 +45,49 @@ id = builtins.howdy_args.arguments[0]
 
 # Loop though all encodings and check if they match the argument
 for enc in encodings:
-	if str(enc["id"]) == id:
-		# Only ask the user if there's no -y flag
-		if not builtins.howdy_args.y:
-			# Double check with the user
-			print(_('This will remove the model called "{label}" for {user}').format(label=enc["label"], user=user))
-			ans = input(_("Do you want to continue [y/N]: "))
-
-			# Abort if the answer isn't yes
-			if (ans.lower() != "y"):
-				print(_('\nInterpreting as a "NO", aborting'))
-				sys.exit(1)
-
-			# Add a padding empty  line
-			print()
-
-		# Mark as found and print an enter
-		found = True
-		break
+    if str(enc["id"]) == id:
+        # Only ask the user if there's no -y flag
+        if not builtins.howdy_args.y:
+            # Double check with the user
+            print(
+                _('This will remove the model called "{label}" for {user}').format(
+                    label=enc["label"], user=user
+                )
+            )
+            ans = input(_("Do you want to continue [y/N]: "))
+
+            # Abort if the answer isn't yes
+            if ans.lower() != "y":
+                print(_('\nInterpreting as a "NO", aborting'))
+                sys.exit(1)
+
+            # Add a padding empty  line
+            print()
+
+        # Mark as found and print an enter
+        found = True
+        break
 
 # Abort if no matching id was found
 if not found:
-	print(_("No model with ID {id} exists for {user}").format(id=id, user=user))
-	sys.exit(1)
+    print(_("No model with ID {id} exists for {user}").format(id=id, user=user))
+    sys.exit(1)
 
 # Remove the entire file if this encoding is the only one
 if len(encodings) == 1:
-	os.remove(paths_factory.user_model_path(user))
-	print(_("Removed last model, howdy disabled for user"))
+    os.remove(paths_factory.user_model_path(user))
+    print(_("Removed last model, howdy disabled for user"))
 else:
-	# A place holder to contain the encodings that will remain
-	new_encodings = []
+    # A place holder to contain the encodings that will remain
+    new_encodings = []
 
-	# Loop though all encodings and only add those that don't need to be removed
-	for enc in encodings:
-		if str(enc["id"]) != id:
-			new_encodings.append(enc)
+    # Loop though all encodings and only add those that don't need to be removed
+    for enc in encodings:
+        if str(enc["id"]) != id:
+            new_encodings.append(enc)
 
-	# Save this new set to disk
-	with open(enc_file, "w") as datafile:
-		json.dump(new_encodings, datafile)
+    # Save this new set to disk
+    with open(enc_file, "w") as datafile:
+        json.dump(new_encodings, datafile)
 
-	print(_("Removed model {}").format(id))
+    print(_("Removed model {}").format(id))
diff --git a/howdy/src/cli/set.py b/howdy/src/cli/set.py
index ba981c2..6dc62fd 100644
--- a/howdy/src/cli/set.py
+++ b/howdy/src/cli/set.py
@@ -14,10 +14,10 @@ config_path = paths_factory.config_file_path()
 
 # Check if enough arguments have been passed
 if len(builtins.howdy_args.arguments) < 2:
-	print(_("Please add a setting you would like to change and the value to set it to"))
-	print(_("For example:"))
-	print("\n\thowdy set certainty 3\n")
-	sys.exit(1)
+    print(_("Please add a setting you would like to change and the value to set it to"))
+    print(_("For example:"))
+    print("\n\thowdy set certainty 3\n")
+    sys.exit(1)
 
 # Get the name and value from the cli
 set_name = builtins.howdy_args.arguments[0]
@@ -28,17 +28,17 @@ found_line = ""
 
 # Loop through all lines in the config file
 for line in fileinput.input([config_path]):
-	# Save the line if it starts with the requested config option
-	if line.startswith(set_name + " "):
-		found_line = line
+    # Save the line if it starts with the requested config option
+    if line.startswith(set_name + " "):
+        found_line = line
 
 # If we don't have the line it is not in the config file
 if not found_line:
-	print(_('Could not find a "{}" config option to set').format(set_name))
-	sys.exit(1)
+    print(_('Could not find a "{}" config option to set').format(set_name))
+    sys.exit(1)
 
 # Go through the file again and update the correct line
 for line in fileinput.input([config_path], inplace=1):
-	print(line.replace(found_line, set_name + " = " + set_value + "\n"), end="")
+    print(line.replace(found_line, set_name + " = " + set_value + "\n"), end="")
 
 print(_("Config option updated"))
diff --git a/howdy/src/cli/snap.py b/howdy/src/cli/snap.py
index f8ba3f9..b4e7470 100644
--- a/howdy/src/cli/snap.py
+++ b/howdy/src/cli/snap.py
@@ -28,23 +28,28 @@ dark_threshold = config.getfloat("video", "dark_threshold", fallback=60)
 frames = []
 
 while True:
-	# Grab a single frame of video
-	frame, gsframe = video_capture.read_frame()
+    # Grab a single frame of video
+    frame, gsframe = video_capture.read_frame()
 
-	# Add the frame to the list
-	frames.append(frame)
+    # Add the frame to the list
+    frames.append(frame)
 
-	# Stop the loop if we have 4 frames
-	if len(frames) >= 4:
-		break
+    # Stop the loop if we have 4 frames
+    if len(frames) >= 4:
+        break
 
 # Generate a snapshot image from the frames
-file = snapshot.generate(frames, [
-	_("GENERATED SNAPSHOT"),
-	_("Date: ") + datetime.now(timezone.utc).strftime("%Y/%m/%d %H:%M:%S UTC"),
-	_("Dark threshold config: ") + str(config.getfloat("video", "dark_threshold", fallback=60.0)),
-	_("Certainty config: ") + str(config.getfloat("video", "certainty", fallback=3.5))
-])
+file = snapshot.generate(
+    frames,
+    [
+        _("GENERATED SNAPSHOT"),
+        _("Date: ") + datetime.now(timezone.utc).strftime("%Y/%m/%d %H:%M:%S UTC"),
+        _("Dark threshold config: ")
+        + str(config.getfloat("video", "dark_threshold", fallback=60.0)),
+        _("Certainty config: ")
+        + str(config.getfloat("video", "certainty", fallback=3.5)),
+    ],
+)
 
 # Show the file location in console
 print(_("Generated snapshot saved as"))
diff --git a/howdy/src/cli/test.py b/howdy/src/cli/test.py
index f18c4eb..5615a90 100644
--- a/howdy/src/cli/test.py
+++ b/howdy/src/cli/test.py
@@ -20,8 +20,12 @@ config = configparser.ConfigParser()
 config.read(paths_factory.config_file_path())
 
 if config.get("video", "recording_plugin", fallback="opencv") != "opencv":
-	print(_("Howdy has been configured to use a recorder which doesn't support the test command yet, aborting"))
-	sys.exit(12)
+    print(
+        _(
+            "Howdy has been configured to use a recorder which doesn't support the test command yet, aborting"
+        )
+    )
+    sys.exit(12)
 
 video_capture = VideoCapture(config)
 
@@ -31,51 +35,68 @@ exposure = config.getint("video", "exposure", fallback=-1)
 dark_threshold = config.getfloat("video", "dark_threshold", fallback=60)
 
 # Let the user know what's up
-print(_("""
+print(
+    _(
+        """
 Opening a window with a test feed
 
 Press ctrl+C in this terminal to quit
 Click on the image to enable or disable slow mode
-"""))
+"""
+    )
+)
 
 
 def mouse(event, x, y, flags, param):
-	"""Handle mouse events"""
-	global slow_mode
+    """Handle mouse events"""
+    global slow_mode
 
-	# Toggle slowmode on click
-	if event == cv2.EVENT_LBUTTONDOWN:
-		slow_mode = not slow_mode
+    # Toggle slowmode on click
+    if event == cv2.EVENT_LBUTTONDOWN:
+        slow_mode = not slow_mode
 
 
 def print_text(line_number, text):
-	"""Print the status text by line number"""
-	cv2.putText(overlay, text, (10, height - 10 - (10 * line_number)), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0), 0, cv2.LINE_AA)
+    """Print the status text by line number"""
+    cv2.putText(
+        overlay,
+        text,
+        (10, height - 10 - (10 * line_number)),
+        cv2.FONT_HERSHEY_SIMPLEX,
+        0.3,
+        (0, 255, 0),
+        0,
+        cv2.LINE_AA,
+    )
 
 
-use_cnn = config.getboolean('core', 'use_cnn', fallback=False)
+use_cnn = config.getboolean("core", "use_cnn", fallback=False)
 
 if use_cnn:
-	face_detector = dlib.cnn_face_detection_model_v1(
-		paths_factory.mmod_human_face_detector_path()
-	)
+    face_detector = dlib.cnn_face_detection_model_v1(
+        paths_factory.mmod_human_face_detector_path()
+    )
 else:
-	face_detector = dlib.get_frontal_face_detector()
+    face_detector = dlib.get_frontal_face_detector()
 
-pose_predictor = dlib.shape_predictor(paths_factory.shape_predictor_5_face_landmarks_path())
-face_encoder = dlib.face_recognition_model_v1(paths_factory.dlib_face_recognition_resnet_model_v1_path())
+pose_predictor = dlib.shape_predictor(
+    paths_factory.shape_predictor_5_face_landmarks_path()
+)
+face_encoder = dlib.face_recognition_model_v1(
+    paths_factory.dlib_face_recognition_resnet_model_v1_path()
+)
 
 encodings = []
 models = None
 
 try:
-	user = builtins.howdy_user
-	models = json.load(open(paths_factory.user_model_path(user)))
+    user = builtins.howdy_user
+    models = json.load(open(paths_factory.user_model_path(user)))
 
-	for model in models:
-		encodings += model["data"]
+    for model in models:
+        encodings += model["data"]
 except FileNotFoundError:
-	pass
+    pass
 
 clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
 
@@ -98,153 +119,204 @@ rec_tm = 0
 
 # Wrap everything in an keyboard interrupt handler
 try:
-	while True:
-		frame_tm = time.time()
-
-		# Increment the frames
-		total_frames += 1
-		sec_frames += 1
-
-		# Id we've entered a new second
-		if sec != int(frame_tm):
-			# Set the last seconds FPS
-			fps = sec_frames
-
-			# Set the new second and reset the counter
-			sec = int(frame_tm)
-			sec_frames = 0
-
-		# Grab a single frame of video
-		orig_frame, frame = video_capture.read_frame()
-
-		frame = clahe.apply(frame)
-		# Make a frame to put overlays in
-		overlay = frame.copy()
-		overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2BGR)
-
-		# Fetch the frame height and width
-		height, width = frame.shape[:2]
-
-		# Create a histogram of the image with 8 values
-		hist = cv2.calcHist([frame], [0], None, [8], [0, 256])
-		# All values combined for percentage calculation
-		hist_total = int(sum(hist)[0])
-		# Fill with the overall containing percentage
-		hist_perc = []
-
-		# Loop though all values to calculate a percentage and add it to the overlay
-		for index, value in enumerate(hist):
-			value_perc = float(value[0]) / hist_total * 100
-			hist_perc.append(value_perc)
-
-			# Top left point, 10px margins
-			p1 = (20 + (10 * index), 10)
-			# Bottom right point makes the bar 10px thick, with an height of half the percentage
-			p2 = (10 + (10 * index), int(value_perc / 2 + 10))
-			# Draw the bar in green
-			cv2.rectangle(overlay, p1, p2, (0, 200, 0), thickness=cv2.FILLED)
-
-		# Print the statis in the bottom left
-		print_text(0, _("RESOLUTION: %dx%d") % (height, width))
-		print_text(1, _("FPS: %d") % (fps, ))
-		print_text(2, _("FRAMES: %d") % (total_frames, ))
-		print_text(3, _("RECOGNITION: %dms") % (round(rec_tm * 1000), ))
-
-		# Show that slow mode is on, if it's on
-		if slow_mode:
-			cv2.putText(overlay, _("SLOW MODE"), (width - 66, height - 10), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 0, 255), 0, cv2.LINE_AA)
-
-		# Ignore dark frames
-		if hist_perc[0] > dark_threshold:
-			# Show that this is an ignored frame in the top right
-			cv2.putText(overlay, _("DARK FRAME"), (width - 68, 16), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 0, 255), 0, cv2.LINE_AA)
-		else:
-			# Show that this is an active frame
-			cv2.putText(overlay, _("SCAN FRAME"), (width - 68, 16), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0), 0, cv2.LINE_AA)
-
-			rec_tm = time.time()
-
-			# Get the locations of all faces and their locations
-			# Upsample it once
-			face_locations = face_detector(frame, 1)
-			rec_tm = time.time() - rec_tm
-
-			# Loop though all faces and paint a circle around them
-			for loc in face_locations:
-				if use_cnn:
-					loc = loc.rect
-
-				# By default the circle around the face is red for no match
-				color = (0, 0, 230)
-
-				# Get the center X and Y from the rectangular points
-				x = int((loc.right() - loc.left()) / 2) + loc.left()
-				y = int((loc.bottom() - loc.top()) / 2) + loc.top()
-
-				# Get the raduis from the with of the square
-				r = (loc.right() - loc.left()) / 2
-				# Add 20% padding
-				r = int(r + (r * 0.2))
-
-				# If we have models defined for the current user
-				if models:
-					# Get the encoding of the face in the frame
-					face_landmark = pose_predictor(orig_frame, loc)
-					face_encoding = np.array(face_encoder.compute_face_descriptor(orig_frame, face_landmark, 1))
-
-					# Match this found face against a known face
-					matches = np.linalg.norm(encodings - face_encoding, axis=1)
-
-					# Get best match
-					match_index = np.argmin(matches)
-					match = matches[match_index]
-
-					# If a model matches
-					if 0 < match < video_certainty:
-						# Turn the circle green
-						color = (0, 230, 0)
-
-						# Print the name of the model next to the circle
-						circle_text = "{} (certainty: {})".format(models[match_index]["label"], round(match * 10, 3))
-						cv2.putText(overlay, circle_text, (int(x + r / 3), y - r), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 255, 0), 0, cv2.LINE_AA)
-					# If no approved matches, show red text
-					else:
-						cv2.putText(overlay, "no match", (int(x + r / 3), y - r), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 0, 255), 0, cv2.LINE_AA)
-
-				# Draw the Circle in green
-				cv2.circle(overlay, (x, y), r, color, 2)
-
-		# Add the overlay to the frame with some transparency
-		alpha = 0.65
-		frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
-		cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)
-
-		# Show the image in a window
-		cv2.imshow("Howdy Test", frame)
-
-		# Quit on any keypress
-		if cv2.waitKey(1) != -1:
-			raise KeyboardInterrupt()
-
-		frame_time = time.time() - frame_tm
-
-		# Delay the frame if slowmode is on
-		if slow_mode:
-			time.sleep(max([.5 - frame_time, 0.0]))
-
-		if exposure != -1:
-			# For a strange reason on some cameras (e.g. Lenoxo X1E)
-			# setting manual exposure works only after a couple frames
-			# are captured and even after a delay it does not
-			# always work. Setting exposure at every frame is
-			# reliable though.
-			video_capture.internal.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1.0)  # 1 = Manual
-			video_capture.internal.set(cv2.CAP_PROP_EXPOSURE, float(exposure))
+    while True:
+        frame_tm = time.time()
+
+        # Increment the frames
+        total_frames += 1
+        sec_frames += 1
+
+        # Id we've entered a new second
+        if sec != int(frame_tm):
+            # Set the last seconds FPS
+            fps = sec_frames
+
+            # Set the new second and reset the counter
+            sec = int(frame_tm)
+            sec_frames = 0
+
+        # Grab a single frame of video
+        orig_frame, frame = video_capture.read_frame()
+
+        frame = clahe.apply(frame)
+        # Make a frame to put overlays in
+        overlay = frame.copy()
+        overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2BGR)
+
+        # Fetch the frame height and width
+        height, width = frame.shape[:2]
+
+        # Create a histogram of the image with 8 values
+        hist = cv2.calcHist([frame], [0], None, [8], [0, 256])
+        # All values combined for percentage calculation
+        hist_total = int(sum(hist)[0])
+        # Fill with the overall containing percentage
+        hist_perc = []
+
+        # Loop though all values to calculate a percentage and add it to the overlay
+        for index, value in enumerate(hist):
+            value_perc = float(value[0]) / hist_total * 100
+            hist_perc.append(value_perc)
+
+            # Top left point, 10px margins
+            p1 = (20 + (10 * index), 10)
+            # Bottom right point makes the bar 10px thick, with an height of half the percentage
+            p2 = (10 + (10 * index), int(value_perc / 2 + 10))
+            # Draw the bar in green
+            cv2.rectangle(overlay, p1, p2, (0, 200, 0), thickness=cv2.FILLED)
+
+        # Print the statis in the bottom left
+        print_text(0, _("RESOLUTION: %dx%d") % (height, width))
+        print_text(1, _("FPS: %d") % (fps,))
+        print_text(2, _("FRAMES: %d") % (total_frames,))
+        print_text(3, _("RECOGNITION: %dms") % (round(rec_tm * 1000),))
+
+        # Show that slow mode is on, if it's on
+        if slow_mode:
+            cv2.putText(
+                overlay,
+                _("SLOW MODE"),
+                (width - 66, height - 10),
+                cv2.FONT_HERSHEY_SIMPLEX,
+                0.3,
+                (0, 0, 255),
+                0,
+                cv2.LINE_AA,
+            )
+
+        # Ignore dark frames
+        if hist_perc[0] > dark_threshold:
+            # Show that this is an ignored frame in the top right
+            cv2.putText(
+                overlay,
+                _("DARK FRAME"),
+                (width - 68, 16),
+                cv2.FONT_HERSHEY_SIMPLEX,
+                0.3,
+                (0, 0, 255),
+                0,
+                cv2.LINE_AA,
+            )
+        else:
+            # Show that this is an active frame
+            cv2.putText(
+                overlay,
+                _("SCAN FRAME"),
+                (width - 68, 16),
+                cv2.FONT_HERSHEY_SIMPLEX,
+                0.3,
+                (0, 255, 0),
+                0,
+                cv2.LINE_AA,
+            )
+
+            rec_tm = time.time()
+
+            # Get the locations of all faces and their locations
+            # Upsample it once
+            face_locations = face_detector(frame, 1)
+            rec_tm = time.time() - rec_tm
+
+            # Loop though all faces and paint a circle around them
+            for loc in face_locations:
+                if use_cnn:
+                    loc = loc.rect
+
+                # By default the circle around the face is red for no match
+                color = (0, 0, 230)
+
+                # Get the center X and Y from the rectangular points
+                x = int((loc.right() - loc.left()) / 2) + loc.left()
+                y = int((loc.bottom() - loc.top()) / 2) + loc.top()
+
+                # Get the raduis from the with of the square
+                r = (loc.right() - loc.left()) / 2
+                # Add 20% padding
+                r = int(r + (r * 0.2))
+
+                # If we have models defined for the current user
+                if models:
+                    # Get the encoding of the face in the frame
+                    face_landmark = pose_predictor(orig_frame, loc)
+                    face_encoding = np.array(
+                        face_encoder.compute_face_descriptor(
+                            orig_frame, face_landmark, 1
+                        )
+                    )
+
+                    # Match this found face against a known face
+                    matches = np.linalg.norm(encodings - face_encoding, axis=1)
+
+                    # Get best match
+                    match_index = np.argmin(matches)
+                    match = matches[match_index]
+
+                    # If a model matches
+                    if 0 < match < video_certainty:
+                        # Turn the circle green
+                        color = (0, 230, 0)
+
+                        # Print the name of the model next to the circle
+                        circle_text = "{} (certainty: {})".format(
+                            models[match_index]["label"], round(match * 10, 3)
+                        )
+                        cv2.putText(
+                            overlay,
+                            circle_text,
+                            (int(x + r / 3), y - r),
+                            cv2.FONT_HERSHEY_SIMPLEX,
+                            0.3,
+                            (0, 255, 0),
+                            0,
+                            cv2.LINE_AA,
+                        )
+                    # If no approved matches, show red text
+                    else:
+                        cv2.putText(
+                            overlay,
+                            "no match",
+                            (int(x + r / 3), y - r),
+                            cv2.FONT_HERSHEY_SIMPLEX,
+                            0.3,
+                            (0, 0, 255),
+                            0,
+                            cv2.LINE_AA,
+                        )
+
+                # Draw the Circle in green
+                cv2.circle(overlay, (x, y), r, color, 2)
+
+        # Add the overlay to the frame with some transparency
+        alpha = 0.65
+        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
+        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)
+
+        # Show the image in a window
+        cv2.imshow("Howdy Test", frame)
+
+        # Quit on any keypress
+        if cv2.waitKey(1) != -1:
+            raise KeyboardInterrupt()
+
+        frame_time = time.time() - frame_tm
+
+        # Delay the frame if slowmode is on
+        if slow_mode:
+            time.sleep(max([0.5 - frame_time, 0.0]))
+
+        if exposure != -1:
+            # For a strange reason on some cameras (e.g. Lenoxo X1E)
+            # setting manual exposure works only after a couple frames
+            # are captured and even after a delay it does not
+            # always work. Setting exposure at every frame is
+            # reliable though.
+            video_capture.internal.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1.0)  # 1 = Manual
+            video_capture.internal.set(cv2.CAP_PROP_EXPOSURE, float(exposure))
 
 # On ctrl+C
 except KeyboardInterrupt:
-	# Let the user know we're stopping
-	print(_("\nClosing window"))
+    # Let the user know we're stopping
+    print(_("\nClosing window"))
 
-	# Release handle to the webcam
-	cv2.destroyAllWindows()
+    # Release handle to the webcam
+    cv2.destroyAllWindows()
diff --git a/howdy/src/compare.py b/howdy/src/compare.py
index 9989884..444b0e2 100644
--- a/howdy/src/compare.py
+++ b/howdy/src/compare.py
@@ -5,9 +5,7 @@
 import time
 
 # Start timing
-timings = {
-	"st": time.time()
-}
+timings = {"st": time.time()}
 
 # Import required modules
 import sys
@@ -26,78 +24,94 @@ import paths_factory
 from recorders.video_capture import VideoCapture
 from i18n import _
 
+
 def exit(code=None):
-	"""Exit while closing howdy-gtk properly"""
-	global gtk_proc
+    """Exit while closing howdy-gtk properly"""
+    global gtk_proc
 
-	# Exit the auth ui process if there is one
-	if "gtk_proc" in globals():
-		gtk_proc.terminate()
+    # Exit the auth ui process if there is one
+    if "gtk_proc" in globals():
+        gtk_proc.terminate()
 
-	# Exit compare
-	if code is not None:
-		sys.exit(code)
+    # Exit compare
+    if code is not None:
+        sys.exit(code)
 
 
 def init_detector(lock):
-	"""Start face detector, encoder and predictor in a new thread"""
-	global face_detector, pose_predictor, face_encoder
-
-	# Test if at lest 1 of the data files is there and abort if it's not
-	if not os.path.isfile(paths_factory.shape_predictor_5_face_landmarks_path()):
-		print(_("Data files have not been downloaded, please run the following commands:"))
-		print("\n\tcd " + paths_factory.dlib_data_dir_path())
-		print("\tsudo ./install.sh\n")
-		lock.release()
-		exit(1)
-
-	# Use the CNN detector if enabled
-	if use_cnn:
-		face_detector = dlib.cnn_face_detection_model_v1(paths_factory.mmod_human_face_detector_path())
-	else:
-		face_detector = dlib.get_frontal_face_detector()
-
-	# Start the others regardless
-	pose_predictor = dlib.shape_predictor(paths_factory.shape_predictor_5_face_landmarks_path())
-	face_encoder = dlib.face_recognition_model_v1(paths_factory.dlib_face_recognition_resnet_model_v1_path())
-
-	# Note the time it took to initialize detectors
-	timings["ll"] = time.time() - timings["ll"]
-	lock.release()
+    """Start face detector, encoder and predictor in a new thread"""
+    global face_detector, pose_predictor, face_encoder
+
+    # Test if at lest 1 of the data files is there and abort if it's not
+    if not os.path.isfile(paths_factory.shape_predictor_5_face_landmarks_path()):
+        print(
+            _("Data files have not been downloaded, please run the following commands:")
+        )
+        print("\n\tcd " + paths_factory.dlib_data_dir_path())
+        print("\tsudo ./install.sh\n")
+        lock.release()
+        exit(1)
+
+    # Use the CNN detector if enabled
+    if use_cnn:
+        face_detector = dlib.cnn_face_detection_model_v1(
+            paths_factory.mmod_human_face_detector_path()
+        )
+    else:
+        face_detector = dlib.get_frontal_face_detector()
+
+    # Start the others regardless
+    pose_predictor = dlib.shape_predictor(
+        paths_factory.shape_predictor_5_face_landmarks_path()
+    )
+    face_encoder = dlib.face_recognition_model_v1(
+        paths_factory.dlib_face_recognition_resnet_model_v1_path()
+    )
+
+    # Note the time it took to initialize detectors
+    timings["ll"] = time.time() - timings["ll"]
+    lock.release()
 
 
 def make_snapshot(type):
-	"""Generate snapshot after detection"""
-	snapshot.generate(snapframes, [
-		type + _(" LOGIN"),
-		_("Date: ") + datetime.now(timezone.utc).strftime("%Y/%m/%d %H:%M:%S UTC"),
-		_("Scan time: ") + str(round(time.time() - timings["fr"], 2)) + "s",
-		_("Frames: ") + str(frames) + " (" + str(round(frames / (time.time() - timings["fr"]), 2)) + "FPS)",
-		_("Hostname: ") + os.uname().nodename,
-		_("Best certainty value: ") + str(round(lowest_certainty * 10, 1))
-	])
+    """Generate snapshot after detection"""
+    snapshot.generate(
+        snapframes,
+        [
+            type + _(" LOGIN"),
+            _("Date: ") + datetime.now(timezone.utc).strftime("%Y/%m/%d %H:%M:%S UTC"),
+            _("Scan time: ") + str(round(time.time() - timings["fr"], 2)) + "s",
+            _("Frames: ")
+            + str(frames)
+            + " ("
+            + str(round(frames / (time.time() - timings["fr"]), 2))
+            + "FPS)",
+            _("Hostname: ") + os.uname().nodename,
+            _("Best certainty value: ") + str(round(lowest_certainty * 10, 1)),
+        ],
+    )
 
 
 def send_to_ui(type, message):
-	"""Send message to the auth ui"""
-	global gtk_proc
+    """Send message to the auth ui"""
+    global gtk_proc
 
-	# Only execute of the process started
-	if "gtk_proc" in globals():
-		# Format message so the ui can parse it
-		message = type + "=" + message + " \n"
+    # Only execute of the process started
+    if "gtk_proc" in globals():
+        # Format message so the ui can parse it
+        message = type + "=" + message + " \n"
 
-		# Try to send the message to the auth ui, but it's okay if that fails
-		try:
-			gtk_proc.stdin.write(bytearray(message.encode("utf-8")))
-			gtk_proc.stdin.flush()
-		except IOError:
-			pass
+        # Try to send the message to the auth ui, but it's okay if that fails
+        try:
+            gtk_proc.stdin.write(bytearray(message.encode("utf-8")))
+            gtk_proc.stdin.flush()
+        except IOError:
+            pass
 
 
 # Make sure we were given an username to test against
 if len(sys.argv) < 2:
-	exit(12)
+    exit(12)
 
 # The username of the user being authenticated
 user = sys.argv[1]
@@ -122,16 +136,16 @@ face_encoder = None
 
 # Try to load the face model from the models folder
 try:
-	models = json.load(open(paths_factory.user_model_path(user)))
+    models = json.load(open(paths_factory.user_model_path(user)))
 
-	for model in models:
-		encodings += model["data"]
+    for model in models:
+        encodings += model["data"]
 except FileNotFoundError:
-	exit(10)
+    exit(10)
 
 # Check if the file contains a model
 if len(models) < 1:
-	exit(10)
+    exit(10)
 
 # Read config from disk
 config = configparser.ConfigParser()
@@ -153,10 +167,15 @@ gtk_pipe = sys.stdout if gtk_stdout else subprocess.DEVNULL
 
 # Start the auth ui, register it to be always be closed on exit
 try:
-	gtk_proc = subprocess.Popen(["howdy-gtk", "--start-auth-ui"], stdin=subprocess.PIPE, stdout=gtk_pipe, stderr=gtk_pipe)
-	atexit.register(exit)
+    gtk_proc = subprocess.Popen(
+        ["howdy-gtk", "--start-auth-ui"],
+        stdin=subprocess.PIPE,
+        stdout=gtk_pipe,
+        stderr=gtk_pipe,
+    )
+    atexit.register(exit)
 except FileNotFoundError:
-	pass
+    pass
 
 # Write to the stdin to redraw ui
 send_to_ui("M", _("Starting up..."))
@@ -170,7 +189,7 @@ timings["ll"] = time.time()
 # Start threading and wait for init to finish
 lock = thread.allocate_lock()
 lock.acquire()
-thread.start_new_thread(init_detector, (lock, ))
+thread.start_new_thread(init_detector, (lock,))
 
 # Start video capture on the IR camera
 timings["ic"] = time.time()
@@ -194,7 +213,7 @@ max_height = config.getfloat("video", "max_height", fallback=320.0)
 # Get the height of the image (which would be the width if screen is portrait oriented)
 height = video_capture.internal.get(cv2.CAP_PROP_FRAME_HEIGHT) or 1
 if rotate == 2:
-	height = video_capture.internal.get(cv2.CAP_PROP_FRAME_WIDTH) or 1
+    height = video_capture.internal.get(cv2.CAP_PROP_FRAME_WIDTH) or 1
 # Calculate the amount the image has to shrink
 scaling_factor = (max_height / height) or 1
 
@@ -216,169 +235,206 @@ timings["fr"] = time.time()
 dark_running_total = 0
 
 while True:
-	# Increment the frame count every loop
-	frames += 1
-
-	# Form a string to let the user know we're real busy
-	ui_subtext = "Scanned " + str(valid_frames - dark_tries) + " frames"
-	if (dark_tries > 1):
-		ui_subtext += " (skipped " + str(dark_tries) + " dark frames)"
-	# Show it in the ui as subtext
-	send_to_ui("S", ui_subtext)
-
-	# Stop if we've exceeded the time limit
-	if time.time() - timings["fr"] > timeout:
-		# Create a timeout snapshot if enabled
-		if save_failed:
-			make_snapshot(_("FAILED"))
-
-		if dark_tries == valid_frames:
-			print(_("All frames were too dark, please check dark_threshold in config"))
-			print(_("Average darkness: {avg}, Threshold: {threshold}").format(avg=str(dark_running_total / max(1, valid_frames)), threshold=str(dark_threshold)))
-			exit(13)
-		else:
-			exit(11)
-
-	# Grab a single frame of video
-	frame, gsframe = video_capture.read_frame()
-	gsframe = clahe.apply(gsframe)
-
-	# If snapshots have been turned on
-	if save_failed or save_successful:
-		# Start capturing frames for the snapshot
-		if len(snapframes) < 3:
-			snapframes.append(frame)
-
-	# Create a histogram of the image with 8 values
-	hist = cv2.calcHist([gsframe], [0], None, [8], [0, 256])
-	# All values combined for percentage calculation
-	hist_total = np.sum(hist)
-
-	# Calculate frame darkness
-	darkness = (hist[0] / hist_total * 100)
-
-	# If the image is fully black due to a bad camera read,
-	# skip to the next frame
-	if (hist_total == 0) or (darkness == 100):
-		black_tries += 1
-		continue
-
-	dark_running_total += darkness
-	valid_frames += 1
-
-	# If the image exceeds darkness threshold due to subject distance,
-	# skip to the next frame
-	if (darkness > dark_threshold):
-		dark_tries += 1
-		continue
-
-	# If the height is too high
-	if scaling_factor != 1:
-		# Apply that factor to the frame
-		frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)
-		gsframe = cv2.resize(gsframe, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)
-
-	# If camera is configured to rotate = 1, check portrait in addition to landscape
-	if rotate == 1:
-		if frames % 3 == 1:
-			frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
-			gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_COUNTERCLOCKWISE)
-		if frames % 3 == 2:
-			frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
-			gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_CLOCKWISE)
-
-	# If camera is configured to rotate = 2, check portrait orientation
-	elif rotate == 2:
-		if frames % 2 == 0:
-			frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
-			gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_COUNTERCLOCKWISE)
-		else:
-			frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
-			gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_CLOCKWISE)
-
-	# Get all faces from that frame as encodings
-	# Upsamples 1 time
-	face_locations = face_detector(gsframe, 1)
-	# Loop through each face
-	for fl in face_locations:
-		if use_cnn:
-			fl = fl.rect
-
-		# Fetch the faces in the image
-		face_landmark = pose_predictor(frame, fl)
-		face_encoding = np.array(face_encoder.compute_face_descriptor(frame, face_landmark, 1))
-
-		# Match this found face against a known face
-		matches = np.linalg.norm(encodings - face_encoding, axis=1)
-
-		# Get best match
-		match_index = np.argmin(matches)
-		match = matches[match_index]
-
-		# Update certainty if we have a new low
-		if lowest_certainty > match:
-			lowest_certainty = match
-
-		# Check if a match that's confident enough
-		if 0 < match < video_certainty:
-			timings["tt"] = time.time() - timings["st"]
-			timings["fl"] = time.time() - timings["fr"]
-
-			# If set to true in the config, print debug text
-			if end_report:
-				def print_timing(label, k):
-					"""Helper function to print a timing from the list"""
-					print("  %s: %dms" % (label, round(timings[k] * 1000)))
-
-				# Print a nice timing report
-				print(_("Time spent"))
-				print_timing(_("Starting up"), "in")
-				print(_("  Open cam + load libs: %dms") % (round(max(timings["ll"], timings["ic"]) * 1000, )))
-				print_timing(_("  Opening the camera"), "ic")
-				print_timing(_("  Importing recognition libs"), "ll")
-				print_timing(_("Searching for known face"), "fl")
-				print_timing(_("Total time"), "tt")
-
-				print(_("\nResolution"))
-				width = video_capture.fw or 1
-				print(_("  Native: %dx%d") % (height, width))
-				# Save the new size for diagnostics
-				scale_height, scale_width = frame.shape[:2]
-				print(_("  Used: %dx%d") % (scale_height, scale_width))
-
-				# Show the total number of frames and calculate the FPS by dividing it by the total scan time
-				print(_("\nFrames searched: %d (%.2f fps)") % (frames, frames / timings["fl"]))
-				print(_("Black frames ignored: %d ") % (black_tries, ))
-				print(_("Dark frames ignored: %d ") % (dark_tries, ))
-				print(_("Certainty of winning frame: %.3f") % (match * 10, ))
-
-				print(_("Winning model: %d (\"%s\")") % (match_index, models[match_index]["label"]))
-
-			# Make snapshot if enabled
-			if save_successful:
-				make_snapshot(_("SUCCESSFUL"))
-
-			# Run rubberstamps if enabled
-			if config.getboolean("rubberstamps", "enabled", fallback=False):
-				import rubberstamps
-
-				send_to_ui("S", "")
-
-				if "gtk_proc" not in vars():
-					gtk_proc = None
-
-				rubberstamps.execute(config, gtk_proc, {
-					"video_capture": video_capture,
-					"face_detector": face_detector,
-					"pose_predictor": pose_predictor,
-					"clahe": clahe
-				})
-
-			# End peacefully
-			exit(0)
-
-	if exposure != -1:
-		# For a strange reason on some cameras (e.g. Lenoxo X1E) setting manual exposure works only after a couple frames
-		# are captured and even after a delay it does not always work. Setting exposure at every frame is reliable though.
-		video_capture.internal.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1.0)  # 1 = Manual
-		video_capture.internal.set(cv2.CAP_PROP_EXPOSURE, float(exposure))
+    # Increment the frame count every loop
+    frames += 1
+
+    # Form a string to let the user know we're real busy
+    ui_subtext = "Scanned " + str(valid_frames - dark_tries) + " frames"
+    if dark_tries > 1:
+        ui_subtext += " (skipped " + str(dark_tries) + " dark frames)"
+    # Show it in the ui as subtext
+    send_to_ui("S", ui_subtext)
+
+    # Stop if we've exceeded the time limit
+    if time.time() - timings["fr"] > timeout:
+        # Create a timeout snapshot if enabled
+        if save_failed:
+            make_snapshot(_("FAILED"))
+
+        if dark_tries == valid_frames:
+            print(_("All frames were too dark, please check dark_threshold in config"))
+            print(
+                _("Average darkness: {avg}, Threshold: {threshold}").format(
+                    avg=str(dark_running_total / max(1, valid_frames)),
+                    threshold=str(dark_threshold),
+                )
+            )
+            exit(13)
+        else:
+            exit(11)
+
+    # Grab a single frame of video
+    frame, gsframe = video_capture.read_frame()
+    gsframe = clahe.apply(gsframe)
+
+    # If snapshots have been turned on
+    if save_failed or save_successful:
+        # Start capturing frames for the snapshot
+        if len(snapframes) < 3:
+            snapframes.append(frame)
+
+    # Create a histogram of the image with 8 values
+    hist = cv2.calcHist([gsframe], [0], None, [8], [0, 256])
+    # All values combined for percentage calculation
+    hist_total = np.sum(hist)
+
+    # Calculate frame darkness
+    darkness = hist[0] / hist_total * 100
+
+    # If the image is fully black due to a bad camera read,
+    # skip to the next frame
+    if (hist_total == 0) or (darkness == 100):
+        black_tries += 1
+        continue
+
+    dark_running_total += darkness
+    valid_frames += 1
+
+    # If the image exceeds darkness threshold due to subject distance,
+    # skip to the next frame
+    if darkness > dark_threshold:
+        dark_tries += 1
+        continue
+
+    # If the height is too high
+    if scaling_factor != 1:
+        # Apply that factor to the frame
+        frame = cv2.resize(
+            frame,
+            None,
+            fx=scaling_factor,
+            fy=scaling_factor,
+            interpolation=cv2.INTER_AREA,
+        )
+        gsframe = cv2.resize(
+            gsframe,
+            None,
+            fx=scaling_factor,
+            fy=scaling_factor,
+            interpolation=cv2.INTER_AREA,
+        )
+
+    # If camera is configured to rotate = 1, check portrait in addition to landscape
+    if rotate == 1:
+        if frames % 3 == 1:
+            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
+            gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_COUNTERCLOCKWISE)
+        if frames % 3 == 2:
+            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
+            gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_CLOCKWISE)
+
+    # If camera is configured to rotate = 2, check portrait orientation
+    elif rotate == 2:
+        if frames % 2 == 0:
+            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
+            gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_COUNTERCLOCKWISE)
+        else:
+            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
+            gsframe = cv2.rotate(gsframe, cv2.ROTATE_90_CLOCKWISE)
+
+    # Get all faces from that frame as encodings
+    # Upsamples 1 time
+    face_locations = face_detector(gsframe, 1)
+    # Loop through each face
+    for fl in face_locations:
+        if use_cnn:
+            fl = fl.rect
+
+        # Fetch the faces in the image
+        face_landmark = pose_predictor(frame, fl)
+        face_encoding = np.array(
+            face_encoder.compute_face_descriptor(frame, face_landmark, 1)
+        )
+
+        # Match this found face against a known face
+        matches = np.linalg.norm(encodings - face_encoding, axis=1)
+
+        # Get best match
+        match_index = np.argmin(matches)
+        match = matches[match_index]
+
+        # Update certainty if we have a new low
+        if lowest_certainty > match:
+            lowest_certainty = match
+
+        # Check if a match that's confident enough
+        if 0 < match < video_certainty:
+            timings["tt"] = time.time() - timings["st"]
+            timings["fl"] = time.time() - timings["fr"]
+
+            # If set to true in the config, print debug text
+            if end_report:
+
+                def print_timing(label, k):
+                    """Helper function to print a timing from the list"""
+                    print("  %s: %dms" % (label, round(timings[k] * 1000)))
+
+                # Print a nice timing report
+                print(_("Time spent"))
+                print_timing(_("Starting up"), "in")
+                print(
+                    _("  Open cam + load libs: %dms")
+                    % (
+                        round(
+                            max(timings["ll"], timings["ic"]) * 1000,
+                        )
+                    )
+                )
+                print_timing(_("  Opening the camera"), "ic")
+                print_timing(_("  Importing recognition libs"), "ll")
+                print_timing(_("Searching for known face"), "fl")
+                print_timing(_("Total time"), "tt")
+
+                print(_("\nResolution"))
+                width = video_capture.fw or 1
+                print(_("  Native: %dx%d") % (height, width))
+                # Save the new size for diagnostics
+                scale_height, scale_width = frame.shape[:2]
+                print(_("  Used: %dx%d") % (scale_height, scale_width))
+
+                # Show the total number of frames and calculate the FPS by dividing it by the total scan time
+                print(
+                    _("\nFrames searched: %d (%.2f fps)")
+                    % (frames, frames / timings["fl"])
+                )
+                print(_("Black frames ignored: %d ") % (black_tries,))
+                print(_("Dark frames ignored: %d ") % (dark_tries,))
+                print(_("Certainty of winning frame: %.3f") % (match * 10,))
+
+                print(
+                    _('Winning model: %d ("%s")')
+                    % (match_index, models[match_index]["label"])
+                )
+
+            # Make snapshot if enabled
+            if save_successful:
+                make_snapshot(_("SUCCESSFUL"))
+
+            # Run rubberstamps if enabled
+            if config.getboolean("rubberstamps", "enabled", fallback=False):
+                import rubberstamps
+
+                send_to_ui("S", "")
+
+                if "gtk_proc" not in vars():
+                    gtk_proc = None
+
+                rubberstamps.execute(
+                    config,
+                    gtk_proc,
+                    {
+                        "video_capture": video_capture,
+                        "face_detector": face_detector,
+                        "pose_predictor": pose_predictor,
+                        "clahe": clahe,
+                    },
+                )
+
+            # End peacefully
+            exit(0)
+
+    if exposure != -1:
+        # For a strange reason on some cameras (e.g. Lenoxo X1E) setting manual exposure works only after a couple frames
+        # are captured and even after a delay it does not always work. Setting exposure at every frame is reliable though.
+        video_capture.internal.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1.0)  # 1 = Manual
+        video_capture.internal.set(cv2.CAP_PROP_EXPOSURE, float(exposure))
diff --git a/howdy/src/i18n.py b/howdy/src/i18n.py
index cec8fef..61ad7e0 100644
--- a/howdy/src/i18n.py
+++ b/howdy/src/i18n.py
@@ -5,7 +5,9 @@ import gettext
 import os
 
 # Get the right translation based on locale, falling back to base if none found
-translation = gettext.translation("core", localedir=os.path.join(os.path.dirname(__file__), 'locales'), fallback=True)
+translation = gettext.translation(
+    "core", localedir=os.path.join(os.path.dirname(__file__), "locales"), fallback=True
+)
 translation.install()
 
 # Export translation function as _
diff --git a/howdy/src/recorders/ffmpeg_reader.py b/howdy/src/recorders/ffmpeg_reader.py
index 9cb2573..505fcef 100644
--- a/howdy/src/recorders/ffmpeg_reader.py
+++ b/howdy/src/recorders/ffmpeg_reader.py
@@ -11,124 +11,129 @@ from cv2 import CAP_PROP_FRAME_HEIGHT
 from i18n import _
 
 try:
-	import ffmpeg
+    import ffmpeg
 except ImportError:
-	print(_("Missing ffmpeg module, please run:"))
-	print(" pip3 install ffmpeg-python\n")
-	sys.exit(12)
+    print(_("Missing ffmpeg module, please run:"))
+    print(" pip3 install ffmpeg-python\n")
+    sys.exit(12)
 
 
 class ffmpeg_reader:
-	""" This class was created to look as similar to the openCV features used in Howdy as possible for overall code cleanliness. """
-
-	def __init__(self, device_path, device_format, numframes=10):
-		self.device_path = device_path
-		self.device_format = device_format
-		self.numframes = numframes
-		self.video = ()
-		self.num_frames_read = 0
-		self.height = 0
-		self.width = 0
-		self.init_camera = True
-
-	def set(self, prop, setting):
-		""" Setter method for height and width """
-		if prop == CAP_PROP_FRAME_WIDTH:
-			self.width = setting
-		elif prop == CAP_PROP_FRAME_HEIGHT:
-			self.height = setting
-
-	def get(self, prop):
-		""" Getter method for height and width """
-		if prop == CAP_PROP_FRAME_WIDTH:
-			return self.width
-		elif prop == CAP_PROP_FRAME_HEIGHT:
-			return self.height
-
-	def probe(self):
-		""" Probe the video device to get height and width info """
-
-		# Running this command on ffmpeg unfortunately returns with an exit code of 1, which is silly.
-		# Returns an error code of 1 and this text:  "/dev/video2: Immediate exit requested"
-		args = ["ffmpeg", "-f", self.device_format, "-list_formats", "all", "-i", self.device_path]
-		process = Popen(args, stdout=PIPE, stderr=PIPE)
-		out, err = process.communicate()
-		return_code = process.poll()
-
-		# Worst case scenario, err will equal en empty byte string, b'', so probe will get set to [] here.
-		regex = re.compile(r"\s\d{3,4}x\d{3,4}")
-		probe = regex.findall(str(err.decode("utf-8")))
-
-		if not return_code == 1 or len(probe) < 1:
-			# Could not determine the resolution from ffmpeg call. Reverting to ffmpeg.probe()
-			probe = ffmpeg.probe(self.device_path)
-			height = probe["streams"][0]["height"]
-			width = probe["streams"][0]["width"]
-		else:
-			(height, width) = [x.strip() for x in probe[0].split("x")]
-
-		# Set height and width from probe if they haven't been set already
-		if height.isdigit() and self.get(CAP_PROP_FRAME_HEIGHT) == 0:
-			self.set(CAP_PROP_FRAME_HEIGHT, int(height))
-		if width.isdigit() and self.get(CAP_PROP_FRAME_WIDTH) == 0:
-			self.set(CAP_PROP_FRAME_WIDTH, int(width))
-
-	def record(self, numframes):
-		""" Record a video, saving it to self.video array for processing later """
-
-		# Eensure we have set our width and height before we record, otherwise our numpy call will fail
-		if self.get(CAP_PROP_FRAME_WIDTH) == 0 or self.get(CAP_PROP_FRAME_HEIGHT) == 0:
-			self.probe()
-
-		# Ensure num_frames_read is reset to 0
-		self.num_frames_read = 0
-
-		# Record a predetermined amount of frames from the camera
-		stream, ret = (
-			ffmpeg
-			.input(self.device_path, format=self.device_format)
-			.output("pipe:", format="rawvideo", pix_fmt="rgb24", vframes=numframes)
-			.run(capture_stdout=True, quiet=True)
-		)
-		self.video = (
-			numpy
-			.frombuffer(stream, numpy.uint8)
-			.reshape([-1, self.width, self.height, 3])
-		)
-
-	def read(self):
-		""" Read a single frame from the self.video array. Will record a video if array is empty. """
-
-		# First time we are called, we want to initialize the camera by probing it, to ensure we have height/width
-		# and then take numframes of video to fill the buffer for faster recognition.
-		if self.init_camera:
-			self.init_camera = False
-			self.video = ()
-			self.record(self.numframes)
-			return 0, self.video
-
-		# If we are called and self.video is empty, we should record self.numframes to fill the video buffer
-		if self.video == ():
-			self.record(self.numframes)
-
-		# If we've read max frames, but still are being requested to read more, we simply record another batch.
-		# Note, the video array is 0 based, so if numframes is 10, we must subtract 1 or run into an array index
-		# error.
-		if self.num_frames_read >= (self.numframes - 1):
-			self.record(self.numframes)
-
-		# Add one to num_frames_read. If we were at 0, that's fine as frame 0 is almost 100% going to be black
-		# as the IR lights aren't fully active yet anyways. Saves us one iteration in the while loop ni add/compare.py.
-		self.num_frames_read += 1
-
-		# Return a single frame of video
-		return 0, self.video[self.num_frames_read]
-
-	def release(self):
-		""" Empty our array. If we had a hold on the camera, we would give it back here. """
-		self.video = ()
-		self.num_frames_read = 0
-
-	def grab(self):
-		""" Redirect grab() to read() for compatibility """
-		self.read()
+    """This class was created to look as similar to the openCV features used in Howdy as possible for overall code cleanliness."""
+
+    def __init__(self, device_path, device_format, numframes=10):
+        self.device_path = device_path
+        self.device_format = device_format
+        self.numframes = numframes
+        self.video = ()
+        self.num_frames_read = 0
+        self.height = 0
+        self.width = 0
+        self.init_camera = True
+
+    def set(self, prop, setting):
+        """Setter method for height and width"""
+        if prop == CAP_PROP_FRAME_WIDTH:
+            self.width = setting
+        elif prop == CAP_PROP_FRAME_HEIGHT:
+            self.height = setting
+
+    def get(self, prop):
+        """Getter method for height and width"""
+        if prop == CAP_PROP_FRAME_WIDTH:
+            return self.width
+        elif prop == CAP_PROP_FRAME_HEIGHT:
+            return self.height
+
+    def probe(self):
+        """Probe the video device to get height and width info"""
+
+        # Running this command on ffmpeg unfortunately returns with an exit code of 1, which is silly.
+        # Returns an error code of 1 and this text:  "/dev/video2: Immediate exit requested"
+        args = [
+            "ffmpeg",
+            "-f",
+            self.device_format,
+            "-list_formats",
+            "all",
+            "-i",
+            self.device_path,
+        ]
+        process = Popen(args, stdout=PIPE, stderr=PIPE)
+        out, err = process.communicate()
+        return_code = process.poll()
+
+        # Worst case scenario, err will equal en empty byte string, b'', so probe will get set to [] here.
+        regex = re.compile(r"\s\d{3,4}x\d{3,4}")
+        probe = regex.findall(str(err.decode("utf-8")))
+
+        if not return_code == 1 or len(probe) < 1:
+            # Could not determine the resolution from ffmpeg call. Reverting to ffmpeg.probe()
+            probe = ffmpeg.probe(self.device_path)
+            height = probe["streams"][0]["height"]
+            width = probe["streams"][0]["width"]
+        else:
+            (height, width) = [x.strip() for x in probe[0].split("x")]
+
+        # Set height and width from probe if they haven't been set already
+        if height.isdigit() and self.get(CAP_PROP_FRAME_HEIGHT) == 0:
+            self.set(CAP_PROP_FRAME_HEIGHT, int(height))
+        if width.isdigit() and self.get(CAP_PROP_FRAME_WIDTH) == 0:
+            self.set(CAP_PROP_FRAME_WIDTH, int(width))
+
+    def record(self, numframes):
+        """Record a video, saving it to self.video array for processing later"""
+
+        # Eensure we have set our width and height before we record, otherwise our numpy call will fail
+        if self.get(CAP_PROP_FRAME_WIDTH) == 0 or self.get(CAP_PROP_FRAME_HEIGHT) == 0:
+            self.probe()
+
+        # Ensure num_frames_read is reset to 0
+        self.num_frames_read = 0
+
+        # Record a predetermined amount of frames from the camera
+        stream, ret = (
+            ffmpeg.input(self.device_path, format=self.device_format)
+            .output("pipe:", format="rawvideo", pix_fmt="rgb24", vframes=numframes)
+            .run(capture_stdout=True, quiet=True)
+        )
+        self.video = numpy.frombuffer(stream, numpy.uint8).reshape(
+            [-1, self.width, self.height, 3]
+        )
+
+    def read(self):
+        """Read a single frame from the self.video array. Will record a video if array is empty."""
+
+        # First time we are called, we want to initialize the camera by probing it, to ensure we have height/width
+        # and then take numframes of video to fill the buffer for faster recognition.
+        if self.init_camera:
+            self.init_camera = False
+            self.video = ()
+            self.record(self.numframes)
+            return 0, self.video
+
+        # If we are called and self.video is empty, we should record self.numframes to fill the video buffer
+        if self.video == ():
+            self.record(self.numframes)
+
+        # If we've read max frames, but still are being requested to read more, we simply record another batch.
+        # Note, the video array is 0 based, so if numframes is 10, we must subtract 1 or run into an array index
+        # error.
+        if self.num_frames_read >= (self.numframes - 1):
+            self.record(self.numframes)
+
+        # Add one to num_frames_read. If we were at 0, that's fine as frame 0 is almost 100% going to be black
+        # as the IR lights aren't fully active yet anyways. Saves us one iteration in the while loop ni add/compare.py.
+        self.num_frames_read += 1
+
+        # Return a single frame of video
+        return 0, self.video[self.num_frames_read]
+
+    def release(self):
+        """Empty our array. If we had a hold on the camera, we would give it back here."""
+        self.video = ()
+        self.num_frames_read = 0
+
+    def grab(self):
+        """Redirect grab() to read() for compatibility"""
+        self.read()
diff --git a/howdy/src/recorders/pyv4l2_reader.py b/howdy/src/recorders/pyv4l2_reader.py
index 3be3be1..36e026e 100644
--- a/howdy/src/recorders/pyv4l2_reader.py
+++ b/howdy/src/recorders/pyv4l2_reader.py
@@ -10,95 +10,96 @@ from cv2 import cvtColor, COLOR_GRAY2BGR, CAP_PROP_FRAME_WIDTH, CAP_PROP_FRAME_H
 from i18n import _
 
 try:
-	from pyv4l2.frame import Frame
+    from pyv4l2.frame import Frame
 except ImportError:
-	print(_("Missing pyv4l2 module, please run:"))
-	print(" pip3 install pyv4l2\n")
-	sys.exit(13)
+    print(_("Missing pyv4l2 module, please run:"))
+    print(" pip3 install pyv4l2\n")
+    sys.exit(13)
 
 
 class pyv4l2_reader:
-	""" This class was created to look as similar to the openCV features used in Howdy as possible for overall code cleanliness. """
-
-	# Init
-	def __init__(self, device_name, device_format):
-		self.device_name = device_name
-		self.device_format = device_format
-		self.height = 0
-		self.width = 0
-		self.probe()
-		self.frame = ""
-
-	def set(self, prop, setting):
-		""" Setter method for height and width """
-		if prop == CAP_PROP_FRAME_WIDTH:
-			self.width = setting
-		elif prop == CAP_PROP_FRAME_HEIGHT:
-			self.height = setting
-
-	def get(self, prop):
-		""" Getter method for height and width """
-		if prop == CAP_PROP_FRAME_WIDTH:
-			return self.width
-		elif prop == CAP_PROP_FRAME_HEIGHT:
-			return self.height
-
-	def probe(self):
-		""" Probe the video device to get height and width info """
-
-		vd = open(self.device_name, 'r')
-		fmt = v4l2.v4l2_format()
-		fmt.type = v4l2.V4L2_BUF_TYPE_VIDEO_CAPTURE
-		ret = fcntl.ioctl(vd, v4l2.VIDIOC_G_FMT, fmt)
-		vd.close()
-		if ret == 0:
-			height = fmt.fmt.pix.height
-			width = fmt.fmt.pix.width
-		else:
-			# Could not determine the resolution from ioctl call. Reverting to slower ffmpeg.probe() method
-			import ffmpeg
-			probe = ffmpeg.probe(self.device_name)
-			height = int(probe['streams'][0]['height'])
-			width = int(probe['streams'][0]['width'])
-
-		if self.get(CAP_PROP_FRAME_HEIGHT) == 0:
-			self.set(CAP_PROP_FRAME_HEIGHT, int(height))
-
-		if self.get(CAP_PROP_FRAME_WIDTH) == 0:
-			self.set(CAP_PROP_FRAME_WIDTH, int(width))
-
-	def record(self):
-		""" Start recording """
-		self.frame = Frame(self.device_name)
-
-	def grab(self):
-		""" Read a single frame from the IR camera. """
-		self.read()
-
-	def read(self):
-		""" Read a single frame from the IR camera. """
-
-		if not self.frame:
-			self.record()
-
-		# Grab a raw frame from the camera
-		frame_data = self.frame.get_frame()
-
-		# Convert the raw frame_date to a numpy array
-		img = (numpy.frombuffer(frame_data, numpy.uint8))
-
-		# Convert the numpy array to a proper grayscale image array
-		img_bgr = cvtColor(img, COLOR_GRAY2BGR)
-
-		# Convert the grayscale image array into a proper RGB style numpy array
-		img2 = (numpy.frombuffer(img_bgr, numpy.uint8).reshape([352, 352, 3]))
-
-		# Return a single frame of video
-		return 0, img2
-
-	def release(self):
-		""" Empty our array. If we had a hold on the camera, we would give it back here. """
-		self.video = ()
-		self.num_frames_read = 0
-		if self.frame:
-			self.frame.close()
+    """This class was created to look as similar to the openCV features used in Howdy as possible for overall code cleanliness."""
+
+    # Init
+    def __init__(self, device_name, device_format):
+        self.device_name = device_name
+        self.device_format = device_format
+        self.height = 0
+        self.width = 0
+        self.probe()
+        self.frame = ""
+
+    def set(self, prop, setting):
+        """Setter method for height and width"""
+        if prop == CAP_PROP_FRAME_WIDTH:
+            self.width = setting
+        elif prop == CAP_PROP_FRAME_HEIGHT:
+            self.height = setting
+
+    def get(self, prop):
+        """Getter method for height and width"""
+        if prop == CAP_PROP_FRAME_WIDTH:
+            return self.width
+        elif prop == CAP_PROP_FRAME_HEIGHT:
+            return self.height
+
+    def probe(self):
+        """Probe the video device to get height and width info"""
+
+        vd = open(self.device_name, "r")
+        fmt = v4l2.v4l2_format()
+        fmt.type = v4l2.V4L2_BUF_TYPE_VIDEO_CAPTURE
+        ret = fcntl.ioctl(vd, v4l2.VIDIOC_G_FMT, fmt)
+        vd.close()
+        if ret == 0:
+            height = fmt.fmt.pix.height
+            width = fmt.fmt.pix.width
+        else:
+            # Could not determine the resolution from ioctl call. Reverting to slower ffmpeg.probe() method
+            import ffmpeg
+
+            probe = ffmpeg.probe(self.device_name)
+            height = int(probe["streams"][0]["height"])
+            width = int(probe["streams"][0]["width"])
+
+        if self.get(CAP_PROP_FRAME_HEIGHT) == 0:
+            self.set(CAP_PROP_FRAME_HEIGHT, int(height))
+
+        if self.get(CAP_PROP_FRAME_WIDTH) == 0:
+            self.set(CAP_PROP_FRAME_WIDTH, int(width))
+
+    def record(self):
+        """Start recording"""
+        self.frame = Frame(self.device_name)
+
+    def grab(self):
+        """Read a single frame from the IR camera."""
+        self.read()
+
+    def read(self):
+        """Read a single frame from the IR camera."""
+
+        if not self.frame:
+            self.record()
+
+        # Grab a raw frame from the camera
+        frame_data = self.frame.get_frame()
+
+        # Convert the raw frame_date to a numpy array
+        img = numpy.frombuffer(frame_data, numpy.uint8)
+
+        # Convert the numpy array to a proper grayscale image array
+        img_bgr = cvtColor(img, COLOR_GRAY2BGR)
+
+        # Convert the grayscale image array into a proper RGB style numpy array
+        img2 = numpy.frombuffer(img_bgr, numpy.uint8).reshape([352, 352, 3])
+
+        # Return a single frame of video
+        return 0, img2
+
+    def release(self):
+        """Empty our array. If we had a hold on the camera, we would give it back here."""
+        self.video = ()
+        self.num_frames_read = 0
+        if self.frame:
+            self.frame.close()
diff --git a/howdy/src/recorders/v4l2.py b/howdy/src/recorders/v4l2.py
index 6edf49b..04ee2dc 100644
--- a/howdy/src/recorders/v4l2.py
+++ b/howdy/src/recorders/v4l2.py
@@ -61,15 +61,16 @@ _IOC_DIRSHIFT = _IOC_SIZESHIFT + _IOC_SIZEBITS
 
 _IOC_NONE = 0
 _IOC_WRITE = 1
-_IOC_READ  = 2
+_IOC_READ = 2
 
 
 def _IOC(dir_, type_, nr, size):
     return (
-        ctypes.c_int32(dir_ << _IOC_DIRSHIFT).value |
-        ctypes.c_int32(ord(type_) << _IOC_TYPESHIFT).value |
-        ctypes.c_int32(nr << _IOC_NRSHIFT).value |
-        ctypes.c_int32(size << _IOC_SIZESHIFT).value)
+        ctypes.c_int32(dir_ << _IOC_DIRSHIFT).value
+        | ctypes.c_int32(ord(type_) << _IOC_TYPESHIFT).value
+        | ctypes.c_int32(nr << _IOC_NRSHIFT).value
+        | ctypes.c_int32(size << _IOC_SIZESHIFT).value
+    )
 
 
 def _IOC_TYPECHECK(t):
@@ -104,10 +105,11 @@ c_int = ctypes.c_int
 # time
 #
 
+
 class timeval(ctypes.Structure):
     _fields_ = [
-        ('secs', ctypes.c_long),
-        ('usecs', ctypes.c_long),
+        ("secs", ctypes.c_long),
+        ("usecs", ctypes.c_long),
     ]
 
 
@@ -126,7 +128,7 @@ VID_TYPE_OVERLAY = 8
 VID_TYPE_CHROMAKEY = 16
 VID_TYPE_CLIPPING = 32
 VID_TYPE_FRAMERAM = 64
-VID_TYPE_SCALES	= 128
+VID_TYPE_SCALES = 128
 VID_TYPE_MONOCHROME = 256
 VID_TYPE_SUBCAPTURE = 512
 VID_TYPE_MPEG_DECODER = 1024
@@ -156,31 +158,34 @@ v4l2_field = enum
 
 def V4L2_FIELD_HAS_TOP(field):
     return (
-	field == V4L2_FIELD_TOP or
-	field == V4L2_FIELD_INTERLACED or
-	field == V4L2_FIELD_INTERLACED_TB or
-	field == V4L2_FIELD_INTERLACED_BT or
-	field == V4L2_FIELD_SEQ_TB or
-	field == V4L2_FIELD_SEQ_BT)
+        field == V4L2_FIELD_TOP
+        or field == V4L2_FIELD_INTERLACED
+        or field == V4L2_FIELD_INTERLACED_TB
+        or field == V4L2_FIELD_INTERLACED_BT
+        or field == V4L2_FIELD_SEQ_TB
+        or field == V4L2_FIELD_SEQ_BT
+    )
 
 
 def V4L2_FIELD_HAS_BOTTOM(field):
     return (
-        field == V4L2_FIELD_BOTTOM or
-        field == V4L2_FIELD_INTERLACED or
-        field == V4L2_FIELD_INTERLACED_TB or
-        field == V4L2_FIELD_INTERLACED_BT or
-        field == V4L2_FIELD_SEQ_TB or
-        field == V4L2_FIELD_SEQ_BT)
+        field == V4L2_FIELD_BOTTOM
+        or field == V4L2_FIELD_INTERLACED
+        or field == V4L2_FIELD_INTERLACED_TB
+        or field == V4L2_FIELD_INTERLACED_BT
+        or field == V4L2_FIELD_SEQ_TB
+        or field == V4L2_FIELD_SEQ_BT
+    )
 
 
 def V4L2_FIELD_HAS_BOTH(field):
     return (
-        field == V4L2_FIELD_INTERLACED or
-        field == V4L2_FIELD_INTERLACED_TB or
-        field == V4L2_FIELD_INTERLACED_BT or
-        field == V4L2_FIELD_SEQ_TB or
-        field == V4L2_FIELD_SEQ_BT)
+        field == V4L2_FIELD_INTERLACED
+        or field == V4L2_FIELD_INTERLACED_TB
+        or field == V4L2_FIELD_INTERLACED_BT
+        or field == V4L2_FIELD_SEQ_TB
+        or field == V4L2_FIELD_SEQ_BT
+    )
 
 
 v4l2_buf_type = enum
@@ -250,17 +255,17 @@ v4l2_priority = enum
 
 class v4l2_rect(ctypes.Structure):
     _fields_ = [
-        ('left', ctypes.c_int32),
-        ('top', ctypes.c_int32),
-        ('width', ctypes.c_int32),
-        ('height', ctypes.c_int32),
+        ("left", ctypes.c_int32),
+        ("top", ctypes.c_int32),
+        ("width", ctypes.c_int32),
+        ("height", ctypes.c_int32),
     ]
 
 
 class v4l2_fract(ctypes.Structure):
     _fields_ = [
-        ('numerator', ctypes.c_uint32),
-        ('denominator', ctypes.c_uint32),
+        ("numerator", ctypes.c_uint32),
+        ("denominator", ctypes.c_uint32),
     ]
 
 
@@ -268,14 +273,15 @@ class v4l2_fract(ctypes.Structure):
 # Driver capabilities
 #
 
+
 class v4l2_capability(ctypes.Structure):
     _fields_ = [
-        ('driver', ctypes.c_char * 16),
-        ('card', ctypes.c_char * 32),
-        ('bus_info', ctypes.c_char * 32),
-        ('version', ctypes.c_uint32),
-        ('capabilities', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("driver", ctypes.c_char * 16),
+        ("card", ctypes.c_char * 32),
+        ("bus_info", ctypes.c_char * 32),
+        ("version", ctypes.c_uint32),
+        ("capabilities", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -309,118 +315,122 @@ V4L2_CAP_STREAMING = 0x04000000
 # Video image format
 #
 
+
 class v4l2_pix_format(ctypes.Structure):
     _fields_ = [
-        ('width', ctypes.c_uint32),
-        ('height', ctypes.c_uint32),
-        ('pixelformat', ctypes.c_uint32),
-        ('field', v4l2_field),
-        ('bytesperline', ctypes.c_uint32),
-        ('sizeimage', ctypes.c_uint32),
-        ('colorspace', v4l2_colorspace),
-        ('priv', ctypes.c_uint32),
+        ("width", ctypes.c_uint32),
+        ("height", ctypes.c_uint32),
+        ("pixelformat", ctypes.c_uint32),
+        ("field", v4l2_field),
+        ("bytesperline", ctypes.c_uint32),
+        ("sizeimage", ctypes.c_uint32),
+        ("colorspace", v4l2_colorspace),
+        ("priv", ctypes.c_uint32),
     ]
 
+
 # RGB formats
-V4L2_PIX_FMT_RGB332 = v4l2_fourcc('R', 'G', 'B', '1')
-V4L2_PIX_FMT_RGB444 = v4l2_fourcc('R', '4', '4', '4')
-V4L2_PIX_FMT_RGB555 = v4l2_fourcc('R', 'G', 'B', 'O')
-V4L2_PIX_FMT_RGB565 = v4l2_fourcc('R', 'G', 'B', 'P')
-V4L2_PIX_FMT_RGB555X = v4l2_fourcc('R', 'G', 'B', 'Q')
-V4L2_PIX_FMT_RGB565X = v4l2_fourcc('R', 'G', 'B', 'R')
-V4L2_PIX_FMT_BGR24 = v4l2_fourcc('B', 'G', 'R', '3')
-V4L2_PIX_FMT_RGB24 = v4l2_fourcc('R', 'G', 'B', '3')
-V4L2_PIX_FMT_BGR32 = v4l2_fourcc('B', 'G', 'R', '4')
-V4L2_PIX_FMT_RGB32 = v4l2_fourcc('R', 'G', 'B', '4')
+V4L2_PIX_FMT_RGB332 = v4l2_fourcc("R", "G", "B", "1")
+V4L2_PIX_FMT_RGB444 = v4l2_fourcc("R", "4", "4", "4")
+V4L2_PIX_FMT_RGB555 = v4l2_fourcc("R", "G", "B", "O")
+V4L2_PIX_FMT_RGB565 = v4l2_fourcc("R", "G", "B", "P")
+V4L2_PIX_FMT_RGB555X = v4l2_fourcc("R", "G", "B", "Q")
+V4L2_PIX_FMT_RGB565X = v4l2_fourcc("R", "G", "B", "R")
+V4L2_PIX_FMT_BGR24 = v4l2_fourcc("B", "G", "R", "3")
+V4L2_PIX_FMT_RGB24 = v4l2_fourcc("R", "G", "B", "3")
+V4L2_PIX_FMT_BGR32 = v4l2_fourcc("B", "G", "R", "4")
+V4L2_PIX_FMT_RGB32 = v4l2_fourcc("R", "G", "B", "4")
 
 # Grey formats
-V4L2_PIX_FMT_GREY = v4l2_fourcc('G', 'R', 'E', 'Y')
-V4L2_PIX_FMT_Y10 =  v4l2_fourcc('Y', '1', '0', ' ')
-V4L2_PIX_FMT_Y16 = v4l2_fourcc('Y', '1', '6', ' ')
+V4L2_PIX_FMT_GREY = v4l2_fourcc("G", "R", "E", "Y")
+V4L2_PIX_FMT_Y10 = v4l2_fourcc("Y", "1", "0", " ")
+V4L2_PIX_FMT_Y16 = v4l2_fourcc("Y", "1", "6", " ")
 
 # Palette formats
-V4L2_PIX_FMT_PAL8 = v4l2_fourcc('P', 'A', 'L', '8')
+V4L2_PIX_FMT_PAL8 = v4l2_fourcc("P", "A", "L", "8")
 
 # Luminance+Chrominance formats
-V4L2_PIX_FMT_YVU410 = v4l2_fourcc('Y', 'V', 'U', '9')
-V4L2_PIX_FMT_YVU420 = v4l2_fourcc('Y', 'V', '1', '2')
-V4L2_PIX_FMT_YUYV = v4l2_fourcc('Y', 'U', 'Y', 'V')
-V4L2_PIX_FMT_YYUV = v4l2_fourcc('Y', 'Y', 'U', 'V')
-V4L2_PIX_FMT_YVYU = v4l2_fourcc('Y', 'V', 'Y', 'U')
-V4L2_PIX_FMT_UYVY = v4l2_fourcc('U', 'Y', 'V', 'Y')
-V4L2_PIX_FMT_VYUY = v4l2_fourcc('V', 'Y', 'U', 'Y')
-V4L2_PIX_FMT_YUV422P = v4l2_fourcc('4', '2', '2', 'P')
-V4L2_PIX_FMT_YUV411P = v4l2_fourcc('4', '1', '1', 'P')
-V4L2_PIX_FMT_Y41P = v4l2_fourcc('Y', '4', '1', 'P')
-V4L2_PIX_FMT_YUV444 = v4l2_fourcc('Y', '4', '4', '4')
-V4L2_PIX_FMT_YUV555 = v4l2_fourcc('Y', 'U', 'V', 'O')
-V4L2_PIX_FMT_YUV565 = v4l2_fourcc('Y', 'U', 'V', 'P')
-V4L2_PIX_FMT_YUV32 = v4l2_fourcc('Y', 'U', 'V', '4')
-V4L2_PIX_FMT_YUV410 = v4l2_fourcc('Y', 'U', 'V', '9')
-V4L2_PIX_FMT_YUV420 = v4l2_fourcc('Y', 'U', '1', '2')
-V4L2_PIX_FMT_HI240 = v4l2_fourcc('H', 'I', '2', '4')
-V4L2_PIX_FMT_HM12 = v4l2_fourcc('H', 'M', '1', '2')
+V4L2_PIX_FMT_YVU410 = v4l2_fourcc("Y", "V", "U", "9")
+V4L2_PIX_FMT_YVU420 = v4l2_fourcc("Y", "V", "1", "2")
+V4L2_PIX_FMT_YUYV = v4l2_fourcc("Y", "U", "Y", "V")
+V4L2_PIX_FMT_YYUV = v4l2_fourcc("Y", "Y", "U", "V")
+V4L2_PIX_FMT_YVYU = v4l2_fourcc("Y", "V", "Y", "U")
+V4L2_PIX_FMT_UYVY = v4l2_fourcc("U", "Y", "V", "Y")
+V4L2_PIX_FMT_VYUY = v4l2_fourcc("V", "Y", "U", "Y")
+V4L2_PIX_FMT_YUV422P = v4l2_fourcc("4", "2", "2", "P")
+V4L2_PIX_FMT_YUV411P = v4l2_fourcc("4", "1", "1", "P")
+V4L2_PIX_FMT_Y41P = v4l2_fourcc("Y", "4", "1", "P")
+V4L2_PIX_FMT_YUV444 = v4l2_fourcc("Y", "4", "4", "4")
+V4L2_PIX_FMT_YUV555 = v4l2_fourcc("Y", "U", "V", "O")
+V4L2_PIX_FMT_YUV565 = v4l2_fourcc("Y", "U", "V", "P")
+V4L2_PIX_FMT_YUV32 = v4l2_fourcc("Y", "U", "V", "4")
+V4L2_PIX_FMT_YUV410 = v4l2_fourcc("Y", "U", "V", "9")
+V4L2_PIX_FMT_YUV420 = v4l2_fourcc("Y", "U", "1", "2")
+V4L2_PIX_FMT_HI240 = v4l2_fourcc("H", "I", "2", "4")
+V4L2_PIX_FMT_HM12 = v4l2_fourcc("H", "M", "1", "2")
 
 # two planes -- one Y, one Cr + Cb interleaved
-V4L2_PIX_FMT_NV12 = v4l2_fourcc('N', 'V', '1', '2')
-V4L2_PIX_FMT_NV21 = v4l2_fourcc('N', 'V', '2', '1')
-V4L2_PIX_FMT_NV16 = v4l2_fourcc('N', 'V', '1', '6')
-V4L2_PIX_FMT_NV61 = v4l2_fourcc('N', 'V', '6', '1')
+V4L2_PIX_FMT_NV12 = v4l2_fourcc("N", "V", "1", "2")
+V4L2_PIX_FMT_NV21 = v4l2_fourcc("N", "V", "2", "1")
+V4L2_PIX_FMT_NV16 = v4l2_fourcc("N", "V", "1", "6")
+V4L2_PIX_FMT_NV61 = v4l2_fourcc("N", "V", "6", "1")
 
 # Bayer formats - see http://www.siliconimaging.com/RGB%20Bayer.htm
-V4L2_PIX_FMT_SBGGR8 = v4l2_fourcc('B', 'A', '8', '1')
-V4L2_PIX_FMT_SGBRG8 = v4l2_fourcc('G', 'B', 'R', 'G')
-V4L2_PIX_FMT_SGRBG8 = v4l2_fourcc('G', 'R', 'B', 'G')
-V4L2_PIX_FMT_SRGGB8 = v4l2_fourcc('R', 'G', 'G', 'B')
-V4L2_PIX_FMT_SBGGR10 = v4l2_fourcc('B', 'G', '1', '0')
-V4L2_PIX_FMT_SGBRG10 = v4l2_fourcc('G', 'B', '1', '0')
-V4L2_PIX_FMT_SGRBG10 = v4l2_fourcc('B', 'A', '1', '0')
-V4L2_PIX_FMT_SRGGB10 = v4l2_fourcc('R', 'G', '1', '0')
-V4L2_PIX_FMT_SGRBG10DPCM8 = v4l2_fourcc('B', 'D', '1', '0')
-V4L2_PIX_FMT_SBGGR16 = v4l2_fourcc('B', 'Y', 'R', '2')
+V4L2_PIX_FMT_SBGGR8 = v4l2_fourcc("B", "A", "8", "1")
+V4L2_PIX_FMT_SGBRG8 = v4l2_fourcc("G", "B", "R", "G")
+V4L2_PIX_FMT_SGRBG8 = v4l2_fourcc("G", "R", "B", "G")
+V4L2_PIX_FMT_SRGGB8 = v4l2_fourcc("R", "G", "G", "B")
+V4L2_PIX_FMT_SBGGR10 = v4l2_fourcc("B", "G", "1", "0")
+V4L2_PIX_FMT_SGBRG10 = v4l2_fourcc("G", "B", "1", "0")
+V4L2_PIX_FMT_SGRBG10 = v4l2_fourcc("B", "A", "1", "0")
+V4L2_PIX_FMT_SRGGB10 = v4l2_fourcc("R", "G", "1", "0")
+V4L2_PIX_FMT_SGRBG10DPCM8 = v4l2_fourcc("B", "D", "1", "0")
+V4L2_PIX_FMT_SBGGR16 = v4l2_fourcc("B", "Y", "R", "2")
 
 # compressed formats
-V4L2_PIX_FMT_MJPEG = v4l2_fourcc('M', 'J', 'P', 'G')
-V4L2_PIX_FMT_JPEG = v4l2_fourcc('J', 'P', 'E', 'G')
-V4L2_PIX_FMT_DV = v4l2_fourcc('d', 'v', 's', 'd')
-V4L2_PIX_FMT_MPEG = v4l2_fourcc('M', 'P', 'E', 'G')
+V4L2_PIX_FMT_MJPEG = v4l2_fourcc("M", "J", "P", "G")
+V4L2_PIX_FMT_JPEG = v4l2_fourcc("J", "P", "E", "G")
+V4L2_PIX_FMT_DV = v4l2_fourcc("d", "v", "s", "d")
+V4L2_PIX_FMT_MPEG = v4l2_fourcc("M", "P", "E", "G")
 
 # Vendor-specific formats
-V4L2_PIX_FMT_CPIA1 = v4l2_fourcc('C', 'P', 'I', 'A')
-V4L2_PIX_FMT_WNVA = v4l2_fourcc('W', 'N', 'V', 'A')
-V4L2_PIX_FMT_SN9C10X = v4l2_fourcc('S', '9', '1', '0')
-V4L2_PIX_FMT_SN9C20X_I420 = v4l2_fourcc('S', '9', '2', '0')
-V4L2_PIX_FMT_PWC1 = v4l2_fourcc('P', 'W', 'C', '1')
-V4L2_PIX_FMT_PWC2 = v4l2_fourcc('P', 'W', 'C', '2')
-V4L2_PIX_FMT_ET61X251 = v4l2_fourcc('E', '6', '2', '5')
-V4L2_PIX_FMT_SPCA501 = v4l2_fourcc('S', '5', '0', '1')
-V4L2_PIX_FMT_SPCA505 = v4l2_fourcc('S', '5', '0', '5')
-V4L2_PIX_FMT_SPCA508 = v4l2_fourcc('S', '5', '0', '8')
-V4L2_PIX_FMT_SPCA561 = v4l2_fourcc('S', '5', '6', '1')
-V4L2_PIX_FMT_PAC207 = v4l2_fourcc('P', '2', '0', '7')
-V4L2_PIX_FMT_MR97310A = v4l2_fourcc('M', '3', '1', '0')
-V4L2_PIX_FMT_SN9C2028 = v4l2_fourcc('S', 'O', 'N', 'X')
-V4L2_PIX_FMT_SQ905C = v4l2_fourcc('9', '0', '5', 'C')
-V4L2_PIX_FMT_PJPG = v4l2_fourcc('P', 'J', 'P', 'G')
-V4L2_PIX_FMT_OV511 = v4l2_fourcc('O', '5', '1', '1')
-V4L2_PIX_FMT_OV518 = v4l2_fourcc('O', '5', '1', '8')
-V4L2_PIX_FMT_STV0680 = v4l2_fourcc('S', '6', '8', '0')
+V4L2_PIX_FMT_CPIA1 = v4l2_fourcc("C", "P", "I", "A")
+V4L2_PIX_FMT_WNVA = v4l2_fourcc("W", "N", "V", "A")
+V4L2_PIX_FMT_SN9C10X = v4l2_fourcc("S", "9", "1", "0")
+V4L2_PIX_FMT_SN9C20X_I420 = v4l2_fourcc("S", "9", "2", "0")
+V4L2_PIX_FMT_PWC1 = v4l2_fourcc("P", "W", "C", "1")
+V4L2_PIX_FMT_PWC2 = v4l2_fourcc("P", "W", "C", "2")
+V4L2_PIX_FMT_ET61X251 = v4l2_fourcc("E", "6", "2", "5")
+V4L2_PIX_FMT_SPCA501 = v4l2_fourcc("S", "5", "0", "1")
+V4L2_PIX_FMT_SPCA505 = v4l2_fourcc("S", "5", "0", "5")
+V4L2_PIX_FMT_SPCA508 = v4l2_fourcc("S", "5", "0", "8")
+V4L2_PIX_FMT_SPCA561 = v4l2_fourcc("S", "5", "6", "1")
+V4L2_PIX_FMT_PAC207 = v4l2_fourcc("P", "2", "0", "7")
+V4L2_PIX_FMT_MR97310A = v4l2_fourcc("M", "3", "1", "0")
+V4L2_PIX_FMT_SN9C2028 = v4l2_fourcc("S", "O", "N", "X")
+V4L2_PIX_FMT_SQ905C = v4l2_fourcc("9", "0", "5", "C")
+V4L2_PIX_FMT_PJPG = v4l2_fourcc("P", "J", "P", "G")
+V4L2_PIX_FMT_OV511 = v4l2_fourcc("O", "5", "1", "1")
+V4L2_PIX_FMT_OV518 = v4l2_fourcc("O", "5", "1", "8")
+V4L2_PIX_FMT_STV0680 = v4l2_fourcc("S", "6", "8", "0")
 
 
 #
 # Format enumeration
 #
 
+
 class v4l2_fmtdesc(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('type', ctypes.c_int),
-        ('flags', ctypes.c_uint32),
-        ('description', ctypes.c_char * 32),
-        ('pixelformat', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("type", ctypes.c_int),
+        ("flags", ctypes.c_uint32),
+        ("description", ctypes.c_char * 32),
+        ("pixelformat", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
+
 V4L2_FMT_FLAG_COMPRESSED = 0x0001
 V4L2_FMT_FLAG_EMULATED = 0x0002
 
@@ -439,38 +449,38 @@ v4l2_frmsizetypes = enum
 
 class v4l2_frmsize_discrete(ctypes.Structure):
     _fields_ = [
-        ('width', ctypes.c_uint32),
-        ('height', ctypes.c_uint32),
+        ("width", ctypes.c_uint32),
+        ("height", ctypes.c_uint32),
     ]
 
 
 class v4l2_frmsize_stepwise(ctypes.Structure):
     _fields_ = [
-        ('min_width', ctypes.c_uint32),
-        ('min_height', ctypes.c_uint32),
-        ('step_width', ctypes.c_uint32),
-        ('min_height', ctypes.c_uint32),
-        ('max_height', ctypes.c_uint32),
-        ('step_height', ctypes.c_uint32),
+        ("min_width", ctypes.c_uint32),
+        ("min_height", ctypes.c_uint32),
+        ("step_width", ctypes.c_uint32),
+        ("min_height", ctypes.c_uint32),
+        ("max_height", ctypes.c_uint32),
+        ("step_height", ctypes.c_uint32),
     ]
 
 
 class v4l2_frmsizeenum(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('discrete', v4l2_frmsize_discrete),
-            ('stepwise', v4l2_frmsize_stepwise),
+            ("discrete", v4l2_frmsize_discrete),
+            ("stepwise", v4l2_frmsize_stepwise),
         ]
 
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('pixel_format', ctypes.c_uint32),
-        ('type', ctypes.c_uint32),
-        ('_u', _u),
-        ('reserved', ctypes.c_uint32 * 2)
+        ("index", ctypes.c_uint32),
+        ("pixel_format", ctypes.c_uint32),
+        ("type", ctypes.c_uint32),
+        ("_u", _u),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
 
 
 #
@@ -487,45 +497,46 @@ v4l2_frmivaltypes = enum
 
 class v4l2_frmival_stepwise(ctypes.Structure):
     _fields_ = [
-        ('min', v4l2_fract),
-        ('max', v4l2_fract),
-        ('step', v4l2_fract),
+        ("min", v4l2_fract),
+        ("max", v4l2_fract),
+        ("step", v4l2_fract),
     ]
 
 
 class v4l2_frmivalenum(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('discrete', v4l2_fract),
-            ('stepwise', v4l2_frmival_stepwise),
+            ("discrete", v4l2_fract),
+            ("stepwise", v4l2_frmival_stepwise),
         ]
 
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('pixel_format', ctypes.c_uint32),
-        ('width', ctypes.c_uint32),
-        ('height', ctypes.c_uint32),
-        ('type', ctypes.c_uint32),
-        ('_u', _u),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("index", ctypes.c_uint32),
+        ("pixel_format", ctypes.c_uint32),
+        ("width", ctypes.c_uint32),
+        ("height", ctypes.c_uint32),
+        ("type", ctypes.c_uint32),
+        ("_u", _u),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
 
 
 #
 # Timecode
 #
 
+
 class v4l2_timecode(ctypes.Structure):
     _fields_ = [
-        ('type', ctypes.c_uint32),
-        ('flags', ctypes.c_uint32),
-        ('frames', ctypes.c_uint8),
-        ('seconds', ctypes.c_uint8),
-        ('minutes', ctypes.c_uint8),
-        ('hours', ctypes.c_uint8),
-        ('userbits', ctypes.c_uint8 * 4),
+        ("type", ctypes.c_uint32),
+        ("flags", ctypes.c_uint32),
+        ("frames", ctypes.c_uint8),
+        ("seconds", ctypes.c_uint8),
+        ("minutes", ctypes.c_uint8),
+        ("hours", ctypes.c_uint8),
+        ("userbits", ctypes.c_uint8 * 4),
     ]
 
 
@@ -544,13 +555,13 @@ V4L2_TC_USERBITS_8BITCHARS = 0x0008
 
 class v4l2_jpegcompression(ctypes.Structure):
     _fields_ = [
-        ('quality', ctypes.c_int),
-        ('APPn', ctypes.c_int),
-        ('APP_len', ctypes.c_int),
-        ('APP_data', ctypes.c_char * 60),
-        ('COM_len', ctypes.c_int),
-        ('COM_data', ctypes.c_char * 60),
-        ('jpeg_markers', ctypes.c_uint32),
+        ("quality", ctypes.c_int),
+        ("APPn", ctypes.c_int),
+        ("APP_len", ctypes.c_int),
+        ("APP_data", ctypes.c_char * 60),
+        ("COM_len", ctypes.c_int),
+        ("COM_data", ctypes.c_char * 60),
+        ("jpeg_markers", ctypes.c_uint32),
     ]
 
 
@@ -565,36 +576,37 @@ V4L2_JPEG_MARKER_APP = 1 << 7
 # Memory-mapping buffers
 #
 
+
 class v4l2_requestbuffers(ctypes.Structure):
     _fields_ = [
-        ('count', ctypes.c_uint32),
-        ('type', v4l2_buf_type),
-        ('memory', v4l2_memory),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("count", ctypes.c_uint32),
+        ("type", v4l2_buf_type),
+        ("memory", v4l2_memory),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
 class v4l2_buffer(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('offset', ctypes.c_uint32),
-            ('userptr', ctypes.c_ulong),
+            ("offset", ctypes.c_uint32),
+            ("userptr", ctypes.c_ulong),
         ]
 
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('type', v4l2_buf_type),
-        ('bytesused', ctypes.c_uint32),
-        ('flags', ctypes.c_uint32),
-        ('field', v4l2_field),
-        ('timestamp', timeval),
-        ('timecode', v4l2_timecode),
-        ('sequence', ctypes.c_uint32),
-        ('memory', v4l2_memory),
-        ('m', _u),
-        ('length', ctypes.c_uint32),
-        ('input', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32),
+        ("index", ctypes.c_uint32),
+        ("type", v4l2_buf_type),
+        ("bytesused", ctypes.c_uint32),
+        ("flags", ctypes.c_uint32),
+        ("field", v4l2_field),
+        ("timestamp", timeval),
+        ("timecode", v4l2_timecode),
+        ("sequence", ctypes.c_uint32),
+        ("memory", v4l2_memory),
+        ("m", _u),
+        ("length", ctypes.c_uint32),
+        ("input", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32),
     ]
 
 
@@ -612,16 +624,18 @@ V4L2_BUF_FLAG_INPUT = 0x0200
 # Overlay preview
 #
 
+
 class v4l2_framebuffer(ctypes.Structure):
     _fields_ = [
-        ('capability', ctypes.c_uint32),
-        ('flags', ctypes.c_uint32),
-        ('base', ctypes.c_void_p),
-        ('fmt', v4l2_pix_format),
+        ("capability", ctypes.c_uint32),
+        ("flags", ctypes.c_uint32),
+        ("base", ctypes.c_void_p),
+        ("fmt", v4l2_pix_format),
     ]
 
+
 V4L2_FBUF_CAP_EXTERNOVERLAY = 0x0001
-V4L2_FBUF_CAP_CHROMAKEY	= 0x0002
+V4L2_FBUF_CAP_CHROMAKEY = 0x0002
 V4L2_FBUF_CAP_LIST_CLIPPING = 0x0004
 V4L2_FBUF_CAP_BITMAP_CLIPPING = 0x0008
 V4L2_FBUF_CAP_LOCAL_ALPHA = 0x0010
@@ -640,21 +654,23 @@ V4L2_FBUF_FLAG_SRC_CHROMAKEY = 0x0040
 
 class v4l2_clip(ctypes.Structure):
     pass
+
+
 v4l2_clip._fields_ = [
-    ('c', v4l2_rect),
-    ('next', ctypes.POINTER(v4l2_clip)),
+    ("c", v4l2_rect),
+    ("next", ctypes.POINTER(v4l2_clip)),
 ]
 
 
 class v4l2_window(ctypes.Structure):
     _fields_ = [
-        ('w', v4l2_rect),
-        ('field', v4l2_field),
-        ('chromakey', ctypes.c_uint32),
-        ('clips', ctypes.POINTER(v4l2_clip)),
-        ('clipcount', ctypes.c_uint32),
-        ('bitmap', ctypes.c_void_p),
-        ('global_alpha', ctypes.c_uint8),
+        ("w", v4l2_rect),
+        ("field", v4l2_field),
+        ("chromakey", ctypes.c_uint32),
+        ("clips", ctypes.POINTER(v4l2_clip)),
+        ("clipcount", ctypes.c_uint32),
+        ("bitmap", ctypes.c_void_p),
+        ("global_alpha", ctypes.c_uint8),
     ]
 
 
@@ -662,14 +678,15 @@ class v4l2_window(ctypes.Structure):
 # Capture parameters
 #
 
+
 class v4l2_captureparm(ctypes.Structure):
     _fields_ = [
-        ('capability', ctypes.c_uint32),
-        ('capturemode', ctypes.c_uint32),
-        ('timeperframe', v4l2_fract),
-        ('extendedmode', ctypes.c_uint32),
-        ('readbuffers', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("capability", ctypes.c_uint32),
+        ("capturemode", ctypes.c_uint32),
+        ("timeperframe", v4l2_fract),
+        ("extendedmode", ctypes.c_uint32),
+        ("readbuffers", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -679,12 +696,12 @@ V4L2_CAP_TIMEPERFRAME = 0x1000
 
 class v4l2_outputparm(ctypes.Structure):
     _fields_ = [
-        ('capability', ctypes.c_uint32),
-        ('outputmode', ctypes.c_uint32),
-        ('timeperframe', v4l2_fract),
-        ('extendedmode', ctypes.c_uint32),
-        ('writebuffers', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("capability", ctypes.c_uint32),
+        ("outputmode", ctypes.c_uint32),
+        ("timeperframe", v4l2_fract),
+        ("extendedmode", ctypes.c_uint32),
+        ("writebuffers", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -692,19 +709,20 @@ class v4l2_outputparm(ctypes.Structure):
 # Input image cropping
 #
 
+
 class v4l2_cropcap(ctypes.Structure):
     _fields_ = [
-        ('type', v4l2_buf_type),
-        ('bounds', v4l2_rect),
-        ('defrect', v4l2_rect),
-        ('pixelaspect', v4l2_fract),
+        ("type", v4l2_buf_type),
+        ("bounds", v4l2_rect),
+        ("defrect", v4l2_rect),
+        ("pixelaspect", v4l2_fract),
     ]
 
 
 class v4l2_crop(ctypes.Structure):
     _fields_ = [
-        ('type', ctypes.c_int),
-        ('c', v4l2_rect),
+        ("type", ctypes.c_int),
+        ("c", v4l2_rect),
     ]
 
 
@@ -748,35 +766,42 @@ V4L2_STD_ATSC_16_VSB = 0x02000000
 
 
 # some common needed stuff
-V4L2_STD_PAL_BG = (V4L2_STD_PAL_B | V4L2_STD_PAL_B1 | V4L2_STD_PAL_G)
-V4L2_STD_PAL_DK = (V4L2_STD_PAL_D | V4L2_STD_PAL_D1 | V4L2_STD_PAL_K)
-V4L2_STD_PAL = (V4L2_STD_PAL_BG | V4L2_STD_PAL_DK | V4L2_STD_PAL_H | V4L2_STD_PAL_I)
-V4L2_STD_NTSC = (V4L2_STD_NTSC_M | V4L2_STD_NTSC_M_JP | V4L2_STD_NTSC_M_KR)
-V4L2_STD_SECAM_DK = (V4L2_STD_SECAM_D | V4L2_STD_SECAM_K | V4L2_STD_SECAM_K1)
-V4L2_STD_SECAM = (V4L2_STD_SECAM_B | V4L2_STD_SECAM_G | V4L2_STD_SECAM_H | V4L2_STD_SECAM_DK | V4L2_STD_SECAM_L | V4L2_STD_SECAM_LC)
-
-V4L2_STD_525_60 = (V4L2_STD_PAL_M | V4L2_STD_PAL_60 | V4L2_STD_NTSC | V4L2_STD_NTSC_443)
-V4L2_STD_625_50 = (V4L2_STD_PAL | V4L2_STD_PAL_N | V4L2_STD_PAL_Nc | V4L2_STD_SECAM)
-V4L2_STD_ATSC = (V4L2_STD_ATSC_8_VSB | V4L2_STD_ATSC_16_VSB)
+V4L2_STD_PAL_BG = V4L2_STD_PAL_B | V4L2_STD_PAL_B1 | V4L2_STD_PAL_G
+V4L2_STD_PAL_DK = V4L2_STD_PAL_D | V4L2_STD_PAL_D1 | V4L2_STD_PAL_K
+V4L2_STD_PAL = V4L2_STD_PAL_BG | V4L2_STD_PAL_DK | V4L2_STD_PAL_H | V4L2_STD_PAL_I
+V4L2_STD_NTSC = V4L2_STD_NTSC_M | V4L2_STD_NTSC_M_JP | V4L2_STD_NTSC_M_KR
+V4L2_STD_SECAM_DK = V4L2_STD_SECAM_D | V4L2_STD_SECAM_K | V4L2_STD_SECAM_K1
+V4L2_STD_SECAM = (
+    V4L2_STD_SECAM_B
+    | V4L2_STD_SECAM_G
+    | V4L2_STD_SECAM_H
+    | V4L2_STD_SECAM_DK
+    | V4L2_STD_SECAM_L
+    | V4L2_STD_SECAM_LC
+)
+
+V4L2_STD_525_60 = V4L2_STD_PAL_M | V4L2_STD_PAL_60 | V4L2_STD_NTSC | V4L2_STD_NTSC_443
+V4L2_STD_625_50 = V4L2_STD_PAL | V4L2_STD_PAL_N | V4L2_STD_PAL_Nc | V4L2_STD_SECAM
+V4L2_STD_ATSC = V4L2_STD_ATSC_8_VSB | V4L2_STD_ATSC_16_VSB
 
 V4L2_STD_UNKNOWN = 0
-V4L2_STD_ALL = (V4L2_STD_525_60 | V4L2_STD_625_50)
+V4L2_STD_ALL = V4L2_STD_525_60 | V4L2_STD_625_50
 
 # some merged standards
-V4L2_STD_MN = (V4L2_STD_PAL_M | V4L2_STD_PAL_N | V4L2_STD_PAL_Nc | V4L2_STD_NTSC)
-V4L2_STD_B = (V4L2_STD_PAL_B | V4L2_STD_PAL_B1 | V4L2_STD_SECAM_B)
-V4L2_STD_GH = (V4L2_STD_PAL_G | V4L2_STD_PAL_H|V4L2_STD_SECAM_G | V4L2_STD_SECAM_H)
-V4L2_STD_DK = (V4L2_STD_PAL_DK | V4L2_STD_SECAM_DK)
+V4L2_STD_MN = V4L2_STD_PAL_M | V4L2_STD_PAL_N | V4L2_STD_PAL_Nc | V4L2_STD_NTSC
+V4L2_STD_B = V4L2_STD_PAL_B | V4L2_STD_PAL_B1 | V4L2_STD_SECAM_B
+V4L2_STD_GH = V4L2_STD_PAL_G | V4L2_STD_PAL_H | V4L2_STD_SECAM_G | V4L2_STD_SECAM_H
+V4L2_STD_DK = V4L2_STD_PAL_DK | V4L2_STD_SECAM_DK
 
 
 class v4l2_standard(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('id', v4l2_std_id),
-        ('name', ctypes.c_char * 24),
-        ('frameperiod', v4l2_fract),
-        ('framelines', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("id", v4l2_std_id),
+        ("name", ctypes.c_char * 24),
+        ("frameperiod", v4l2_fract),
+        ("framelines", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -784,27 +809,27 @@ class v4l2_standard(ctypes.Structure):
 # Video timings dv preset
 #
 
+
 class v4l2_dv_preset(ctypes.Structure):
-    _fields_ = [
-        ('preset', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4)
-    ]
+    _fields_ = [("preset", ctypes.c_uint32), ("reserved", ctypes.c_uint32 * 4)]
 
 
 #
 # DV preset enumeration
 #
 
+
 class v4l2_dv_enum_preset(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('preset', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('width', ctypes.c_uint32),
-        ('height', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("preset", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("width", ctypes.c_uint32),
+        ("height", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
+
 #
 # DV preset values
 #
@@ -819,42 +844,44 @@ V4L2_DV_720P50 = 6
 V4L2_DV_720P59_94 = 7
 V4L2_DV_720P60 = 8
 V4L2_DV_1080I29_97 = 9
-V4L2_DV_1080I30	= 10
-V4L2_DV_1080I25	= 11
-V4L2_DV_1080I50	= 12
-V4L2_DV_1080I60	= 13
-V4L2_DV_1080P24	= 14
-V4L2_DV_1080P25	= 15
-V4L2_DV_1080P30	= 16
-V4L2_DV_1080P50	= 17
-V4L2_DV_1080P60	= 18
+V4L2_DV_1080I30 = 10
+V4L2_DV_1080I25 = 11
+V4L2_DV_1080I50 = 12
+V4L2_DV_1080I60 = 13
+V4L2_DV_1080P24 = 14
+V4L2_DV_1080P25 = 15
+V4L2_DV_1080P30 = 16
+V4L2_DV_1080P50 = 17
+V4L2_DV_1080P60 = 18
 
 
 #
 # DV BT timings
 #
 
+
 class v4l2_bt_timings(ctypes.Structure):
     _fields_ = [
-        ('width', ctypes.c_uint32),
-        ('height', ctypes.c_uint32),
-        ('interlaced', ctypes.c_uint32),
-        ('polarities', ctypes.c_uint32),
-        ('pixelclock', ctypes.c_uint64),
-        ('hfrontporch', ctypes.c_uint32),
-        ('hsync', ctypes.c_uint32),
-        ('hbackporch', ctypes.c_uint32),
-        ('vfrontporch', ctypes.c_uint32),
-        ('vsync', ctypes.c_uint32),
-        ('vbackporch', ctypes.c_uint32),
-        ('il_vfrontporch', ctypes.c_uint32),
-        ('il_vsync', ctypes.c_uint32),
-        ('il_vbackporch', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 16),
+        ("width", ctypes.c_uint32),
+        ("height", ctypes.c_uint32),
+        ("interlaced", ctypes.c_uint32),
+        ("polarities", ctypes.c_uint32),
+        ("pixelclock", ctypes.c_uint64),
+        ("hfrontporch", ctypes.c_uint32),
+        ("hsync", ctypes.c_uint32),
+        ("hbackporch", ctypes.c_uint32),
+        ("vfrontporch", ctypes.c_uint32),
+        ("vsync", ctypes.c_uint32),
+        ("vbackporch", ctypes.c_uint32),
+        ("il_vfrontporch", ctypes.c_uint32),
+        ("il_vsync", ctypes.c_uint32),
+        ("il_vbackporch", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 16),
     ]
 
     _pack_ = True
 
+
 # Interlaced or progressive format
 V4L2_DV_PROGRESSIVE = 0
 V4L2_DV_INTERLACED = 1
@@ -867,16 +894,16 @@ V4L2_DV_HSYNC_POS_POL = 0x00000002
 class v4l2_dv_timings(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('bt', v4l2_bt_timings),
-            ('reserved', ctypes.c_uint32 * 32),
+            ("bt", v4l2_bt_timings),
+            ("reserved", ctypes.c_uint32 * 32),
         ]
 
     _fields_ = [
-        ('type', ctypes.c_uint32),
-        ('_u', _u),
+        ("type", ctypes.c_uint32),
+        ("_u", _u),
     ]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
     _pack_ = True
 
 
@@ -888,16 +915,17 @@ V4L2_DV_BT_656_1120 = 0
 # Video inputs
 #
 
+
 class v4l2_input(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('type', ctypes.c_uint32),
-        ('audioset', ctypes.c_uint32),
-        ('tuner', ctypes.c_uint32),
-        ('std', v4l2_std_id),
-        ('status', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("type", ctypes.c_uint32),
+        ("audioset", ctypes.c_uint32),
+        ("tuner", ctypes.c_uint32),
+        ("std", v4l2_std_id),
+        ("status", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -930,20 +958,21 @@ V4L2_IN_CAP_STD = 0x00000004
 # Video outputs
 #
 
+
 class v4l2_output(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('type', ctypes.c_uint32),
-        ('audioset', ctypes.c_uint32),
-        ('modulator', ctypes.c_uint32),
-        ('std', v4l2_std_id),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("type", ctypes.c_uint32),
+        ("audioset", ctypes.c_uint32),
+        ("modulator", ctypes.c_uint32),
+        ("std", v4l2_std_id),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
 V4L2_OUTPUT_TYPE_MODULATOR = 1
-V4L2_OUTPUT_TYPE_ANALOG	= 2
+V4L2_OUTPUT_TYPE_ANALOG = 2
 V4L2_OUTPUT_TYPE_ANALOGVGAOVERLAY = 3
 
 V4L2_OUT_CAP_PRESETS = 0x00000001
@@ -954,79 +983,76 @@ V4L2_OUT_CAP_STD = 0x00000004
 # Controls
 #
 
+
 class v4l2_control(ctypes.Structure):
     _fields_ = [
-        ('id', ctypes.c_uint32),
-        ('value', ctypes.c_int32),
+        ("id", ctypes.c_uint32),
+        ("value", ctypes.c_int32),
     ]
 
 
 class v4l2_ext_control(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('value', ctypes.c_int32),
-            ('value64', ctypes.c_int64),
-            ('reserved', ctypes.c_void_p),
+            ("value", ctypes.c_int32),
+            ("value64", ctypes.c_int64),
+            ("reserved", ctypes.c_void_p),
         ]
 
-    _fields_ = [
-        ('id', ctypes.c_uint32),
-        ('reserved2', ctypes.c_uint32 * 2),
-        ('_u', _u)
-    ]
+    _fields_ = [("id", ctypes.c_uint32), ("reserved2", ctypes.c_uint32 * 2), ("_u", _u)]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
     _pack_ = True
 
 
 class v4l2_ext_controls(ctypes.Structure):
     _fields_ = [
-        ('ctrl_class', ctypes.c_uint32),
-        ('count', ctypes.c_uint32),
-        ('error_idx', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
-        ('controls', ctypes.POINTER(v4l2_ext_control)),
+        ("ctrl_class", ctypes.c_uint32),
+        ("count", ctypes.c_uint32),
+        ("error_idx", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
+        ("controls", ctypes.POINTER(v4l2_ext_control)),
     ]
 
 
 V4L2_CTRL_CLASS_USER = 0x00980000
 V4L2_CTRL_CLASS_MPEG = 0x00990000
-V4L2_CTRL_CLASS_CAMERA = 0x009a0000
-V4L2_CTRL_CLASS_FM_TX = 0x009b0000
+V4L2_CTRL_CLASS_CAMERA = 0x009A0000
+V4L2_CTRL_CLASS_FM_TX = 0x009B0000
 
 
 def V4L2_CTRL_ID_MASK():
-    return 0x0fffffff
+    return 0x0FFFFFFF
 
 
 def V4L2_CTRL_ID2CLASS(id_):
-    return id_ & 0x0fff0000 # unsigned long
+    return id_ & 0x0FFF0000  # unsigned long
 
 
 def V4L2_CTRL_DRIVER_PRIV(id_):
-    return (id_ & 0xffff) >= 0x1000
+    return (id_ & 0xFFFF) >= 0x1000
 
 
 class v4l2_queryctrl(ctypes.Structure):
     _fields_ = [
-        ('id', ctypes.c_uint32),
-        ('type', v4l2_ctrl_type),
-        ('name', ctypes.c_char * 32),
-        ('minimum', ctypes.c_int32),
-        ('maximum', ctypes.c_int32),
-        ('step', ctypes.c_int32),
-        ('default', ctypes.c_int32),
-        ('flags', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("id", ctypes.c_uint32),
+        ("type", v4l2_ctrl_type),
+        ("name", ctypes.c_char * 32),
+        ("minimum", ctypes.c_int32),
+        ("maximum", ctypes.c_int32),
+        ("step", ctypes.c_int32),
+        ("default", ctypes.c_int32),
+        ("flags", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
 class v4l2_querymenu(ctypes.Structure):
     _fields_ = [
-        ('id', ctypes.c_uint32),
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('reserved', ctypes.c_uint32),
+        ("id", ctypes.c_uint32),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("reserved", ctypes.c_uint32),
     ]
 
 
@@ -1055,13 +1081,13 @@ V4L2_CID_AUDIO_BASS = V4L2_CID_BASE + 7
 V4L2_CID_AUDIO_TREBLE = V4L2_CID_BASE + 8
 V4L2_CID_AUDIO_MUTE = V4L2_CID_BASE + 9
 V4L2_CID_AUDIO_LOUDNESS = V4L2_CID_BASE + 10
-V4L2_CID_BLACK_LEVEL = V4L2_CID_BASE + 11 # Deprecated
+V4L2_CID_BLACK_LEVEL = V4L2_CID_BASE + 11  # Deprecated
 V4L2_CID_AUTO_WHITE_BALANCE = V4L2_CID_BASE + 12
 V4L2_CID_DO_WHITE_BALANCE = V4L2_CID_BASE + 13
 V4L2_CID_RED_BALANCE = V4L2_CID_BASE + 14
 V4L2_CID_BLUE_BALANCE = V4L2_CID_BASE + 15
 V4L2_CID_GAMMA = V4L2_CID_BASE + 16
-V4L2_CID_WHITENESS = V4L2_CID_GAMMA # Deprecated
+V4L2_CID_WHITENESS = V4L2_CID_GAMMA  # Deprecated
 V4L2_CID_EXPOSURE = V4L2_CID_BASE + 17
 V4L2_CID_AUTOGAIN = V4L2_CID_BASE + 18
 V4L2_CID_GAIN = V4L2_CID_BASE + 19
@@ -1252,7 +1278,7 @@ v4l2_mpeg_audio_crc = enum
 
 V4L2_CID_MPEG_AUDIO_MUTE = V4L2_CID_MPEG_BASE + 109
 V4L2_CID_MPEG_AUDIO_AAC_BITRATE = V4L2_CID_MPEG_BASE + 110
-V4L2_CID_MPEG_AUDIO_AC3_BITRATE	= V4L2_CID_MPEG_BASE + 111
+V4L2_CID_MPEG_AUDIO_AC3_BITRATE = V4L2_CID_MPEG_BASE + 111
 
 v4l2_mpeg_audio_ac3_bitrate = enum
 (
@@ -1328,7 +1354,7 @@ V4L2_CID_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE = V4L2_CID_MPEG_CX2341X_BAS
 
 v4l2_mpeg_cx2341x_video_luma_spatial_filter_type = enum
 (
-    V4L2_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE_OFF, 
+    V4L2_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE_OFF,
     V4L2_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE_1D_HOR,
     V4L2_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE_1D_VERT,
     V4L2_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE_2D_HV_SEPARABLE,
@@ -1443,31 +1469,32 @@ V4L2_CID_TUNE_ANTENNA_CAPACITOR = V4L2_CID_FM_TX_CLASS_BASE + 114
 # Tuning
 #
 
+
 class v4l2_tuner(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('type', v4l2_tuner_type),
-        ('capability', ctypes.c_uint32),
-        ('rangelow', ctypes.c_uint32),
-        ('rangehigh', ctypes.c_uint32),
-        ('rxsubchans', ctypes.c_uint32),
-        ('audmode', ctypes.c_uint32),
-        ('signal', ctypes.c_int32),
-        ('afc', ctypes.c_int32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("type", v4l2_tuner_type),
+        ("capability", ctypes.c_uint32),
+        ("rangelow", ctypes.c_uint32),
+        ("rangehigh", ctypes.c_uint32),
+        ("rxsubchans", ctypes.c_uint32),
+        ("audmode", ctypes.c_uint32),
+        ("signal", ctypes.c_int32),
+        ("afc", ctypes.c_int32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
 class v4l2_modulator(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('capability', ctypes.c_uint32),
-        ('rangelow', ctypes.c_uint32),
-        ('rangehigh', ctypes.c_uint32),
-        ('txsubchans', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("capability", ctypes.c_uint32),
+        ("rangelow", ctypes.c_uint32),
+        ("rangehigh", ctypes.c_uint32),
+        ("txsubchans", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
     ]
 
 
@@ -1496,20 +1523,20 @@ V4L2_TUNER_MODE_LANG1_LANG2 = 0x0004
 
 class v4l2_frequency(ctypes.Structure):
     _fields_ = [
-        ('tuner', ctypes.c_uint32),
-        ('type', v4l2_tuner_type),
-        ('frequency', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 8),
+        ("tuner", ctypes.c_uint32),
+        ("type", v4l2_tuner_type),
+        ("frequency", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 8),
     ]
 
 
 class v4l2_hw_freq_seek(ctypes.Structure):
     _fields_ = [
-        ('tuner', ctypes.c_uint32),
-        ('type', v4l2_tuner_type),
-        ('seek_upward', ctypes.c_uint32),
-        ('wrap_around', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 8),
+        ("tuner", ctypes.c_uint32),
+        ("type", v4l2_tuner_type),
+        ("seek_upward", ctypes.c_uint32),
+        ("wrap_around", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 8),
     ]
 
 
@@ -1517,17 +1544,18 @@ class v4l2_hw_freq_seek(ctypes.Structure):
 # RDS
 #
 
+
 class v4l2_rds_data(ctypes.Structure):
     _fields_ = [
-        ('lsb', ctypes.c_char),
-        ('msb', ctypes.c_char),
-        ('block', ctypes.c_char),
+        ("lsb", ctypes.c_char),
+        ("msb", ctypes.c_char),
+        ("block", ctypes.c_char),
     ]
 
     _pack_ = True
 
 
-V4L2_RDS_BLOCK_MSK =  0x7
+V4L2_RDS_BLOCK_MSK = 0x7
 V4L2_RDS_BLOCK_A = 0
 V4L2_RDS_BLOCK_B = 1
 V4L2_RDS_BLOCK_C = 2
@@ -1543,13 +1571,14 @@ V4L2_RDS_BLOCK_ERROR = 0x80
 # Audio
 #
 
+
 class v4l2_audio(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('capability', ctypes.c_uint32),
-        ('mode', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("capability", ctypes.c_uint32),
+        ("mode", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
@@ -1561,11 +1590,11 @@ V4L2_AUDMODE_AVL = 0x00001
 
 class v4l2_audioout(ctypes.Structure):
     _fields_ = [
-        ('index', ctypes.c_uint32),
-        ('name', ctypes.c_char * 32),
-        ('capability', ctypes.c_uint32),
-        ('mode', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("index", ctypes.c_uint32),
+        ("name", ctypes.c_char * 32),
+        ("capability", ctypes.c_uint32),
+        ("mode", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
@@ -1576,16 +1605,16 @@ class v4l2_audioout(ctypes.Structure):
 V4L2_ENC_IDX_FRAME_I = 0
 V4L2_ENC_IDX_FRAME_P = 1
 V4L2_ENC_IDX_FRAME_B = 2
-V4L2_ENC_IDX_FRAME_MASK = 0xf
+V4L2_ENC_IDX_FRAME_MASK = 0xF
 
 
 class v4l2_enc_idx_entry(ctypes.Structure):
     _fields_ = [
-        ('offset', ctypes.c_uint64),
-        ('pts', ctypes.c_uint64),
-        ('length', ctypes.c_uint32),
-        ('flags', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("offset", ctypes.c_uint64),
+        ("pts", ctypes.c_uint64),
+        ("length", ctypes.c_uint32),
+        ("flags", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
@@ -1594,10 +1623,10 @@ V4L2_ENC_IDX_ENTRIES = 64
 
 class v4l2_enc_idx(ctypes.Structure):
     _fields_ = [
-        ('entries', ctypes.c_uint32),
-        ('entries_cap', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 4),
-        ('entry', v4l2_enc_idx_entry * V4L2_ENC_IDX_ENTRIES),
+        ("entries", ctypes.c_uint32),
+        ("entries_cap", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 4),
+        ("entry", v4l2_enc_idx_entry * V4L2_ENC_IDX_ENTRIES),
     ]
 
 
@@ -1613,36 +1642,37 @@ class v4l2_encoder_cmd(ctypes.Structure):
     class _u(ctypes.Union):
         class _s(ctypes.Structure):
             _fields_ = [
-                ('data', ctypes.c_uint32 * 8),
+                ("data", ctypes.c_uint32 * 8),
             ]
 
         _fields_ = [
-            ('raw', _s),
+            ("raw", _s),
         ]
 
     _fields_ = [
-        ('cmd', ctypes.c_uint32),
-        ('flags', ctypes.c_uint32),
-        ('_u', _u),
+        ("cmd", ctypes.c_uint32),
+        ("flags", ctypes.c_uint32),
+        ("_u", _u),
     ]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
 
 
 #
 # Data services (VBI)
 #
 
+
 class v4l2_vbi_format(ctypes.Structure):
     _fields_ = [
-        ('sampling_rate', ctypes.c_uint32),
-        ('offset', ctypes.c_uint32),
-        ('samples_per_line', ctypes.c_uint32),
-        ('sample_format', ctypes.c_uint32),
-        ('start', ctypes.c_int32 * 2),
-        ('count', ctypes.c_uint32 * 2),
-        ('flags', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("sampling_rate", ctypes.c_uint32),
+        ("offset", ctypes.c_uint32),
+        ("samples_per_line", ctypes.c_uint32),
+        ("sample_format", ctypes.c_uint32),
+        ("start", ctypes.c_int32 * 2),
+        ("count", ctypes.c_uint32 * 2),
+        ("flags", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
@@ -1652,10 +1682,10 @@ V4L2_VBI_INTERLACED = 1 << 1
 
 class v4l2_sliced_vbi_format(ctypes.Structure):
     _fields_ = [
-        ('service_set', ctypes.c_uint16),
-        ('service_lines', ctypes.c_uint16 * 2 * 24),
-        ('io_size', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32 * 2),
+        ("service_set", ctypes.c_uint16),
+        ("service_lines", ctypes.c_uint16 * 2 * 24),
+        ("io_size", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32 * 2),
     ]
 
 
@@ -1664,26 +1694,25 @@ V4L2_SLICED_VPS = 0x0400
 V4L2_SLICED_CAPTION_525 = 0x1000
 V4L2_SLICED_WSS_625 = 0x4000
 V4L2_SLICED_VBI_525 = V4L2_SLICED_CAPTION_525
-V4L2_SLICED_VBI_625 = (
-    V4L2_SLICED_TELETEXT_B | V4L2_SLICED_VPS | V4L2_SLICED_WSS_625)
+V4L2_SLICED_VBI_625 = V4L2_SLICED_TELETEXT_B | V4L2_SLICED_VPS | V4L2_SLICED_WSS_625
 
 
 class v4l2_sliced_vbi_cap(ctypes.Structure):
     _fields_ = [
-        ('service_set', ctypes.c_uint16),
-        ('service_lines', ctypes.c_uint16 * 2 * 24),
-        ('type', v4l2_buf_type),
-        ('reserved', ctypes.c_uint32 * 3),
+        ("service_set", ctypes.c_uint16),
+        ("service_lines", ctypes.c_uint16 * 2 * 24),
+        ("type", v4l2_buf_type),
+        ("reserved", ctypes.c_uint32 * 3),
     ]
 
 
 class v4l2_sliced_vbi_data(ctypes.Structure):
     _fields_ = [
-        ('id', ctypes.c_uint32),
-        ('field', ctypes.c_uint32),
-        ('line', ctypes.c_uint32),
-        ('reserved', ctypes.c_uint32),
-        ('data', ctypes.c_char * 48),
+        ("id", ctypes.c_uint32),
+        ("field", ctypes.c_uint32),
+        ("line", ctypes.c_uint32),
+        ("reserved", ctypes.c_uint32),
+        ("data", ctypes.c_char * 48),
     ]
 
 
@@ -1700,8 +1729,8 @@ V4L2_MPEG_VBI_IVTV_VPS = 7
 
 class v4l2_mpeg_vbi_itv0_line(ctypes.Structure):
     _fields_ = [
-        ('id', ctypes.c_char),
-        ('data', ctypes.c_char * 42),
+        ("id", ctypes.c_char),
+        ("data", ctypes.c_char * 42),
     ]
 
     _pack_ = True
@@ -1709,8 +1738,8 @@ class v4l2_mpeg_vbi_itv0_line(ctypes.Structure):
 
 class v4l2_mpeg_vbi_itv0(ctypes.Structure):
     _fields_ = [
-        ('linemask', ctypes.c_uint32 * 2), # how to define __le32 in ctypes?
-        ('line', v4l2_mpeg_vbi_itv0_line * 35),
+        ("linemask", ctypes.c_uint32 * 2),  # how to define __le32 in ctypes?
+        ("line", v4l2_mpeg_vbi_itv0_line * 35),
     ]
 
     _pack_ = True
@@ -1718,7 +1747,7 @@ class v4l2_mpeg_vbi_itv0(ctypes.Structure):
 
 class v4l2_mpeg_vbi_ITV0(ctypes.Structure):
     _fields_ = [
-        ('line', v4l2_mpeg_vbi_itv0_line * 36),
+        ("line", v4l2_mpeg_vbi_itv0_line * 36),
     ]
 
     _pack_ = True
@@ -1731,16 +1760,13 @@ V4L2_MPEG_VBI_IVTV_MAGIC1 = "ITV0"
 class v4l2_mpeg_vbi_fmt_ivtv(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('itv0', v4l2_mpeg_vbi_itv0),
-            ('ITV0', v4l2_mpeg_vbi_ITV0),
+            ("itv0", v4l2_mpeg_vbi_itv0),
+            ("ITV0", v4l2_mpeg_vbi_ITV0),
         ]
 
-    _fields_ = [
-        ('magic', ctypes.c_char * 4),
-        ('_u', _u)
-    ]
+    _fields_ = [("magic", ctypes.c_char * 4), ("_u", _u)]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
     _pack_ = True
 
 
@@ -1748,34 +1774,32 @@ class v4l2_mpeg_vbi_fmt_ivtv(ctypes.Structure):
 # Aggregate structures
 #
 
+
 class v4l2_format(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('pix', v4l2_pix_format),
-            ('win', v4l2_window),
-            ('vbi', v4l2_vbi_format),
-            ('sliced', v4l2_sliced_vbi_format),
-            ('raw_data', ctypes.c_char * 200),
+            ("pix", v4l2_pix_format),
+            ("win", v4l2_window),
+            ("vbi", v4l2_vbi_format),
+            ("sliced", v4l2_sliced_vbi_format),
+            ("raw_data", ctypes.c_char * 200),
         ]
 
     _fields_ = [
-        ('type', v4l2_buf_type),
-        ('fmt', _u),
+        ("type", v4l2_buf_type),
+        ("fmt", _u),
     ]
 
 
 class v4l2_streamparm(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('capture', v4l2_captureparm),
-            ('output', v4l2_outputparm),
-            ('raw_data', ctypes.c_char * 200),
+            ("capture", v4l2_captureparm),
+            ("output", v4l2_outputparm),
+            ("raw_data", ctypes.c_char * 200),
         ]
 
-    _fields_ = [
-        ('type', v4l2_buf_type),
-        ('parm', _u)
-    ]
+    _fields_ = [("type", v4l2_buf_type), ("parm", _u)]
 
 
 #
@@ -1791,25 +1815,25 @@ V4L2_CHIP_MATCH_AC97 = 3
 class v4l2_dbg_match(ctypes.Structure):
     class _u(ctypes.Union):
         _fields_ = [
-            ('addr', ctypes.c_uint32),
-            ('name', ctypes.c_char * 32),
+            ("addr", ctypes.c_uint32),
+            ("name", ctypes.c_char * 32),
         ]
 
     _fields_ = [
-        ('type', ctypes.c_uint32),
-        ('_u', _u),
+        ("type", ctypes.c_uint32),
+        ("_u", _u),
     ]
 
-    _anonymous_ = ('_u',)
+    _anonymous_ = ("_u",)
     _pack_ = True
 
 
 class v4l2_dbg_register(ctypes.Structure):
     _fields_ = [
-        ('match', v4l2_dbg_match),
-        ('size', ctypes.c_uint32),
-        ('reg', ctypes.c_uint64),
-        ('val', ctypes.c_uint64),
+        ("match", v4l2_dbg_match),
+        ("size", ctypes.c_uint32),
+        ("reg", ctypes.c_uint64),
+        ("val", ctypes.c_uint64),
     ]
 
     _pack_ = True
@@ -1817,9 +1841,9 @@ class v4l2_dbg_register(ctypes.Structure):
 
 class v4l2_dbg_chip_ident(ctypes.Structure):
     _fields_ = [
-        ('match', v4l2_dbg_match),
-        ('ident', ctypes.c_uint32),
-        ('revision', ctypes.c_uint32),
+        ("match", v4l2_dbg_match),
+        ("ident", ctypes.c_uint32),
+        ("revision", ctypes.c_uint32),
     ]
 
     _pack_ = True
@@ -1829,86 +1853,86 @@ class v4l2_dbg_chip_ident(ctypes.Structure):
 # ioctl codes for video devices
 #
 
-VIDIOC_QUERYCAP = _IOR('V', 0, v4l2_capability)
-VIDIOC_RESERVED = _IO('V', 1)
-VIDIOC_ENUM_FMT = _IOWR('V', 2, v4l2_fmtdesc)
-VIDIOC_G_FMT = _IOWR('V', 4, v4l2_format)
-VIDIOC_S_FMT = _IOWR('V', 5, v4l2_format)
-VIDIOC_REQBUFS = _IOWR('V', 8, v4l2_requestbuffers)
-VIDIOC_QUERYBUF	= _IOWR('V', 9, v4l2_buffer)
-VIDIOC_G_FBUF = _IOR('V', 10, v4l2_framebuffer)
-VIDIOC_S_FBUF = _IOW('V', 11, v4l2_framebuffer)
-VIDIOC_OVERLAY = _IOW('V', 14, ctypes.c_int)
-VIDIOC_QBUF = _IOWR('V', 15, v4l2_buffer)
-VIDIOC_DQBUF = _IOWR('V', 17, v4l2_buffer)
-VIDIOC_STREAMON = _IOW('V', 18, ctypes.c_int)
-VIDIOC_STREAMOFF = _IOW('V', 19, ctypes.c_int)
-VIDIOC_G_PARM = _IOWR('V', 21, v4l2_streamparm)
-VIDIOC_S_PARM = _IOWR('V', 22, v4l2_streamparm)
-VIDIOC_G_STD = _IOR('V', 23, v4l2_std_id)
-VIDIOC_S_STD = _IOW('V', 24, v4l2_std_id)
-VIDIOC_ENUMSTD = _IOWR('V', 25, v4l2_standard)
-VIDIOC_ENUMINPUT = _IOWR('V', 26, v4l2_input)
-VIDIOC_G_CTRL = _IOWR('V', 27, v4l2_control)
-VIDIOC_S_CTRL = _IOWR('V', 28, v4l2_control)
-VIDIOC_G_TUNER = _IOWR('V', 29, v4l2_tuner)
-VIDIOC_S_TUNER = _IOW('V', 30, v4l2_tuner)
-VIDIOC_G_AUDIO = _IOR('V', 33, v4l2_audio)
-VIDIOC_S_AUDIO = _IOW('V', 34, v4l2_audio)
-VIDIOC_QUERYCTRL = _IOWR('V', 36, v4l2_queryctrl)
-VIDIOC_QUERYMENU = _IOWR('V', 37, v4l2_querymenu)
-VIDIOC_G_INPUT = _IOR('V', 38, ctypes.c_int)
-VIDIOC_S_INPUT = _IOWR('V', 39, ctypes.c_int)
-VIDIOC_G_OUTPUT = _IOR('V', 46, ctypes.c_int)
-VIDIOC_S_OUTPUT = _IOWR('V', 47, ctypes.c_int)
-VIDIOC_ENUMOUTPUT = _IOWR('V', 48, v4l2_output)
-VIDIOC_G_AUDOUT = _IOR('V', 49, v4l2_audioout)
-VIDIOC_S_AUDOUT	= _IOW('V', 50, v4l2_audioout)
-VIDIOC_G_MODULATOR = _IOWR('V', 54, v4l2_modulator)
-VIDIOC_S_MODULATOR = _IOW('V', 55, v4l2_modulator)
-VIDIOC_G_FREQUENCY = _IOWR('V', 56, v4l2_frequency)
-VIDIOC_S_FREQUENCY = _IOW('V', 57, v4l2_frequency)
-VIDIOC_CROPCAP = _IOWR('V', 58, v4l2_cropcap)
-VIDIOC_G_CROP = _IOWR('V', 59, v4l2_crop)
-VIDIOC_S_CROP = _IOW('V', 60, v4l2_crop)
-VIDIOC_G_JPEGCOMP = _IOR('V', 61, v4l2_jpegcompression)
-VIDIOC_S_JPEGCOMP = _IOW('V', 62, v4l2_jpegcompression)
-VIDIOC_QUERYSTD = _IOR('V', 63, v4l2_std_id)
-VIDIOC_TRY_FMT = _IOWR('V', 64, v4l2_format)
-VIDIOC_ENUMAUDIO = _IOWR('V', 65, v4l2_audio)
-VIDIOC_ENUMAUDOUT = _IOWR('V', 66, v4l2_audioout)
-VIDIOC_G_PRIORITY = _IOR('V', 67, v4l2_priority)
-VIDIOC_S_PRIORITY = _IOW('V', 68, v4l2_priority)
-VIDIOC_G_SLICED_VBI_CAP = _IOWR('V', 69, v4l2_sliced_vbi_cap)
-VIDIOC_LOG_STATUS = _IO('V', 70)
-VIDIOC_G_EXT_CTRLS = _IOWR('V', 71, v4l2_ext_controls)
-VIDIOC_S_EXT_CTRLS = _IOWR('V', 72, v4l2_ext_controls)
-VIDIOC_TRY_EXT_CTRLS = _IOWR('V', 73, v4l2_ext_controls)
-
-VIDIOC_ENUM_FRAMESIZES = _IOWR('V', 74, v4l2_frmsizeenum)
-VIDIOC_ENUM_FRAMEINTERVALS = _IOWR('V', 75, v4l2_frmivalenum)
-VIDIOC_G_ENC_INDEX = _IOR('V', 76, v4l2_enc_idx)
-VIDIOC_ENCODER_CMD = _IOWR('V', 77, v4l2_encoder_cmd)
-VIDIOC_TRY_ENCODER_CMD = _IOWR('V', 78, v4l2_encoder_cmd)
-
-VIDIOC_DBG_S_REGISTER = _IOW('V', 79, v4l2_dbg_register)
-VIDIOC_DBG_G_REGISTER = _IOWR('V', 80, v4l2_dbg_register)
-
-VIDIOC_DBG_G_CHIP_IDENT = _IOWR('V', 81, v4l2_dbg_chip_ident)
-
-VIDIOC_S_HW_FREQ_SEEK = _IOW('V', 82, v4l2_hw_freq_seek)
-VIDIOC_ENUM_DV_PRESETS = _IOWR('V', 83, v4l2_dv_enum_preset)
-VIDIOC_S_DV_PRESET = _IOWR('V', 84, v4l2_dv_preset)
-VIDIOC_G_DV_PRESET = _IOWR('V', 85, v4l2_dv_preset)
-VIDIOC_QUERY_DV_PRESET = _IOR('V', 86, v4l2_dv_preset)
-VIDIOC_S_DV_TIMINGS = _IOWR('V', 87, v4l2_dv_timings)
-VIDIOC_G_DV_TIMINGS = _IOWR('V', 88, v4l2_dv_timings)
-
-VIDIOC_OVERLAY_OLD = _IOWR('V', 14, ctypes.c_int)
-VIDIOC_S_PARM_OLD = _IOW('V', 22, v4l2_streamparm)
-VIDIOC_S_CTRL_OLD = _IOW('V', 28, v4l2_control)
-VIDIOC_G_AUDIO_OLD = _IOWR('V', 33, v4l2_audio)
-VIDIOC_G_AUDOUT_OLD = _IOWR('V', 49, v4l2_audioout)
-VIDIOC_CROPCAP_OLD = _IOR('V', 58, v4l2_cropcap)
+VIDIOC_QUERYCAP = _IOR("V", 0, v4l2_capability)
+VIDIOC_RESERVED = _IO("V", 1)
+VIDIOC_ENUM_FMT = _IOWR("V", 2, v4l2_fmtdesc)
+VIDIOC_G_FMT = _IOWR("V", 4, v4l2_format)
+VIDIOC_S_FMT = _IOWR("V", 5, v4l2_format)
+VIDIOC_REQBUFS = _IOWR("V", 8, v4l2_requestbuffers)
+VIDIOC_QUERYBUF = _IOWR("V", 9, v4l2_buffer)
+VIDIOC_G_FBUF = _IOR("V", 10, v4l2_framebuffer)
+VIDIOC_S_FBUF = _IOW("V", 11, v4l2_framebuffer)
+VIDIOC_OVERLAY = _IOW("V", 14, ctypes.c_int)
+VIDIOC_QBUF = _IOWR("V", 15, v4l2_buffer)
+VIDIOC_DQBUF = _IOWR("V", 17, v4l2_buffer)
+VIDIOC_STREAMON = _IOW("V", 18, ctypes.c_int)
+VIDIOC_STREAMOFF = _IOW("V", 19, ctypes.c_int)
+VIDIOC_G_PARM = _IOWR("V", 21, v4l2_streamparm)
+VIDIOC_S_PARM = _IOWR("V", 22, v4l2_streamparm)
+VIDIOC_G_STD = _IOR("V", 23, v4l2_std_id)
+VIDIOC_S_STD = _IOW("V", 24, v4l2_std_id)
+VIDIOC_ENUMSTD = _IOWR("V", 25, v4l2_standard)
+VIDIOC_ENUMINPUT = _IOWR("V", 26, v4l2_input)
+VIDIOC_G_CTRL = _IOWR("V", 27, v4l2_control)
+VIDIOC_S_CTRL = _IOWR("V", 28, v4l2_control)
+VIDIOC_G_TUNER = _IOWR("V", 29, v4l2_tuner)
+VIDIOC_S_TUNER = _IOW("V", 30, v4l2_tuner)
+VIDIOC_G_AUDIO = _IOR("V", 33, v4l2_audio)
+VIDIOC_S_AUDIO = _IOW("V", 34, v4l2_audio)
+VIDIOC_QUERYCTRL = _IOWR("V", 36, v4l2_queryctrl)
+VIDIOC_QUERYMENU = _IOWR("V", 37, v4l2_querymenu)
+VIDIOC_G_INPUT = _IOR("V", 38, ctypes.c_int)
+VIDIOC_S_INPUT = _IOWR("V", 39, ctypes.c_int)
+VIDIOC_G_OUTPUT = _IOR("V", 46, ctypes.c_int)
+VIDIOC_S_OUTPUT = _IOWR("V", 47, ctypes.c_int)
+VIDIOC_ENUMOUTPUT = _IOWR("V", 48, v4l2_output)
+VIDIOC_G_AUDOUT = _IOR("V", 49, v4l2_audioout)
+VIDIOC_S_AUDOUT = _IOW("V", 50, v4l2_audioout)
+VIDIOC_G_MODULATOR = _IOWR("V", 54, v4l2_modulator)
+VIDIOC_S_MODULATOR = _IOW("V", 55, v4l2_modulator)
+VIDIOC_G_FREQUENCY = _IOWR("V", 56, v4l2_frequency)
+VIDIOC_S_FREQUENCY = _IOW("V", 57, v4l2_frequency)
+VIDIOC_CROPCAP = _IOWR("V", 58, v4l2_cropcap)
+VIDIOC_G_CROP = _IOWR("V", 59, v4l2_crop)
+VIDIOC_S_CROP = _IOW("V", 60, v4l2_crop)
+VIDIOC_G_JPEGCOMP = _IOR("V", 61, v4l2_jpegcompression)
+VIDIOC_S_JPEGCOMP = _IOW("V", 62, v4l2_jpegcompression)
+VIDIOC_QUERYSTD = _IOR("V", 63, v4l2_std_id)
+VIDIOC_TRY_FMT = _IOWR("V", 64, v4l2_format)
+VIDIOC_ENUMAUDIO = _IOWR("V", 65, v4l2_audio)
+VIDIOC_ENUMAUDOUT = _IOWR("V", 66, v4l2_audioout)
+VIDIOC_G_PRIORITY = _IOR("V", 67, v4l2_priority)
+VIDIOC_S_PRIORITY = _IOW("V", 68, v4l2_priority)
+VIDIOC_G_SLICED_VBI_CAP = _IOWR("V", 69, v4l2_sliced_vbi_cap)
+VIDIOC_LOG_STATUS = _IO("V", 70)
+VIDIOC_G_EXT_CTRLS = _IOWR("V", 71, v4l2_ext_controls)
+VIDIOC_S_EXT_CTRLS = _IOWR("V", 72, v4l2_ext_controls)
+VIDIOC_TRY_EXT_CTRLS = _IOWR("V", 73, v4l2_ext_controls)
+
+VIDIOC_ENUM_FRAMESIZES = _IOWR("V", 74, v4l2_frmsizeenum)
+VIDIOC_ENUM_FRAMEINTERVALS = _IOWR("V", 75, v4l2_frmivalenum)
+VIDIOC_G_ENC_INDEX = _IOR("V", 76, v4l2_enc_idx)
+VIDIOC_ENCODER_CMD = _IOWR("V", 77, v4l2_encoder_cmd)
+VIDIOC_TRY_ENCODER_CMD = _IOWR("V", 78, v4l2_encoder_cmd)
+
+VIDIOC_DBG_S_REGISTER = _IOW("V", 79, v4l2_dbg_register)
+VIDIOC_DBG_G_REGISTER = _IOWR("V", 80, v4l2_dbg_register)
+
+VIDIOC_DBG_G_CHIP_IDENT = _IOWR("V", 81, v4l2_dbg_chip_ident)
+
+VIDIOC_S_HW_FREQ_SEEK = _IOW("V", 82, v4l2_hw_freq_seek)
+VIDIOC_ENUM_DV_PRESETS = _IOWR("V", 83, v4l2_dv_enum_preset)
+VIDIOC_S_DV_PRESET = _IOWR("V", 84, v4l2_dv_preset)
+VIDIOC_G_DV_PRESET = _IOWR("V", 85, v4l2_dv_preset)
+VIDIOC_QUERY_DV_PRESET = _IOR("V", 86, v4l2_dv_preset)
+VIDIOC_S_DV_TIMINGS = _IOWR("V", 87, v4l2_dv_timings)
+VIDIOC_G_DV_TIMINGS = _IOWR("V", 88, v4l2_dv_timings)
+
+VIDIOC_OVERLAY_OLD = _IOWR("V", 14, ctypes.c_int)
+VIDIOC_S_PARM_OLD = _IOW("V", 22, v4l2_streamparm)
+VIDIOC_S_CTRL_OLD = _IOW("V", 28, v4l2_control)
+VIDIOC_G_AUDIO_OLD = _IOWR("V", 33, v4l2_audio)
+VIDIOC_G_AUDOUT_OLD = _IOWR("V", 49, v4l2_audioout)
+VIDIOC_CROPCAP_OLD = _IOR("V", 58, v4l2_cropcap)
 
 BASE_VIDIOC_PRIVATE = 192
diff --git a/howdy/src/recorders/video_capture.py b/howdy/src/recorders/video_capture.py
index 15888a5..df98ffd 100644
--- a/howdy/src/recorders/video_capture.py
+++ b/howdy/src/recorders/video_capture.py
@@ -16,130 +16,145 @@ from i18n import _
 
 
 class VideoCapture:
-	def __init__(self, config):
-		"""
-		Creates a new VideoCapture instance depending on the settings in the
-		provided config file.
-
-		Config can either be a string to the path, or a pre-setup configparser.
-		"""
-
-		# Parse config from string if needed
-		if isinstance(config, str):
-			self.config = configparser.ConfigParser()
-			self.config.read(config)
-		else:
-			self.config = config
-
-		# Check device path
-		if not os.path.exists(self.config.get("video", "device_path")):
-			if self.config.getboolean("video", "warn_no_device", fallback=True):
-				print(_("Howdy could not find a camera device at the path specified in the config file."))
-				print(_("It is very likely that the path is not configured correctly, please edit the 'device_path' config value by running:"))
-				print("\n\tsudo howdy config\n")
-			sys.exit(14)
-
-		# Create reader
-		# The internal video recorder
-		self.internal = None
-		# The frame width
-		self.fw = None
-		# The frame height
-		self.fh = None
-		self._create_reader()
-
-		# Request a frame to wake the camera up
-		self.internal.grab()
-
-	def __del__(self):
-		"""
-		Frees resources when destroyed
-		"""
-		if self is not None:
-			try:
-				self.internal.release()
-			except AttributeError as err:
-				pass
-
-	def release(self):
-		"""
-		Release cameras
-		"""
-		if self is not None:
-			self.internal.release()
-
-	def read_frame(self):
-		"""
-		Reads a frame, returns the frame and an attempted grayscale conversion of
-		the frame in a tuple:
-
-		(frame, grayscale_frame)
-
-		If the grayscale conversion fails, both items in the tuple are identical.
-		"""
-
-		# Grab a single frame of video
-		# Don't remove ret, it doesn't work without it
-		ret, frame = self.internal.read()
-		if not ret:
-			print(_("Failed to read camera specified in the 'device_path' config option, aborting"))
-			sys.exit(14)
-
-		try:
-			# Convert from color to grayscale
-			# First processing of frame, so frame errors show up here
-			gsframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
-		except RuntimeError:
-			gsframe = frame
-		except cv2.error:
-			print("\nAn error occurred in OpenCV\n")
-			raise
-		return frame, gsframe
-
-	def _create_reader(self):
-		"""
-		Sets up the video reader instance
-		"""
-		recording_plugin = self.config.get("video", "recording_plugin", fallback="opencv")
-
-		if recording_plugin == "ffmpeg":
-			# Set the capture source for ffmpeg
-			from recorders.ffmpeg_reader import ffmpeg_reader
-			self.internal = ffmpeg_reader(
-				self.config.get("video", "device_path"),
-				self.config.get("video", "device_format", fallback="v4l2")
-			)
-
-		elif recording_plugin == "pyv4l2":
-			# Set the capture source for pyv4l2
-			from recorders.pyv4l2_reader import pyv4l2_reader
-			self.internal = pyv4l2_reader(
-				self.config.get("video", "device_path"),
-				self.config.get("video", "device_format", fallback="v4l2")
-			)
-
-		else:
-			# Start video capture on the IR camera through OpenCV
-			self.internal = cv2.VideoCapture(
-				self.config.get("video", "device_path"),
-				cv2.CAP_V4L
-			)
-			# Set the capture frame rate
-			# Without this the first detected (and possibly lower) frame rate is used, -1 seems to select the highest
-			# Use 0 as a fallback to avoid breaking an existing setup, new installs should default to -1
-			self.fps = self.config.getint("video", "device_fps", fallback=0)
-			if self.fps != 0:
-				self.internal.set(cv2.CAP_PROP_FPS, self.fps)
-
-		# Force MJPEG decoding if true
-		if self.config.getboolean("video", "force_mjpeg", fallback=False):
-			# Set a magic number, will enable MJPEG but is badly documentated
-			self.internal.set(cv2.CAP_PROP_FOURCC, 1196444237)
-
-		# Set the frame width and height if requested
-		self.fw = self.config.getint("video", "frame_width", fallback=-1)
-		self.fh = self.config.getint("video", "frame_height", fallback=-1)
-		if self.fw != -1:
-			self.internal.set(cv2.CAP_PROP_FRAME_WIDTH, self.fw)
-		if self.fh != -1:
-			self.internal.set(cv2.CAP_PROP_FRAME_HEIGHT, self.fh)
+    def __init__(self, config):
+        """
+        Creates a new VideoCapture instance depending on the settings in the
+        provided config file.
+
+        Config can either be a string to the path, or a pre-setup configparser.
+        """
+
+        # Parse config from string if needed
+        if isinstance(config, str):
+            self.config = configparser.ConfigParser()
+            self.config.read(config)
+        else:
+            self.config = config
+
+        # Check device path
+        if not os.path.exists(self.config.get("video", "device_path")):
+            if self.config.getboolean("video", "warn_no_device", fallback=True):
+                print(
+                    _(
+                        "Howdy could not find a camera device at the path specified in the config file."
+                    )
+                )
+                print(
+                    _(
+                        "It is very likely that the path is not configured correctly, please edit the 'device_path' config value by running:"
+                    )
+                )
+                print("\n\tsudo howdy config\n")
+            sys.exit(14)
+
+        # Create reader
+        # The internal video recorder
+        self.internal = None
+        # The frame width
+        self.fw = None
+        # The frame height
+        self.fh = None
+        self._create_reader()
+
+        # Request a frame to wake the camera up
+        self.internal.grab()
+
+    def __del__(self):
+        """
+        Frees resources when destroyed
+        """
+        if self is not None:
+            try:
+                self.internal.release()
+            except AttributeError as err:
+                pass
+
+    def release(self):
+        """
+        Release cameras
+        """
+        if self is not None:
+            self.internal.release()
+
+    def read_frame(self):
+        """
+        Reads a frame, returns the frame and an attempted grayscale conversion of
+        the frame in a tuple:
+
+        (frame, grayscale_frame)
+
+        If the grayscale conversion fails, both items in the tuple are identical.
+        """
+
+        # Grab a single frame of video
+        # Don't remove ret, it doesn't work without it
+        ret, frame = self.internal.read()
+        if not ret:
+            print(
+                _(
+                    "Failed to read camera specified in the 'device_path' config option, aborting"
+                )
+            )
+            sys.exit(14)
+
+        try:
+            # Convert from color to grayscale
+            # First processing of frame, so frame errors show up here
+            gsframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
+        except RuntimeError:
+            gsframe = frame
+        except cv2.error:
+            print("\nAn error occurred in OpenCV\n")
+            raise
+        return frame, gsframe
+
+    def _create_reader(self):
+        """
+        Sets up the video reader instance
+        """
+        recording_plugin = self.config.get(
+            "video", "recording_plugin", fallback="opencv"
+        )
+
+        if recording_plugin == "ffmpeg":
+            # Set the capture source for ffmpeg
+            from recorders.ffmpeg_reader import ffmpeg_reader
+
+            self.internal = ffmpeg_reader(
+                self.config.get("video", "device_path"),
+                self.config.get("video", "device_format", fallback="v4l2"),
+            )
+
+        elif recording_plugin == "pyv4l2":
+            # Set the capture source for pyv4l2
+            from recorders.pyv4l2_reader import pyv4l2_reader
+
+            self.internal = pyv4l2_reader(
+                self.config.get("video", "device_path"),
+                self.config.get("video", "device_format", fallback="v4l2"),
+            )
+
+        else:
+            # Start video capture on the IR camera through OpenCV
+            self.internal = cv2.VideoCapture(
+                self.config.get("video", "device_path"), cv2.CAP_V4L
+            )
+            # Set the capture frame rate
+            # Without this the first detected (and possibly lower) frame rate is used, -1 seems to select the highest
+            # Use 0 as a fallback to avoid breaking an existing setup, new installs should default to -1
+            self.fps = self.config.getint("video", "device_fps", fallback=0)
+            if self.fps != 0:
+                self.internal.set(cv2.CAP_PROP_FPS, self.fps)
+
+        # Force MJPEG decoding if true
+        if self.config.getboolean("video", "force_mjpeg", fallback=False):
+            # Set a magic number, will enable MJPEG but is badly documentated
+            self.internal.set(cv2.CAP_PROP_FOURCC, 1196444237)
+
+        # Set the frame width and height if requested
+        self.fw = self.config.getint("video", "frame_width", fallback=-1)
+        self.fh = self.config.getint("video", "frame_height", fallback=-1)
+        if self.fw != -1:
+            self.internal.set(cv2.CAP_PROP_FRAME_WIDTH, self.fw)
+        if self.fh != -1:
+            self.internal.set(cv2.CAP_PROP_FRAME_HEIGHT, self.fh)
diff --git a/howdy/src/rubberstamps/__init__.py b/howdy/src/rubberstamps/__init__.py
index 57a115f..8603b93 100644
--- a/howdy/src/rubberstamps/__init__.py
+++ b/howdy/src/rubberstamps/__init__.py
@@ -8,172 +8,180 @@ from importlib.machinery import SourceFileLoader
 
 
 class RubberStamp:
-	"""Howdy rubber stamp"""
+    """Howdy rubber stamp"""
 
-	UI_TEXT = "ui_text"
-	UI_SUBTEXT = "ui_subtext"
+    UI_TEXT = "ui_text"
+    UI_SUBTEXT = "ui_subtext"
 
-	def set_ui_text(self, text, type=None):
-		"""Convert an ui string to input howdy-gtk understands"""
-		typedec = "M"
+    def set_ui_text(self, text, type=None):
+        """Convert an ui string to input howdy-gtk understands"""
+        typedec = "M"
 
-		if type == self.UI_SUBTEXT:
-			typedec = "S"
+        if type == self.UI_SUBTEXT:
+            typedec = "S"
 
-		return self.send_ui_raw(typedec + "=" + text)
+        return self.send_ui_raw(typedec + "=" + text)
 
-	def send_ui_raw(self, command):
-		"""Write raw command to howdy-gtk stdin"""
-		if self.config.getboolean("debug", "verbose_stamps", fallback=False):
-			print("Sending command to howdy-gtk: " + command)
+    def send_ui_raw(self, command):
+        """Write raw command to howdy-gtk stdin"""
+        if self.config.getboolean("debug", "verbose_stamps", fallback=False):
+            print("Sending command to howdy-gtk: " + command)
 
-		# Add a newline because the ui reads per line
-		command += " \n"
+        # Add a newline because the ui reads per line
+        command += " \n"
 
-		# If we're connected to the ui
-		if self.gtk_proc:
-			# Send the command as bytes
-			self.gtk_proc.stdin.write(bytearray(command.encode("utf-8")))
-			self.gtk_proc.stdin.flush()
+        # If we're connected to the ui
+        if self.gtk_proc:
+            # Send the command as bytes
+            self.gtk_proc.stdin.write(bytearray(command.encode("utf-8")))
+            self.gtk_proc.stdin.flush()
 
-			# Write a padding line to force the command through any buffers
-			self.gtk_proc.stdin.write(bytearray("P=_PADDING \n".encode("utf-8")))
-			self.gtk_proc.stdin.flush()
+            # Write a padding line to force the command through any buffers
+            self.gtk_proc.stdin.write(bytearray("P=_PADDING \n".encode("utf-8")))
+            self.gtk_proc.stdin.flush()
 
 
 def execute(config, gtk_proc, opencv):
-	verbose = config.getboolean("debug", "verbose_stamps", fallback=False)
-	dir_path = os.path.dirname(os.path.realpath(__file__))
-	installed_stamps = []
-
-	# Go through each file in the rubberstamp folder
-	for filename in os.listdir(dir_path):
-		# Remove non-readable file or directories
-		if not os.path.isfile(dir_path + "/" + filename):
-			continue
-
-		# Remove meta files
-		if filename in ["__init__.py", ".gitignore"]:
-			continue
-
-		# Add the found file to the list of enabled rubberstamps
-		installed_stamps.append(filename.split(".")[0])
-
-	if verbose: print("Installed rubberstamps: " + ", ".join(installed_stamps))
-
-	# Get the rules defined in the config
-	raw_rules = config.get("rubberstamps", "stamp_rules")
-	rules = raw_rules.split("\n")
-
-	# Go through the rules one by one
-	for rule in rules:
-		rule = rule.strip()
-
-		if len(rule) <= 1:
-			continue
-
-		# Parse the rule with regex
-		regex_result = re.search("^(\w+)\s+([\w\.]+)\s+([a-z]+)(.*)?$", rule, re.IGNORECASE)
-
-		# Error out if the regex did not match (invalid line)
-		if not regex_result:
-			print(_("Error parsing rubberstamp rule: {}").format(rule))
-			continue
-
-		type = regex_result.group(1)
-
-		# Error out if the stamp name in the rule is not a file
-		if type not in installed_stamps:
-			print(_("Stamp not installed: {}").format(type))
-			continue
-
-		# Load the module from file
-		module = SourceFileLoader(type, dir_path + "/" + type + ".py").load_module()
-
-		# Try to get the class with the same name
-		try:
-			constructor = getattr(module, type)
-		except AttributeError:
-			print(_("Stamp error: Class {} not found").format(type))
-			continue
-
-		# Init the class and set common values
-		instance = constructor()
-		instance.verbose = verbose
-		instance.config = config
-		instance.gtk_proc = gtk_proc
-		instance.opencv = opencv
-
-		# Set some opensv shorthands
-		instance.video_capture = opencv["video_capture"]
-		instance.face_detector = opencv["face_detector"]
-		instance.pose_predictor = opencv["pose_predictor"]
-		instance.clahe = opencv["clahe"]
-
-		# Parse and set the 2 required options for all rubberstamps
-		instance.options = {
-			"timeout": float(re.sub("[a-zA-Z]", "", regex_result.group(2))),
-			"failsafe": regex_result.group(3) != "faildeadly"
-		}
-
-		# Try to get the class do declare its other config variables
-		try:
-			instance.declare_config()
-		except Exception:
-			print(_("Internal error in rubberstamp configuration declaration:"))
-
-			import traceback
-			traceback.print_exc()
-			continue
-
-		# Split the optional arguments at the end of the rule by spaces
-		raw_options = regex_result.group(4).split()
-
-		# For each of those aoptional arguments
-		for option in raw_options:
-			# Get the key to the left, and the value to the right of the equal sign
-			key, value = option.split("=")
-
-			# Error out if a key has been set that was not declared by the module before
-			if key not in instance.options:
-				print("Unknown config option for rubberstamp " + type + ": " + key)
-				continue
-
-			# Convert the argument string to an int or float if the declared option has that type
-			if isinstance(instance.options[key], int):
-				value = int(value)
-			elif isinstance(instance.options[key], float):
-				value = float(value)
-
-			instance.options[key] = value
-
-		if verbose:
-			print("Stamp \"" + type + "\" options parsed:")
-			print(instance.options)
-			print("Executing stamp")
-
-		# Make the stamp fail by default
-		result = False
-
-		# Run the stamp code
-		try:
-			result = instance.run()
-		except Exception:
-			print(_("Internal error in rubberstamp:"))
-
-			import traceback
-			traceback.print_exc()
-			continue
-
-		if verbose: print("Stamp \"" + type + "\" returned: " + str(result))
-
-		# Abort authentication if the stamp returned false
-		if result is False:
-			if verbose: print("Authentication aborted by rubber stamp")
-			sys.exit(15)
-
-	# This is outside the for loop, so we've run all the rules
-	if verbose: print("All rubberstamps processed, authentication successful")
-
-	# Exit with no errors
-	sys.exit(0)
+    verbose = config.getboolean("debug", "verbose_stamps", fallback=False)
+    dir_path = os.path.dirname(os.path.realpath(__file__))
+    installed_stamps = []
+
+    # Go through each file in the rubberstamp folder
+    for filename in os.listdir(dir_path):
+        # Remove non-readable file or directories
+        if not os.path.isfile(dir_path + "/" + filename):
+            continue
+
+        # Remove meta files
+        if filename in ["__init__.py", ".gitignore"]:
+            continue
+
+        # Add the found file to the list of enabled rubberstamps
+        installed_stamps.append(filename.split(".")[0])
+
+    if verbose:
+        print("Installed rubberstamps: " + ", ".join(installed_stamps))
+
+    # Get the rules defined in the config
+    raw_rules = config.get("rubberstamps", "stamp_rules")
+    rules = raw_rules.split("\n")
+
+    # Go through the rules one by one
+    for rule in rules:
+        rule = rule.strip()
+
+        if len(rule) <= 1:
+            continue
+
+        # Parse the rule with regex
+        regex_result = re.search(
+            "^(\w+)\s+([\w\.]+)\s+([a-z]+)(.*)?$", rule, re.IGNORECASE
+        )
+
+        # Error out if the regex did not match (invalid line)
+        if not regex_result:
+            print(_("Error parsing rubberstamp rule: {}").format(rule))
+            continue
+
+        type = regex_result.group(1)
+
+        # Error out if the stamp name in the rule is not a file
+        if type not in installed_stamps:
+            print(_("Stamp not installed: {}").format(type))
+            continue
+
+        # Load the module from file
+        module = SourceFileLoader(type, dir_path + "/" + type + ".py").load_module()
+
+        # Try to get the class with the same name
+        try:
+            constructor = getattr(module, type)
+        except AttributeError:
+            print(_("Stamp error: Class {} not found").format(type))
+            continue
+
+        # Init the class and set common values
+        instance = constructor()
+        instance.verbose = verbose
+        instance.config = config
+        instance.gtk_proc = gtk_proc
+        instance.opencv = opencv
+
+        # Set some opensv shorthands
+        instance.video_capture = opencv["video_capture"]
+        instance.face_detector = opencv["face_detector"]
+        instance.pose_predictor = opencv["pose_predictor"]
+        instance.clahe = opencv["clahe"]
+
+        # Parse and set the 2 required options for all rubberstamps
+        instance.options = {
+            "timeout": float(re.sub("[a-zA-Z]", "", regex_result.group(2))),
+            "failsafe": regex_result.group(3) != "faildeadly",
+        }
+
+        # Try to get the class do declare its other config variables
+        try:
+            instance.declare_config()
+        except Exception:
+            print(_("Internal error in rubberstamp configuration declaration:"))
+
+            import traceback
+
+            traceback.print_exc()
+            continue
+
+        # Split the optional arguments at the end of the rule by spaces
+        raw_options = regex_result.group(4).split()
+
+        # For each of those aoptional arguments
+        for option in raw_options:
+            # Get the key to the left, and the value to the right of the equal sign
+            key, value = option.split("=")
+
+            # Error out if a key has been set that was not declared by the module before
+            if key not in instance.options:
+                print("Unknown config option for rubberstamp " + type + ": " + key)
+                continue
+
+            # Convert the argument string to an int or float if the declared option has that type
+            if isinstance(instance.options[key], int):
+                value = int(value)
+            elif isinstance(instance.options[key], float):
+                value = float(value)
+
+            instance.options[key] = value
+
+        if verbose:
+            print('Stamp "' + type + '" options parsed:')
+            print(instance.options)
+            print("Executing stamp")
+
+        # Make the stamp fail by default
+        result = False
+
+        # Run the stamp code
+        try:
+            result = instance.run()
+        except Exception:
+            print(_("Internal error in rubberstamp:"))
+
+            import traceback
+
+            traceback.print_exc()
+            continue
+
+        if verbose:
+            print('Stamp "' + type + '" returned: ' + str(result))
+
+        # Abort authentication if the stamp returned false
+        if result is False:
+            if verbose:
+                print("Authentication aborted by rubber stamp")
+            sys.exit(15)
+
+    # This is outside the for loop, so we've run all the rules
+    if verbose:
+        print("All rubberstamps processed, authentication successful")
+
+    # Exit with no errors
+    sys.exit(0)
diff --git a/howdy/src/rubberstamps/hotkey.py b/howdy/src/rubberstamps/hotkey.py
index 309aff9..b14d5ba 100644
--- a/howdy/src/rubberstamps/hotkey.py
+++ b/howdy/src/rubberstamps/hotkey.py
@@ -8,62 +8,72 @@ from rubberstamps import RubberStamp
 
 
 class hotkey(RubberStamp):
-	pressed_key = "none"
-
-	def declare_config(self):
-		"""Set the default values for the optional arguments"""
-		self.options["abort_key"] = "esc"
-		self.options["confirm_key"] = "enter"
-
-	def run(self):
-		"""Wait for the user to press a hotkey"""
-		time_left = self.options["timeout"]
-		time_string = _("Aborting authorisation in {}") if self.options["failsafe"] else _("Authorising in {}")
-
-		# Set the ui to default strings
-		self.set_ui_text(time_string.format(int(time_left)), self.UI_TEXT)
-		self.set_ui_text(_("Press {abort_key} to abort, {confirm_key} to authorise").format(abort_key=self.options["abort_key"], confirm_key=self.options["confirm_key"]), self.UI_SUBTEXT)
-
-		# Try to import the keyboard module and tell the user to install the module if that fails
-		try:
-			import keyboard
-		except Exception:
-			print("\nMissing module for rubber stamp keyboard!")
-			print("Please run:")
-			print("\t pip3 install keyboard")
-			sys.exit(1)
-
-		# Register hotkeys with the kernel
-		keyboard.add_hotkey(self.options["abort_key"], self.on_key, args=["abort"])
-		keyboard.add_hotkey(self.options["confirm_key"], self.on_key, args=["confirm"])
-
-		# While we have not hit our timeout yet
-		while time_left > 0:
-			# Remove 0.1 seconds from the timer, as that's how long we sleep
-			time_left -= 0.1
-			# Update the ui with the new time
-			self.set_ui_text(time_string.format(str(int(time_left) + 1)), self.UI_TEXT)
-
-			# If the abort key was pressed while the loop was sleeping
-			if self.pressed_key == "abort":
-				# Set the ui to confirm the abort
-				self.set_ui_text(_("Authentication aborted"), self.UI_TEXT)
-				self.set_ui_text("", self.UI_SUBTEXT)
-
-				# Exit
-				time.sleep(1)
-				return False
-
-			# If confirm has pressed, return that auth can continue
-			elif self.pressed_key == "confirm":
-				return True
-
-			# If no key has been pressed, wait for a bit and check again
-			time.sleep(0.1)
-
-		# When our timeout hits, either abort or continue based on failsafe of faildeadly
-		return not self.options["failsafe"]
-
-	def on_key(self, type):
-		"""Called when the user presses a key"""
-		self.pressed_key = type
+    pressed_key = "none"
+
+    def declare_config(self):
+        """Set the default values for the optional arguments"""
+        self.options["abort_key"] = "esc"
+        self.options["confirm_key"] = "enter"
+
+    def run(self):
+        """Wait for the user to press a hotkey"""
+        time_left = self.options["timeout"]
+        time_string = (
+            _("Aborting authorisation in {}")
+            if self.options["failsafe"]
+            else _("Authorising in {}")
+        )
+
+        # Set the ui to default strings
+        self.set_ui_text(time_string.format(int(time_left)), self.UI_TEXT)
+        self.set_ui_text(
+            _("Press {abort_key} to abort, {confirm_key} to authorise").format(
+                abort_key=self.options["abort_key"],
+                confirm_key=self.options["confirm_key"],
+            ),
+            self.UI_SUBTEXT,
+        )
+
+        # Try to import the keyboard module and tell the user to install the module if that fails
+        try:
+            import keyboard
+        except Exception:
+            print("\nMissing module for rubber stamp keyboard!")
+            print("Please run:")
+            print("\t pip3 install keyboard")
+            sys.exit(1)
+
+        # Register hotkeys with the kernel
+        keyboard.add_hotkey(self.options["abort_key"], self.on_key, args=["abort"])
+        keyboard.add_hotkey(self.options["confirm_key"], self.on_key, args=["confirm"])
+
+        # While we have not hit our timeout yet
+        while time_left > 0:
+            # Remove 0.1 seconds from the timer, as that's how long we sleep
+            time_left -= 0.1
+            # Update the ui with the new time
+            self.set_ui_text(time_string.format(str(int(time_left) + 1)), self.UI_TEXT)
+
+            # If the abort key was pressed while the loop was sleeping
+            if self.pressed_key == "abort":
+                # Set the ui to confirm the abort
+                self.set_ui_text(_("Authentication aborted"), self.UI_TEXT)
+                self.set_ui_text("", self.UI_SUBTEXT)
+
+                # Exit
+                time.sleep(1)
+                return False
+
+            # If confirm has pressed, return that auth can continue
+            elif self.pressed_key == "confirm":
+                return True
+
+            # If no key has been pressed, wait for a bit and check again
+            time.sleep(0.1)
+
+        # When our timeout hits, either abort or continue based on failsafe of faildeadly
+        return not self.options["failsafe"]
+
+    def on_key(self, type):
+        """Called when the user presses a key"""
+        self.pressed_key = type
diff --git a/howdy/src/rubberstamps/nod.py b/howdy/src/rubberstamps/nod.py
index 56f3c0a..f8742bd 100644
--- a/howdy/src/rubberstamps/nod.py
+++ b/howdy/src/rubberstamps/nod.py
@@ -7,92 +7,94 @@ from rubberstamps import RubberStamp
 
 
 class nod(RubberStamp):
-	def declare_config(self):
-		"""Set the default values for the optional arguments"""
-		self.options["min_distance"] = 6
-		self.options["min_directions"] = 2
-
-	def run(self):
-		"""Track a users nose to see if they nod yes or no"""
-		self.set_ui_text(_("Nod to confirm"), self.UI_TEXT)
-		self.set_ui_text(_("Shake your head to abort"), self.UI_SUBTEXT)
-
-		# Stores relative distance between the 2 eyes in the last frame
-		# Used to calculate the distance of the nose traveled in relation to face size in the frame
-		last_reldist = -1
-		# Last point the nose was at
-		last_nosepoint = {"x": -1, "y": -1}
-		# Contains booleans recording successful nods and their directions
-		recorded_nods = {"x": [], "y": []}
-
-		starttime = time.time()
-
-		# Keep running the loop while we have not hit timeout yet
-		while time.time() < starttime + self.options["timeout"]:
-			# Read a frame from the camera
-			ret, frame = self.video_capture.read_frame()
-
-			# Apply CLAHE to get a better picture
-			frame = self.clahe.apply(frame)
-
-			# Detect all faces in the frame
-			face_locations = self.face_detector(frame, 1)
-
-			# Only continue if exactly 1 face is visible in the frame
-			if len(face_locations) != 1:
-				continue
-
-			# Get the position of the eyes and tip of the nose
-			face_landmarks = self.pose_predictor(frame, face_locations[0])
-
-			# Calculate the relative distance between the 2 eyes
-			reldist = face_landmarks.part(0).x - face_landmarks.part(2).x
-			# Average this out with the distance found in the last frame to smooth it out
-			avg_reldist = (last_reldist + reldist) / 2
-
-			# Calculate horizontal movement (shaking head) and vertical movement (nodding)
-			for axis in ["x", "y"]:
-				# Get the location of the nose on the active axis
-				nosepoint = getattr(face_landmarks.part(4), axis)
-
-				# If this is the first frame set the previous values to the current ones
-				if last_nosepoint[axis] == -1:
-					last_nosepoint[axis] = nosepoint
-					last_reldist = reldist
-
-				mindist = self.options["min_distance"]
-				# Get the relative movement by taking the distance traveled and dividing it by eye distance
-				movement = (nosepoint - last_nosepoint[axis]) * 100 / max(avg_reldist, 1)
-
-				# If the movement is over the minimal distance threshold
-				if movement < -mindist or movement > mindist:
-					# If this is the first recorded nod, add it to the array
-					if len(recorded_nods[axis]) == 0:
-						recorded_nods[axis].append(movement < 0)
-
-					# Otherwise, only add this nod if the previous nod with in the other direction
-					elif recorded_nods[axis][-1] != (movement < 0):
-						recorded_nods[axis].append(movement < 0)
-
-				# Check if we have nodded enough on this axis
-				if len(recorded_nods[axis]) >= self.options["min_directions"]:
-					# If nodded yes, show confirmation in ui
-					if (axis == "y"):
-						self.set_ui_text(_("Confirmed authentication"), self.UI_TEXT)
-					# If shaken no, show abort message
-					else:
-						self.set_ui_text(_("Aborted authentication"), self.UI_TEXT)
-
-					# Remove subtext
-					self.set_ui_text("", self.UI_SUBTEXT)
-
-					# Return true for nodding yes and false for shaking no
-					time.sleep(0.8)
-					return axis == "y"
-
-				# Save the relative distance and the nosepoint for next loop
-				last_reldist = reldist
-				last_nosepoint[axis] = nosepoint
-
-		# We've fallen out of the loop, so timeout has been hit
-		return not self.options["failsafe"]
+    def declare_config(self):
+        """Set the default values for the optional arguments"""
+        self.options["min_distance"] = 6
+        self.options["min_directions"] = 2
+
+    def run(self):
+        """Track a users nose to see if they nod yes or no"""
+        self.set_ui_text(_("Nod to confirm"), self.UI_TEXT)
+        self.set_ui_text(_("Shake your head to abort"), self.UI_SUBTEXT)
+
+        # Stores relative distance between the 2 eyes in the last frame
+        # Used to calculate the distance of the nose traveled in relation to face size in the frame
+        last_reldist = -1
+        # Last point the nose was at
+        last_nosepoint = {"x": -1, "y": -1}
+        # Contains booleans recording successful nods and their directions
+        recorded_nods = {"x": [], "y": []}
+
+        starttime = time.time()
+
+        # Keep running the loop while we have not hit timeout yet
+        while time.time() < starttime + self.options["timeout"]:
+            # Read a frame from the camera
+            ret, frame = self.video_capture.read_frame()
+
+            # Apply CLAHE to get a better picture
+            frame = self.clahe.apply(frame)
+
+            # Detect all faces in the frame
+            face_locations = self.face_detector(frame, 1)
+
+            # Only continue if exactly 1 face is visible in the frame
+            if len(face_locations) != 1:
+                continue
+
+            # Get the position of the eyes and tip of the nose
+            face_landmarks = self.pose_predictor(frame, face_locations[0])
+
+            # Calculate the relative distance between the 2 eyes
+            reldist = face_landmarks.part(0).x - face_landmarks.part(2).x
+            # Average this out with the distance found in the last frame to smooth it out
+            avg_reldist = (last_reldist + reldist) / 2
+
+            # Calculate horizontal movement (shaking head) and vertical movement (nodding)
+            for axis in ["x", "y"]:
+                # Get the location of the nose on the active axis
+                nosepoint = getattr(face_landmarks.part(4), axis)
+
+                # If this is the first frame set the previous values to the current ones
+                if last_nosepoint[axis] == -1:
+                    last_nosepoint[axis] = nosepoint
+                    last_reldist = reldist
+
+                mindist = self.options["min_distance"]
+                # Get the relative movement by taking the distance traveled and dividing it by eye distance
+                movement = (
+                    (nosepoint - last_nosepoint[axis]) * 100 / max(avg_reldist, 1)
+                )
+
+                # If the movement is over the minimal distance threshold
+                if movement < -mindist or movement > mindist:
+                    # If this is the first recorded nod, add it to the array
+                    if len(recorded_nods[axis]) == 0:
+                        recorded_nods[axis].append(movement < 0)
+
+                    # Otherwise, only add this nod if the previous nod with in the other direction
+                    elif recorded_nods[axis][-1] != (movement < 0):
+                        recorded_nods[axis].append(movement < 0)
+
+                # Check if we have nodded enough on this axis
+                if len(recorded_nods[axis]) >= self.options["min_directions"]:
+                    # If nodded yes, show confirmation in ui
+                    if axis == "y":
+                        self.set_ui_text(_("Confirmed authentication"), self.UI_TEXT)
+                    # If shaken no, show abort message
+                    else:
+                        self.set_ui_text(_("Aborted authentication"), self.UI_TEXT)
+
+                    # Remove subtext
+                    self.set_ui_text("", self.UI_SUBTEXT)
+
+                    # Return true for nodding yes and false for shaking no
+                    time.sleep(0.8)
+                    return axis == "y"
+
+                # Save the relative distance and the nosepoint for next loop
+                last_reldist = reldist
+                last_nosepoint[axis] = nosepoint
+
+        # We've fallen out of the loop, so timeout has been hit
+        return not self.options["failsafe"]
diff --git a/howdy/src/snapshot.py b/howdy/src/snapshot.py
index 81e84d8..3dd4365 100644
--- a/howdy/src/snapshot.py
+++ b/howdy/src/snapshot.py
@@ -9,54 +9,65 @@ import paths_factory
 
 
 def generate(frames, text_lines):
-	"""Generate a snapshot from given frames"""
-
-	# Don't execute if no frames were given
-	if len(frames) == 0:
-		return
-
-	# Get frame dimensions
-	frame_height, frame_width, cc = frames[0].shape
-	# Spread the given frames out horizontally
-	snap = np.concatenate(frames, axis=1)
-
-	# Create colors
-	pad_color = [44, 44, 44]
-	text_color = [255, 255, 255]
-
-	# Add a gray square at the bottom of the image
-	snap = cv2.copyMakeBorder(snap, 0, len(text_lines) * 20 + 40, 0, 0, cv2.BORDER_CONSTANT, value=pad_color)
-
-	# Add the Howdy logo if there's space to do so
-	if len(frames) > 1:
-		# Load the logo from file
-		logo = cv2.imread(paths_factory.logo_path())
-		# Calculate the position of the logo
-		logo_y = frame_height + 20
-		logo_x = frame_width * len(frames) - 210
-
-		# Overlay the logo on top of the image
-		snap[logo_y:logo_y+57, logo_x:logo_x+180] = logo
-
-	# Go through each line
-	line_number = 0
-	for line in text_lines:
-		# Calculate how far the line should be from the top
-		padding_top = frame_height + 30 + (line_number * 20)
-		# Print the line onto the image
-		cv2.putText(snap, line, (30, padding_top), cv2.FONT_HERSHEY_SIMPLEX, .4, text_color, 0, cv2.LINE_AA)
-
-		line_number += 1
-
-	# Made sure a snapshot folder exist
-	if not os.path.exists(paths_factory.snapshots_dir_path()):
-		os.makedirs(paths_factory.snapshots_dir_path())
-
-	# Generate a filename based on the current time
-	filename = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S.jpg")
-	filepath = paths_factory.snapshot_path(filename)
-	# Write the image to that file
-	cv2.imwrite(filepath, snap)
-
-	# Return the saved file location
-	return filepath
+    """Generate a snapshot from given frames"""
+
+    # Don't execute if no frames were given
+    if len(frames) == 0:
+        return
+
+    # Get frame dimensions
+    frame_height, frame_width, cc = frames[0].shape
+    # Spread the given frames out horizontally
+    snap = np.concatenate(frames, axis=1)
+
+    # Create colors
+    pad_color = [44, 44, 44]
+    text_color = [255, 255, 255]
+
+    # Add a gray square at the bottom of the image
+    snap = cv2.copyMakeBorder(
+        snap, 0, len(text_lines) * 20 + 40, 0, 0, cv2.BORDER_CONSTANT, value=pad_color
+    )
+
+    # Add the Howdy logo if there's space to do so
+    if len(frames) > 1:
+        # Load the logo from file
+        logo = cv2.imread(paths_factory.logo_path())
+        # Calculate the position of the logo
+        logo_y = frame_height + 20
+        logo_x = frame_width * len(frames) - 210
+
+        # Overlay the logo on top of the image
+        snap[logo_y : logo_y + 57, logo_x : logo_x + 180] = logo
+
+    # Go through each line
+    line_number = 0
+    for line in text_lines:
+        # Calculate how far the line should be from the top
+        padding_top = frame_height + 30 + (line_number * 20)
+        # Print the line onto the image
+        cv2.putText(
+            snap,
+            line,
+            (30, padding_top),
+            cv2.FONT_HERSHEY_SIMPLEX,
+            0.4,
+            text_color,
+            0,
+            cv2.LINE_AA,
+        )
+
+        line_number += 1
+
+    # Made sure a snapshot folder exist
+    if not os.path.exists(paths_factory.snapshots_dir_path()):
+        os.makedirs(paths_factory.snapshots_dir_path())
+
+    # Generate a filename based on the current time
+    filename = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S.jpg")
+    filepath = paths_factory.snapshot_path(filename)
+    # Write the image to that file
+    cv2.imwrite(filepath, snap)
+
+    # Return the saved file location
+    return filepath
diff --git a/shell.nix b/shell.nix
new file mode 100644
index 0000000..584d599
--- /dev/null
+++ b/shell.nix
@@ -0,0 +1,55 @@
+{
+  pkgs ? import <nixpkgs> {
+    config = {
+      cudaSupport = true;
+      allowUnfree = true;
+    };
+  },
+}:
+
+let
+  pythonEnv = pkgs.python312.withPackages (
+    ps: with ps; [
+      numpy
+      elevate
+      face-recognition
+      keyboard
+      (opencv4.override {
+        enableGtk3 = true;
+        enableCudnn = true;
+      })
+      pycairo
+      pygobject3
+      pygobject-stubs
+    ]
+  );
+in
+pkgs.mkShell {
+  venvDir = ".venv";
+
+  nativeBuildInputs = with pkgs; [
+    bzip2
+    gobject-introspection
+    meson
+    ninja
+    pkg-config
+    wrapGAppsHook3
+    makeWrapper
+
+    python312Packages.venvShellHook
+  ];
+
+  buildInputs = with pkgs; [
+    fmt
+    gettext
+    gtk3
+    inih
+    libevdev
+    pam
+    pythonEnv
+  ];
+
+  shellHook = ''
+    echo " Python virtual environment at .venv ready!"
+  '';
+}
-- 
2.49.0

